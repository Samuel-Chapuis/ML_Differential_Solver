{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daa77e2b",
   "metadata": {},
   "source": [
    "# **1D Simulator**\n",
    "This code is used to simulate 1D equations in order to create some data for ai training. \n",
    "\n",
    "The first part will be the reproduction burger equation in 1D such as :\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} = \\nu \\frac{\\partial^2 u}{\\partial x^2}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab045a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy import ndimage\n",
    "import collections.abc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca4a141",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "The idea behind this code is to resolve the learning approach from the model. Going from getting the Burger formula to use the Laplace approach in order to the model to learn about the equation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5650e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnableDerivative(nn.Module):\n",
    "    def __init__(self, kernel_size=5, channels=1, learn_scaling=True):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.learn_scaling = learn_scaling\n",
    "        \n",
    "        # Convolution to learn spatial derivatives\n",
    "        self.conv = nn.Conv1d(channels, channels, kernel_size, \n",
    "                             padding=kernel_size//2, bias=False, padding_mode='replicate')\n",
    "        \n",
    "        # Learnable scaling factor (replaces 1/dxÂ²)\n",
    "        if learn_scaling:\n",
    "            self.scale = nn.Parameter(torch.tensor(1.0))\n",
    "        else:\n",
    "            self.scale = 1.0\n",
    "            \n",
    "        # Initialize as approximation of Laplacian\n",
    "        self.initialize_as_laplacian(kernel_size)\n",
    "        \n",
    "    def initialize_as_laplacian(self, kernel_size):\n",
    "        \"\"\"Initialize convolution weights to approximate Laplacian\"\"\"\n",
    "        weight = torch.zeros(1, 1, kernel_size)\n",
    "        center = kernel_size // 2\n",
    "        \n",
    "        if kernel_size == 3:\n",
    "            # Standard 3-point stencil: [1, -2, 1]\n",
    "            weight[0, 0, center-1] = 1.0\n",
    "            weight[0, 0, center] = -2.0\n",
    "            weight[0, 0, center+1] = 1.0\n",
    "        elif kernel_size == 5:\n",
    "            # Higher order approximation: [-1/12, 4/3, -5/2, 4/3, -1/12]\n",
    "            weight[0, 0, center-2] = -1/12\n",
    "            weight[0, 0, center-1] = 4/3\n",
    "            weight[0, 0, center] = -5/2\n",
    "            weight[0, 0, center+1] = 4/3\n",
    "            weight[0, 0, center+2] = -1/12\n",
    "        elif kernel_size == 7:\n",
    "            # Even higher order\n",
    "            weight[0, 0, center-3] = 1/90\n",
    "            weight[0, 0, center-2] = -3/20\n",
    "            weight[0, 0, center-1] = 3/2\n",
    "            weight[0, 0, center] = -49/18\n",
    "            weight[0, 0, center+1] = 3/2\n",
    "            weight[0, 0, center+2] = -3/20\n",
    "            weight[0, 0, center+3] = 1/90\n",
    "            \n",
    "        self.conv.weight = nn.Parameter(weight)\n",
    "        self.conv.weight.requires_grad = True\n",
    "        \n",
    "    def forward(self, u):\n",
    "        # u shape: (batch, channels, spatial)\n",
    "        result = self.conv(u)\n",
    "        if self.learn_scaling:\n",
    "            result = result * self.scale\n",
    "        return result\n",
    "\n",
    "    def get_effective_weights(self):\n",
    "        \"\"\"Get the effective finite difference weights\"\"\"\n",
    "        with torch.no_grad():\n",
    "            weights = self.conv.weight.data.clone()\n",
    "            if self.learn_scaling:\n",
    "                weights = weights * self.scale.item()\n",
    "            return weights.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "403f5a99-2e58-4ad8-bd0f-a5e3b0d48361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main change regarding the burgers1D learning with Laplace.Before -> Burgers1D review Samuel's code to confirm the accuracy.\n",
    "\n",
    "class Burgers1DLearned:\n",
    "    def __init__(self, grid, nu, initial_condition, \n",
    "                 boundary_condition=None, cfl_safety=0.5,\n",
    "                 learn_laplacian=True, kernel_size=5, device='cpu'):\n",
    "        self.grid = grid\n",
    "        self.nu = float(nu)\n",
    "        self.initial_condition = initial_condition\n",
    "        self.boundary_condition = boundary_condition or bc_periodic\n",
    "        self.cfl_safety = float(cfl_safety)\n",
    "        self.device = device\n",
    "        \n",
    "        # Learnable components\n",
    "        self.learn_laplacian = learn_laplacian\n",
    "        if learn_laplacian:\n",
    "            self.laplacian_net = LearnableDerivative(kernel_size=kernel_size).to(device)\n",
    "            self.optimizer = optim.Adam(self.laplacian_net.parameters(), lr=1e-3)\n",
    "            self.loss_history = []\n",
    "        else:\n",
    "            self.laplacian_net = None\n",
    "\n",
    "    def compute_laplacian(self, U):\n",
    "        \"\"\"Compute Laplacian using either learned or analytical method\"\"\"\n",
    "        if self.learn_laplacian and self.laplacian_net is not None:\n",
    "            return self.learned_laplacian(U)\n",
    "        else:\n",
    "            return self.analytical_laplacian(U)\n",
    "    \n",
    "    def learned_laplacian(self, U):\n",
    "        \"\"\"Compute Laplacian using learned convolution\"\"\"\n",
    "        # Convert to tensor and add batch/channel dimensions\n",
    "        u_tensor = torch.FloatTensor(U).unsqueeze(0).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            laplacian_tensor = self.laplacian_net(u_tensor)\n",
    "        \n",
    "        # Convert back to numpy and remove extra dimensions\n",
    "        laplacian = laplacian_tensor.squeeze().cpu().numpy()\n",
    "        \n",
    "        # Apply the correct scaling (1/dxÂ²)\n",
    "        scaling_factor = 1.0 / (self.grid.dx ** 2)\n",
    "        return laplacian * scaling_factor\n",
    "    \n",
    "    def analytical_laplacian(self, U):\n",
    "        \"\"\"Original finite difference Laplacian\"\"\"\n",
    "        #return Burgers1D(self.grid, self.nu, self.initial_condition).analytical_laplacian(U) OLD \n",
    "        return Burgers1D(self.grid, self.nu, self.initial_condition).deriv2order(U)\n",
    "\n",
    "    def generate_high_res_reference(self, U_low_res, scale_factor=2):\n",
    "        \"\"\"Generate high-resolution reference using interpolation\"\"\"\n",
    "        # Use spline interpolation for smooth high-res data\n",
    "        from scipy import ndimage\n",
    "        \n",
    "        # Interpolate to higher resolution\n",
    "        U_high_res = ndimage.zoom(U_low_res, scale_factor, order=3)\n",
    "        return U_high_res\n",
    "\n",
    "    def compute_reference_laplacian(self, U, dx):\n",
    "        \"\"\"Compute reference Laplacian using high-order finite differences\"\"\"\n",
    "        # Use 5-point stencil for accurate reference\n",
    "        laplacian = np.zeros_like(U)\n",
    "        n = len(U)\n",
    "        \n",
    "        if n >= 5:\n",
    "            # Interior points (5-point stencil)\n",
    "            laplacian[2:-2] = (-U[4:] + 16*U[3:-1] - 30*U[2:-2] + 16*U[1:-3] - U[:-4]) / (12 * dx**2)\n",
    "            # Boundaries with lower order\n",
    "            laplacian[0] = (U[1] - 2*U[0] + U[0]) / dx**2\n",
    "            laplacian[1] = (U[2] - 2*U[1] + U[0]) / dx**2\n",
    "            laplacian[-2] = (U[-1] - 2*U[-2] + U[-3]) / dx**2\n",
    "            laplacian[-1] = (U[-1] - 2*U[-1] + U[-2]) / dx**2\n",
    "        \n",
    "        return laplacian\n",
    "\n",
    "    def train_laplacian(self, training_data=None, epochs=1000, batch_size=8):\n",
    "        \"\"\"Train the Laplacian network on accurate reference data\"\"\"\n",
    "        if not self.learn_laplacian:\n",
    "            return\n",
    "            \n",
    "        if training_data is None:\n",
    "            # Generate training data if not provided\n",
    "            training_data = self.generate_training_data(n_samples=100)\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        self.loss_history = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            num_batches = 0\n",
    "            \n",
    "            # Shuffle training data\n",
    "            np.random.shuffle(training_data)\n",
    "            \n",
    "            for i in range(0, len(training_data), batch_size):\n",
    "                batch_data = training_data[i:i+batch_size]\n",
    "                batch_loss = 0\n",
    "                \n",
    "                for U_low_res in batch_data:\n",
    "                    # Generate high-resolution reference\n",
    "                    U_high_res = self.generate_high_res_reference(U_low_res)\n",
    "                    dx_high_res = self.grid.dx / 2  # Higher resolution\n",
    "                    \n",
    "                    # Compute target Laplacian from high-res data using accurate method\n",
    "                    target_laplacian = self.compute_reference_laplacian(U_high_res, dx_high_res)\n",
    "                    \n",
    "                    # Downsample target to match low-res grid\n",
    "                    target_low_res = target_laplacian[::2]\n",
    "                    \n",
    "                    # Prepare tensors\n",
    "                    U_low_tensor = torch.FloatTensor(U_low_res).unsqueeze(0).unsqueeze(0).to(self.device)\n",
    "                    target_tensor = torch.FloatTensor(target_low_res).unsqueeze(0).unsqueeze(0).to(self.device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    self.optimizer.zero_grad()\n",
    "                    pred_laplacian = self.laplacian_net(U_low_tensor)\n",
    "                    \n",
    "                    # Compute loss (with proper scaling)\n",
    "                    scaling = 1.0 / (self.grid.dx ** 2)\n",
    "                    pred_laplacian_scaled = pred_laplacian * scaling\n",
    "                    \n",
    "                    loss = criterion(pred_laplacian_scaled, target_tensor)\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    \n",
    "                    batch_loss += loss.item()\n",
    "                \n",
    "                total_loss += batch_loss / len(batch_data)\n",
    "                num_batches += 1\n",
    "            \n",
    "            avg_loss = total_loss / num_batches if num_batches > 0 else total_loss\n",
    "            self.loss_history.append(avg_loss)\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {avg_loss:.6f}\")\n",
    "                # Print learned weights for monitoring\n",
    "                if self.learn_laplacian:\n",
    "                    weights = self.laplacian_net.get_effective_weights()\n",
    "                    print(f\"  Learned weights: {weights}\")\n",
    "\n",
    "    def generate_training_data(self, n_samples=100):\n",
    "        \"\"\"Generate diverse training data with accurate Laplacians\"\"\"\n",
    "        training_data = []\n",
    "        \n",
    "        # Create various initial conditions\n",
    "        speeds = np.random.uniform(1.0, 4.0, n_samples)\n",
    "        kinds = [\"shock\", \"rarefaction\", \"sine\", \"smooth\"]\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            speed = speeds[i]\n",
    "            kind = kinds[i % len(kinds)]\n",
    "            \n",
    "            # Generate initial condition - FIXED LINE\n",
    "            ic_fn, _ = make_initial_condition_burgers_fn(self.grid.x, speed, kind)\n",
    "            u0 = ic_fn(self.grid.x)\n",
    "            \n",
    "            # Add some noise or variations\n",
    "            if np.random.random() > 0.5:\n",
    "                noise = np.random.normal(0, 0.1, len(u0))\n",
    "                u0 = u0 + noise\n",
    "            \n",
    "            training_data.append(u0)\n",
    "        \n",
    "        return training_data\n",
    "\n",
    "    def simulate(self):\n",
    "        \"\"\"Run simulation using learned operators\"\"\"\n",
    "        # Use the parent class simulation but with learned Laplacian\n",
    "        parent_sim = Burgers1D(self.grid, self.nu, self.initial_condition,\n",
    "                              self.boundary_condition, self.cfl_safety)\n",
    "        \n",
    "        nt = len(self.grid.t)\n",
    "        nx = len(self.grid.x)\n",
    "        dx = float(self.grid.dx)\n",
    "        dt = float(self.grid.dt)\n",
    "        nu = float(self.nu)\n",
    "\n",
    "        U = np.zeros((nt, nx))\n",
    "        U[0, :] = self.initial_condition(self.grid.x)\n",
    "\n",
    "        parent_sim.check_cfl_burgers(U[0, :])\n",
    "\n",
    "        for n in range(nt - 1):\n",
    "            u = U[n, :]\n",
    "\n",
    "            up = parent_sim._apply_boundary(u)\n",
    "            uL = up[:-1]\n",
    "            uR = up[1:]\n",
    "            Fh = parent_sim.rusanov_flux(uL, uR)\n",
    "            conv = -(Fh[1:] - Fh[:-1]) / dx\n",
    "\n",
    "            # Use learned Laplacian\n",
    "            diff = nu * self.compute_laplacian(u)\n",
    "\n",
    "            U[n + 1, :] = u + dt * (conv + diff)\n",
    "\n",
    "            parent_sim.check_cfl_burgers(U[n + 1, :])\n",
    "\n",
    "        return U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2513818b-2447-45ab-86c2-2fd655e9734a",
   "metadata": {},
   "source": [
    "# Physical Simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e307c3b0",
   "metadata": {},
   "source": [
    "## 1D Grid creation helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "badaa1d9-c478-4bf4-9ad5-68c392457241",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid1D:\n",
    "    def __init__(self, nb_points_x, x_min, x_max, t_max, dt):\n",
    "        if x_max < x_min:\n",
    "            x_min, x_max = x_max, x_min\n",
    "        self.nb_points_x = int(nb_points_x)\n",
    "        self.x_min, self.x_max = float(x_min), float(x_max)\n",
    "        self.dt = float(dt)\n",
    "\n",
    "        # temps\n",
    "        self.nb_points_t = int(np.floor(t_max / dt)) + 1\n",
    "        self.t_max = self.dt * (self.nb_points_t - 1)\n",
    "\n",
    "        # espace\n",
    "        self.dx = (self.x_max - self.x_min) / (self.nb_points_x - 1)\n",
    "        self.x = np.linspace(self.x_min, self.x_max, self.nb_points_x)\n",
    "        self.t = np.linspace(0, self.t_max, self.nb_points_t)\n",
    "\n",
    "    def save_npz(self, path, U, *, nu=None, speed=None, tag=\"\"):\n",
    "        meta = dict(\n",
    "            U=U.astype(np.float32),\n",
    "            x=self.x.astype(np.float32),\n",
    "            t=self.t.astype(np.float32),\n",
    "            dx=np.float32(self.dx),\n",
    "            dt=np.float32(self.dt),\n",
    "        )\n",
    "        if nu is not None:\n",
    "            meta[\"nu\"] = np.float32(nu)\n",
    "        if speed is not None:\n",
    "            meta[\"speed\"] = np.float32(speed)\n",
    "        if tag:\n",
    "            meta[\"tag\"] = np.str_(tag)\n",
    "        folder = os.path.dirname(path)\n",
    "        if folder:\n",
    "            os.makedirs(folder, exist_ok=True)\n",
    "        np.savez_compressed(path, **meta)\n",
    "\n",
    "    # (Optionnel) petites aides pour viz rapide\n",
    "    def plot(self, U, title=\"Burgers 1D\", xlabel=\"X\", ylabel=\"T\", label=None, cmap='seismic'):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(\n",
    "            U,\n",
    "            extent=[self.x.min(), self.x.max(), self.t.max(), self.t.min()],\n",
    "            cmap=cmap, interpolation='nearest', aspect='auto', origin='upper'\n",
    "        )\n",
    "        plt.colorbar(label=label)\n",
    "        plt.title(title); plt.xlabel(xlabel); plt.ylabel(ylabel)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_initial_condition(self, U, title=\"U(x, t=0)\", label=None, cmap='seismic'):\n",
    "        # accepte U 2D (t,x) ou 1D (x)\n",
    "        U0 = U[0, :] if getattr(U, \"ndim\", 1) == 2 else U\n",
    "        plt.figure(figsize=(10, 3))\n",
    "        plt.plot(self.x, U0, linewidth=1.8)\n",
    "        if label:\n",
    "            plt.ylabel(label)\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"X\")\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d770bc",
   "metadata": {},
   "source": [
    "## Initial conditions\n",
    "\n",
    "This section documents the simple 1D **initial-condition generators** used by the Burgers simulator.  \n",
    "Each type of initial condition defines the starting velocity field \\( u(x, 0) \\) over the spatial domain.\n",
    "\n",
    "- `shock`: step discontinuity (left/right states of opposite sign)  \n",
    "- `shock_with_gap`: shock separated by a zero plateau in the center  \n",
    "- `rarefaction`: opposite of shock â€” a smooth fan separating two constant states  \n",
    "- `sine`: random sinusoidal perturbation  \n",
    "- `smooth`: random smooth profile (low-pass filtered noise)\n",
    "\n",
    "### Mathematical intent\n",
    "\n",
    "Typical analytic shapes implemented:\n",
    "\n",
    "- **Shock** (discontinuous jump):  \n",
    "  \\[\n",
    "  u(x,0)=\n",
    "  \\begin{cases}\n",
    "  +U, & x < 0 \\\\\n",
    "  -U, & x \\ge 0\n",
    "  \\end{cases}\n",
    "  \\]\n",
    "\n",
    "- **Rarefaction** (reverse sign):  \n",
    "  \\[\n",
    "  u(x,0)=\n",
    "  \\begin{cases}\n",
    "  -U, & x < 0 \\\\\n",
    "  +U, & x \\ge 0\n",
    "  \\end{cases}\n",
    "  \\]\n",
    "\n",
    "- **Sinusoidal**:  \n",
    "  \\[\n",
    "  u(x,0) = U\\,\\sin\\!\\left(\\frac{2\\pi kx}{x_{\\max}} + \\phi\\right),\n",
    "  \\]\n",
    "  where \\(k\\) is a random integer mode (1â€“4) and \\(\\phi\\) a random phase.\n",
    "\n",
    "- **Smooth**: random low-frequency fluctuations obtained by averaging local noise.\n",
    "\n",
    "The helper `make_initial_condition_burgers_fn()` wraps these generators and returns a callable function \\(\\lambda(x)\\) producing the sampled field, together with the chosen kind label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66dd6e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_initial_condition_burgers(x, speed, kind=\"shock\"):\n",
    "    if kind == \"shock\":\n",
    "        u0 = np.where(x < 0, speed, -speed)\n",
    "    elif kind == \"shock_with_gap\":\n",
    "        u0 = np.where((x >= -1.0) & (x <= 1.0), 0.0, np.where(x < 0, 1.0, -1.0)) * speed\n",
    "    elif kind == \"rarefaction\":\n",
    "        u0 = np.where(x < 0, -speed, speed)\n",
    "    elif kind == \"sine\":\n",
    "        k = np.random.randint(1, 5)\n",
    "        phase = np.random.uniform(0, 2*np.pi)\n",
    "        xmax = max(1e-12, x.max())\n",
    "        u0 = np.sin(2*np.pi*k*x/xmax + phase) * speed\n",
    "    else:  # \"smooth\" alÃ©atoire\n",
    "        u0 = (np.random.rand(len(x)) * 2 - 1) * speed\n",
    "        u0 = (u0 + np.roll(u0, 1) + np.roll(u0, -1)) / 3.0\n",
    "    return u0\n",
    "\n",
    "def make_initial_condition_burgers_fn(x, speed, kind=None):\n",
    "    if kind is None:\n",
    "        kind = np.random.choice([\"shock\", \"rarefaction\", \"sine\", \"smooth\"])\n",
    "    u0 = sample_initial_condition_burgers(x, speed, kind)\n",
    "    return (lambda _x: u0), kind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c74a6b",
   "metadata": {},
   "source": [
    "## Boundary conditions\n",
    "\n",
    "This section documents the simple 1D boundary-condition helpers used by the simulator:\n",
    "\n",
    "- `bc_dirichlet_clamp`: Dirichlet (clamped) boundary values\n",
    "- `bc_periodic`: periodic / wrap-around\n",
    "- `bc_neumann_zero`: zero-gradient (Neumann) â€” mirrored ghost cells\n",
    "\n",
    "### Mathematical intent\n",
    "\n",
    "- Dirichlet (clamp): fix the solution at the two domain ends, e.g.\n",
    "\n",
    "  $$u(x_{\\min},t)=u_{\\text{left}},\\qquad u(x_{\\max},t)=u_{\\text{right}}.$$\n",
    "\n",
    "- Periodic: values wrap around the domain, i.e. $$u(x+L)=u(x)\\quad(L=x_{\\max}-x_{\\min}).$$\n",
    "\n",
    "- Neumann zero-gradient: enforce $$\\partial_x u|_{\\text{boundary}} = 0$$ which is commonly implemented by mirroring the edge values into ghost cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14afadb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dirichlet: clamp to fixed endpoints (values stay equal to current ends)\n",
    "def bc_dirichlet_clamp(u):\n",
    "    return (u[0], u[-1])  # returns tuple -> class pads with these\n",
    "\n",
    "# Periodic (wrap)\n",
    "def bc_periodic(u):\n",
    "    up = np.empty(u.size + 2, dtype=u.dtype)\n",
    "    up[1:-1] = u\n",
    "    up[0]  = u[-1]\n",
    "    up[-1] = u[0]\n",
    "    return up  # already padded\n",
    "\n",
    "# Neumann zero-gradient (mirror)\n",
    "def bc_neumann_zero(u):\n",
    "    up = np.empty(u.size + 2, dtype=u.dtype)\n",
    "    up[1:-1] = u\n",
    "    up[0]  = u[0]\n",
    "    up[-1] = u[-1]\n",
    "    return up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35356151",
   "metadata": {},
   "source": [
    "## Numerical scheme for the 1D Burgers equation\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Continuous form\n",
    "\n",
    "The continuous Burgers equation is:\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} = \\nu \\frac{\\partial^2 u}{\\partial x^2}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Discretization overview\n",
    "\n",
    "We consider a **uniform grid** with spacing $\\Delta x$ and a **time step** $\\Delta t$.\n",
    "\n",
    "The semi-discrete conservative form reads:\n",
    "$$\n",
    "\\frac{d u_i}{d t} = -\\frac{F_{i+1/2}-F_{i-1/2}}{\\Delta x}\n",
    "+ \\nu \\frac{u_{i+1}-2u_i+u_{i-1}}{\\Delta x^2}.\n",
    "$$\n",
    "\n",
    "Here, $F(u)=u^2/2$ is the **numerical flux**, and $F_{i+1/2}$ denotes its interface approximation.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Convective flux choices\n",
    "\n",
    "#### (a) Central flux (second-order, non-dissipative)\n",
    "$$\n",
    "F_{i+1/2} = \\frac{1}{2}\\left(\\frac{u_{i+1}^2}{2} + \\frac{u_i^2}{2}\\right)\n",
    "$$\n",
    "âš ï¸ Not stable for strong gradients or shocks.\n",
    "\n",
    "#### (b) Upwind / Godunov flux (shock-stable)\n",
    "\n",
    "For the Rusanov (local Laxâ€“Friedrichs) flux:\n",
    "$$\n",
    "F_{i+1/2} =\n",
    "\\frac{1}{2}\\left(\\frac{u_i^2}{2} + \\frac{u_{i+1}^2}{2}\\right)\n",
    "- \\frac{1}{2}\\lambda_{i+1/2}(u_{i+1}-u_i),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\lambda_{i+1/2} = \\max(|u_i|,|u_{i+1}|)\n",
    "$$\n",
    "acts as a **numerical dissipation coefficient**.\n",
    "\n",
    "A simple upwind version uses the **sign of $u_i$**:\n",
    "- If $u_i > 0$: use backward difference.  \n",
    "- If $u_i < 0$: use forward difference.  \n",
    "This adds dissipation and stabilizes shocks.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Diffusive term\n",
    "\n",
    "A second-order centered approximation:\n",
    "$$\n",
    "\\partial_{xx} u(x_i,t^n) \\approx\n",
    "\\frac{u_{i+1}^n - 2u_i^n + u_{i-1}^n}{\\Delta x^2}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Time discretization (explicit Euler)\n",
    "\n",
    "Combining both contributions, the explicit update reads:\n",
    "$$\n",
    "u_i^{n+1} = u_i^n\n",
    "- \\frac{\\Delta t}{\\Delta x}\\big(F_{i+1/2}^n - F_{i-1/2}^n\\big)\n",
    "+ \\nu \\Delta t\\;\\frac{u_{i+1}^n - 2u_i^n + u_{i-1}^n}{\\Delta x^2}.\n",
    "$$\n",
    "\n",
    "This gives the **conservative discrete form**:  \n",
    "flux differences for convection + centered second difference for diffusion.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Stability / CFL constraints\n",
    "\n",
    "#### (a) Convective CFL (hyperbolic term)\n",
    "$$\n",
    "\\Delta t \\le \\frac{\\Delta x}{\\max_i |u_i|}\n",
    "$$\n",
    "\n",
    "#### (b) Diffusive constraint (parabolic term)\n",
    "$$\n",
    "\\Delta t \\le \\frac{\\Delta x^2}{2\\nu}\n",
    "$$\n",
    "\n",
    "#### (c) Practical combined condition\n",
    "$$\n",
    "\\Delta t \\le\n",
    "\\min\\!\\left(\n",
    "  C_c\\frac{\\Delta x}{\\max|u|},\n",
    "  C_d\\frac{\\Delta x^2}{2\\nu}\n",
    "\\right)\n",
    "$$\n",
    "with safety factors $C_c, C_d \\in (0,1)$ (typically 0.5).  \n",
    "If violated, the explicit scheme will diverge or produce non-physical oscillations.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "881c036a-3ced-4820-a44d-c8c0a9ee60da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data_for_learning(n_samples=100, grid_params=None):\n",
    "    \"\"\"Generate high-resolution data for training the learned operators\"\"\"\n",
    "    if grid_params is None:\n",
    "        grid_params = {'nbx': 256, 'x_min': -5, 'x_max': 5, 'dt': 1e-3, 'n_steps': 50}\n",
    "    \n",
    "    training_data = []\n",
    "    \n",
    "    # Create high-resolution grid\n",
    "    high_res_grid = make_grid(**grid_params)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Generate high-resolution simulation\n",
    "        speed = np.random.uniform(1.0, 4.0)\n",
    "        ic_fn, _ = make_initial_condition_burgers(high_res_grid.x, speed, kind=None)\n",
    "        \n",
    "        sim = Burgers1D(high_res_grid, nu=0.1, initial_condition=ic_fn, \n",
    "                       boundary_condition=bc_neumann_zero)\n",
    "        U_high_res = sim.simulate()\n",
    "        \n",
    "        # Use the final state as training sample\n",
    "        training_data.append(U_high_res[-1, :])\n",
    "    \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "937eae64-d945-462f-9c5f-770f1068d193",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Burgers1D:\n",
    "    def __init__(self, grid, nu, initial_condition, boundary_condition=bc_periodic, cfl_safety=0.5):\n",
    "        self.grid = grid\n",
    "        self.nu = float(nu)\n",
    "        self.initial_condition = initial_condition\n",
    "        self.boundary_condition = boundary_condition\n",
    "        self.cfl_safety = float(cfl_safety)\n",
    "\n",
    "    @staticmethod\n",
    "    def rusanov_flux(uL, uR):\n",
    "        lam = np.maximum(np.abs(uL), np.abs(uR))\n",
    "        return 0.5 * (0.5*uL*uL + 0.5*uR*uR) - 0.5 * lam * (uR - uL)\n",
    "\n",
    "    def deriv2order(self, U):\n",
    "        nx = U.shape[0]\n",
    "        d2 = np.zeros_like(U)\n",
    "        if nx >= 3:\n",
    "            d2[1:-1] = (U[2:] - 2*U[1:-1] + U[:-2]) / (self.grid.dx ** 2)\n",
    "        if nx >= 2:\n",
    "            d2[0]  = (U[1] - 2*U[0]  + U[0])   / (self.grid.dx ** 2)\n",
    "            d2[-1] = (U[-1] - 2*U[-1] + U[-2]) / (self.grid.dx ** 2)\n",
    "        return d2\n",
    "\n",
    "    def _apply_boundary(self, u):\n",
    "        if self.boundary_condition is None:\n",
    "            return np.concatenate(([u[0]], u, [u[-1]]))\n",
    "        padded = self.boundary_condition(u)\n",
    "        return np.asarray(padded)\n",
    "\n",
    "    def check_cfl_burgers(self, u_now):\n",
    "        dx = abs(float(self.grid.dx))\n",
    "        dt = float(self.grid.dt)\n",
    "        nu = float(self.nu)\n",
    "        umax = float(np.max(np.abs(u_now))) + 1e-14\n",
    "        conv_limit = dx / umax\n",
    "        diff_limit = dx * dx / (2.0 * nu + 1e-14)\n",
    "        limit = self.cfl_safety * min(conv_limit, diff_limit)\n",
    "        if dt > limit:\n",
    "            raise ValueError(\n",
    "                \"Unstable explicit step:\\n\"\n",
    "                f\"  dt={dt:.3e} > safety*min(dx/|u|, dx^2/(2Î½))={limit:.3e}\\n\"\n",
    "                f\"  (dx={dx:.3e}, Î½={nu:.3e}, max|u|={umax:.3e}, safety={self.cfl_safety:.2f})\"\n",
    "            )\n",
    "\n",
    "    def simulate(self):\n",
    "        nt = len(self.grid.t)\n",
    "        nx = len(self.grid.x)\n",
    "        dx = float(self.grid.dx)\n",
    "        dt = float(self.grid.dt)\n",
    "        nu = float(self.nu)\n",
    "\n",
    "        U = np.zeros((nt, nx))\n",
    "        U[0, :] = self.initial_condition(self.grid.x)\n",
    "\n",
    "        self.check_cfl_burgers(U[0, :])\n",
    "\n",
    "        for n in range(nt - 1):\n",
    "            u = U[n, :]\n",
    "\n",
    "            up = self._apply_boundary(u)       # taille nx+2\n",
    "            uL = up[:-1]\n",
    "            uR = up[1:]\n",
    "            Fh = self.rusanov_flux(uL, uR)     # taille nx+1\n",
    "            conv = -(Fh[1:] - Fh[:-1]) / dx\n",
    "\n",
    "            diff = nu * self.deriv2order(u)\n",
    "\n",
    "            U[n + 1, :] = u + dt * (conv + diff)\n",
    "\n",
    "            self.check_cfl_burgers(U[n + 1, :])\n",
    "\n",
    "        return U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a8777c",
   "metadata": {},
   "source": [
    "## Generation Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53f6cebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tolist(x):\n",
    "    return list(x) if (isinstance(x, collections.abc.Iterable) and not isinstance(x, (str, bytes))) else [x]\n",
    "\n",
    "def make_grid(nbx, x_min, x_max, dt, *, n_steps=None, t_final=None):\n",
    "    if t_final is None:\n",
    "        if n_steps is None:\n",
    "            raise ValueError(\"SpÃ©cifie n_steps ou t_final.\")\n",
    "        t_final = (int(n_steps) - 1) * dt\n",
    "    return Grid1D(int(nbx), float(x_min), float(x_max), float(t_final), float(dt))\n",
    "\n",
    "def run_one_sim_burgers(grid, nu, speed, ic_kind=None, cfl_safety=0.5, max_retries=4, boundary_condition=bc_periodic):\n",
    "    dt = grid.dt\n",
    "    nbx = grid.nb_points_x\n",
    "    x_min, x_max = grid.x_min, grid.x_max\n",
    "    n_steps = len(grid.t)\n",
    "    ic_fn, kind_used = make_initial_condition_burgers_fn(grid.x, speed, ic_kind)\n",
    "    for k in range(max_retries):\n",
    "        try:\n",
    "            sim = Burgers1D(grid, nu, ic_fn, boundary_condition=boundary_condition, cfl_safety=cfl_safety)\n",
    "            U = sim.simulate()\n",
    "            return grid, U, kind_used\n",
    "        except ValueError as e:\n",
    "            if \"unstable\" in str(e).lower():\n",
    "                dt *= 0.5\n",
    "                grid = make_grid(nbx, x_min, x_max, dt, n_steps=n_steps)\n",
    "            else:\n",
    "                raise\n",
    "        if k == max_retries - 2 and ic_kind is None:\n",
    "            ic_fn, kind_used = make_initial_condition_burgers_fn(grid.x, speed, None)\n",
    "    raise RuntimeError(\"Impossible de stabiliser la simulation aprÃ¨s plusieurs essais.\")\n",
    "\n",
    "\n",
    "def generate_dataset_burgers(\n",
    "    *,\n",
    "    out_dir=\"generated_1d_burgers\",\n",
    "    nbx=30, x_min=-5, x_max=5, dt=5e-3, n_steps=20,\n",
    "    nu=0.1, speed=4.0,\n",
    "    boundary_condition=bc_periodic,\n",
    "    ic_kinds=None,\n",
    "    n_train=100, n_test=20,\n",
    "    cfl_safety=0.5\n",
    "):\n",
    "    \"\"\"Tous les fichiers sont stockÃ©s dans generated_1d_burgers/{train,test}.\"\"\"\n",
    "    train_dir = os.path.join(out_dir, \"train\")\n",
    "    test_dir  = os.path.join(out_dir, \"test\")\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir,  exist_ok=True)\n",
    "\n",
    "    xmins   = _tolist(x_min)\n",
    "    xmaxs   = _tolist(x_max)\n",
    "    stepsLs = _tolist(n_steps)\n",
    "    speeds  = _tolist(speed)\n",
    "\n",
    "    if len(xmins) != len(xmaxs):\n",
    "        if len(xmins) == 1:\n",
    "            xmins = xmins * len(xmaxs)\n",
    "        elif len(xmaxs) == 1:\n",
    "            xmaxs = xmaxs * len(xmins)\n",
    "        else:\n",
    "            raise ValueError(\"x_min et x_max doivent avoir la mÃªme longueur ou Ãªtre scalaires.\")\n",
    "    x_pairs = list(zip(xmins, xmaxs))\n",
    "\n",
    "    def do_split(N, split_dir, xmin_val, xmax_val, nst_val, spd_val):\n",
    "        for i in range(N):\n",
    "            grid0 = make_grid(nbx, xmin_val, xmax_val, dt, n_steps=int(nst_val))\n",
    "            kind = None if ic_kinds is None else random.choice(ic_kinds)\n",
    "            grid_i, U, kind_used = run_one_sim_burgers(\n",
    "                grid0, nu=float(nu), speed=float(spd_val),\n",
    "                ic_kind=kind, cfl_safety=cfl_safety, boundary_condition=boundary_condition\n",
    "            )\n",
    "\n",
    "            tag = f\"{kind_used}|x=[{xmin_val},{xmax_val}]|T={int(nst_val)}|v={float(spd_val)}\"\n",
    "            # ðŸ”§ Ajoute x_min/x_max dans le nom de fichier pour Ã©viter l'Ã©crasement\n",
    "            filename = (\n",
    "                f\"sample_{i:04d}\"\n",
    "                f\"_x{float(xmin_val):+.2f}_{float(xmax_val):+.2f}\"\n",
    "                f\"_v{float(spd_val):.2f}\"\n",
    "                f\"_T{int(nst_val)}.npz\"\n",
    "            )\n",
    "\n",
    "            grid_i.save_npz(\n",
    "                os.path.join(split_dir, filename),\n",
    "                U, nu=float(nu), speed=float(spd_val), tag=tag\n",
    "            )\n",
    "\n",
    "\n",
    "    for (xmin, xmax) in x_pairs:\n",
    "        for nst in stepsLs:\n",
    "            for spd in speeds:\n",
    "                do_split(n_train, train_dir, xmin, xmax, nst, spd)\n",
    "                do_split(n_test,  test_dir,  xmin, xmax, nst, spd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08bccc5",
   "metadata": {},
   "source": [
    "## File visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da28324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_random_sample(out_dir=\"generated_1d_burgers/test\", cmap=\"seismic\", label=\"U\", title_prefix=\"Burgers\"):\n",
    "    files = [f for f in os.listdir(out_dir) if f.endswith(\".npz\")]\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"Aucun .npz dans {out_dir}\")\n",
    "    sample = random.choice(files)\n",
    "    path = os.path.join(out_dir, sample)\n",
    "    print(f\"ðŸ“‚ Loaded file: {sample}\")\n",
    "\n",
    "    data = np.load(path, allow_pickle=True)\n",
    "    U = data[\"U\"]; x = data[\"x\"]; t = data[\"t\"]; dt = float(data[\"dt\"])\n",
    "    tag = data[\"tag\"].item() if \"tag\" in data else \"\"\n",
    "    nu  = float(data[\"nu\"]) if \"nu\" in data.files else None\n",
    "    spd = float(data[\"speed\"]) if \"speed\" in data.files else None\n",
    "\n",
    "    grid_vis = Grid1D(len(x), float(x.min()), float(x.max()), float(t.max()), dt)\n",
    "    grid_vis.plot_initial_condition(U, title=f\"Initial condition â€” {tag or '?'}\", label=label, cmap=cmap)\n",
    "\n",
    "    tit = f\"{title_prefix} (Î½={nu:.3f}, speed={spd:.3f}, type={tag or '?'})\"\n",
    "    grid_vis.plot(U, title=tit, xlabel=\"X\", ylabel=\"T\", label=label, cmap=cmap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb218c7d",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9a19ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'generate_dataset_burgers(\\n    out_dir=\"generated_1d_burgers\",\\n    nbx=128,\\n    x_min=[-5, -2],\\n    x_max=[5, 2],\\n    dt=5e-3,\\n    n_steps=[128, 256],\\n    nu=0.1,\\n    speed=[1.0, 2.0],\\n    boundary_condition=bc_neumann_zero,\\n    ic_kinds=[\"shock\",\"rarefaction\",\"sine\",\"smooth\"],\\n    n_train=25, n_test=0,\\n    cfl_safety=1\\n)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is Samuel's to review later on\n",
    "'''generate_dataset_burgers(\n",
    "    out_dir=\"generated_1d_burgers\",\n",
    "    nbx=128,\n",
    "    x_min=[-5, -2],\n",
    "    x_max=[5, 2],\n",
    "    dt=5e-3,\n",
    "    n_steps=[128, 256],\n",
    "    nu=0.1,\n",
    "    speed=[1.0, 2.0],\n",
    "    boundary_condition=bc_neumann_zero,\n",
    "    ic_kinds=[\"shock\",\"rarefaction\",\"sine\",\"smooth\"],\n",
    "    n_train=25, n_test=0,\n",
    "    cfl_safety=1\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52ce7571-dcf2-45a5-ab4c-948b3e6c30eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Laplacian for sample 0...\n",
      "Epoch 0, Loss: 3229.493945\n",
      "  Learned weights: [-0.0819331   1.3238842  -2.5387058   1.3947688  -0.09834523]\n",
      "Epoch 100, Loss: 2116.188108\n",
      "  Learned weights: [ 0.12073012  0.97537774 -2.609134    1.8989409  -0.38438347]\n",
      "Epoch 200, Loss: 2145.969148\n",
      "  Learned weights: [ 0.12209132  0.97344726 -2.6053298   1.8948456  -0.38465324]\n",
      "Epoch 300, Loss: 2927.159836\n",
      "  Learned weights: [ 0.11909266  0.98022443 -2.6187332   1.9054776  -0.38970265]\n",
      "Epoch 400, Loss: 2291.699251\n",
      "  Learned weights: [ 0.12019306  0.96991444 -2.6045008   1.8926274  -0.3827841 ]\n",
      "Saved sample_0000_x-5.00_+5.00_v1.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 1...\n",
      "Epoch 0, Loss: 2654.550614\n",
      "  Learned weights: [-0.0828905  1.3371621 -2.5587018  1.400705  -0.0982867]\n",
      "Epoch 100, Loss: 1540.574618\n",
      "  Learned weights: [ 0.08938209  1.049819   -2.6807125   1.951712   -0.40412608]\n",
      "Epoch 200, Loss: 1908.554018\n",
      "  Learned weights: [ 0.08776996  1.0481194  -2.6803875   1.9498895  -0.4036921 ]\n",
      "Epoch 300, Loss: 1990.415066\n",
      "  Learned weights: [ 0.08723894  1.0496229  -2.683449    1.951302   -0.4075721 ]\n",
      "Epoch 400, Loss: 1537.407731\n",
      "  Learned weights: [ 0.08765768  1.0499706  -2.6863267   1.9514042  -0.4078677 ]\n",
      "Saved sample_0001_x-5.00_+5.00_v1.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 2...\n",
      "Epoch 0, Loss: 4641.595755\n",
      "  Learned weights: [-0.08010887  1.3267705  -2.5429442   1.3933647  -0.09814193]\n",
      "Epoch 100, Loss: 2605.069335\n",
      "  Learned weights: [ 0.11057155  0.9903498  -2.613551    1.8942801  -0.38680086]\n",
      "Epoch 200, Loss: 2611.094899\n",
      "  Learned weights: [ 0.11073495  0.98916197 -2.6119952   1.8937795  -0.3864221 ]\n",
      "Epoch 300, Loss: 2984.086669\n",
      "  Learned weights: [ 0.11087424  0.9894377  -2.612491    1.8941528  -0.38509232]\n",
      "Epoch 400, Loss: 2587.809835\n",
      "  Learned weights: [ 0.11139986  0.98903054 -2.6066728   1.8924658  -0.38315177]\n",
      "Saved sample_0002_x-5.00_+5.00_v1.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 3...\n",
      "Epoch 0, Loss: 3380.433805\n",
      "  Learned weights: [-0.07907333  1.3345921  -2.564184    1.4128087  -0.10342002]\n",
      "Epoch 100, Loss: 1696.157760\n",
      "  Learned weights: [ 0.10648324  0.99685425 -2.629089    1.9112129  -0.38714272]\n",
      "Epoch 200, Loss: 1852.326364\n",
      "  Learned weights: [ 0.10714599  1.0020492  -2.634976    1.9176605  -0.38991508]\n",
      "Epoch 300, Loss: 2557.564248\n",
      "  Learned weights: [ 0.10569347  1.0005466  -2.635578    1.9169039  -0.38764718]\n",
      "Epoch 400, Loss: 1728.604945\n",
      "  Learned weights: [ 0.10701614  0.9980478  -2.631619    1.9121394  -0.38852623]\n",
      "Saved sample_0003_x-5.00_+5.00_v1.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 4...\n",
      "Epoch 0, Loss: 4216.491330\n",
      "  Learned weights: [-0.08198845  1.3334808  -2.563018    1.4104307  -0.10038736]\n",
      "Epoch 100, Loss: 2430.637457\n",
      "  Learned weights: [ 0.08995726  1.0395721  -2.6745665   1.944952   -0.40583926]\n",
      "Epoch 200, Loss: 2130.448016\n",
      "  Learned weights: [ 0.09008967  1.0413116  -2.676629    1.9470048  -0.40551138]\n",
      "Epoch 300, Loss: 2182.665646\n",
      "  Learned weights: [ 0.08919287  1.0399114  -2.6733794   1.9466958  -0.40146518]\n",
      "Epoch 400, Loss: 2149.135210\n",
      "  Learned weights: [ 0.09057472  1.0399059  -2.6722112   1.9444399  -0.40364832]\n",
      "Saved sample_0004_x-5.00_+5.00_v1.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 5...\n",
      "Epoch 0, Loss: 3533.430260\n",
      "  Learned weights: [-0.08912419  1.349493   -2.5807226   1.4196359  -0.10130567]\n",
      "Epoch 100, Loss: 2337.836960\n",
      "  Learned weights: [ 0.0655935   1.095179   -2.7306898   1.9870468  -0.41801715]\n",
      "Epoch 200, Loss: 2579.328003\n",
      "  Learned weights: [ 0.06500691  1.0936655  -2.7312102   1.9833332  -0.42286888]\n",
      "Epoch 300, Loss: 2063.055690\n",
      "  Learned weights: [ 0.06614506  1.0951526  -2.7303388   1.9847304  -0.42084765]\n",
      "Epoch 400, Loss: 3141.812642\n",
      "  Learned weights: [ 0.06649154  1.0959594  -2.7300081   1.9893199  -0.4175398 ]\n",
      "Saved sample_0005_x-5.00_+5.00_v1.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 6...\n",
      "Epoch 0, Loss: 3907.040110\n",
      "  Learned weights: [-0.08522558  1.3297437  -2.5401068   1.3983368  -0.10409336]\n",
      "Epoch 100, Loss: 3004.188452\n",
      "  Learned weights: [ 0.13103093  0.9534189  -2.576681    1.8680935  -0.3804991 ]\n",
      "Epoch 200, Loss: 3526.403655\n",
      "  Learned weights: [ 0.13051566  0.956317   -2.5810027   1.8746725  -0.38184237]\n",
      "Epoch 300, Loss: 2707.065351\n",
      "  Learned weights: [ 0.13092417  0.9541773  -2.5793574   1.8729241  -0.37940267]\n",
      "Epoch 400, Loss: 2684.070443\n",
      "  Learned weights: [ 0.13116007  0.95343256 -2.575594    1.8692534  -0.38075215]\n",
      "Saved sample_0006_x-5.00_+5.00_v1.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 7...\n",
      "Epoch 0, Loss: 3129.376110\n",
      "  Learned weights: [-0.08273118  1.3425341  -2.5771677   1.4184197  -0.09945203]\n",
      "Epoch 100, Loss: 1666.490357\n",
      "  Learned weights: [ 0.07674959  1.0702964  -2.7040899   1.9680705  -0.41530076]\n",
      "Epoch 200, Loss: 1701.203640\n",
      "  Learned weights: [ 0.07681551  1.0685961  -2.7012362   1.9660403  -0.41451803]\n",
      "Epoch 300, Loss: 1707.515592\n",
      "  Learned weights: [ 0.07749395  1.0677327  -2.6985369   1.9628538  -0.41465726]\n",
      "Epoch 400, Loss: 1795.854337\n",
      "  Learned weights: [ 0.07732034  1.067248   -2.6969967   1.9626527  -0.41352087]\n",
      "Saved sample_0007_x-5.00_+5.00_v1.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 8...\n",
      "Epoch 0, Loss: 5021.725914\n",
      "  Learned weights: [-0.0829165   1.3409742  -2.5574777   1.4025162  -0.10246702]\n",
      "Epoch 100, Loss: 3470.795363\n",
      "  Learned weights: [ 0.11246087  0.9962115  -2.6276689   1.9150139  -0.39613673]\n",
      "Epoch 200, Loss: 3637.149561\n",
      "  Learned weights: [ 0.11039868  0.9951479  -2.6302059   1.9179175  -0.39559203]\n",
      "Epoch 300, Loss: 3933.234347\n",
      "  Learned weights: [ 0.11074869  0.99259233 -2.627841    1.9150064  -0.3939154 ]\n",
      "Epoch 400, Loss: 2881.161341\n",
      "  Learned weights: [ 0.11105543  0.9920134  -2.6226084   1.9125019  -0.39386293]\n",
      "Saved sample_0008_x-5.00_+5.00_v1.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 9...\n",
      "Epoch 0, Loss: 4928.903913\n",
      "  Learned weights: [-0.08456598  1.3306181  -2.5513015   1.4007313  -0.09911326]\n",
      "Epoch 100, Loss: 3236.135730\n",
      "  Learned weights: [ 0.1143781   0.98386776 -2.6041937   1.8823588  -0.38075504]\n",
      "Epoch 200, Loss: 2906.494676\n",
      "  Learned weights: [ 0.11608573  0.98283964 -2.600764    1.8794839  -0.38023365]\n",
      "Epoch 300, Loss: 2348.324829\n",
      "  Learned weights: [ 0.11532662  0.9817592  -2.599145    1.8790165  -0.3782363 ]\n",
      "Epoch 400, Loss: 2359.505556\n",
      "  Learned weights: [ 0.11510735  0.98060524 -2.5964317   1.8764114  -0.37982735]\n",
      "Saved sample_0009_x-5.00_+5.00_v1.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 10...\n",
      "Epoch 0, Loss: 2705.926473\n",
      "  Learned weights: [-0.07954334  1.3351325  -2.5698745   1.4144812  -0.10562806]\n",
      "Epoch 100, Loss: 1383.845453\n",
      "  Learned weights: [ 0.08544514  1.0498313  -2.691431    1.9590682  -0.40832064]\n",
      "Epoch 200, Loss: 1577.839940\n",
      "  Learned weights: [ 0.08478125  1.0497103  -2.6915374   1.9600626  -0.40609276]\n",
      "Epoch 300, Loss: 1373.144441\n",
      "  Learned weights: [ 0.08563969  1.0504549  -2.6896563   1.9620299  -0.40350124]\n",
      "Epoch 400, Loss: 1376.319088\n",
      "  Learned weights: [ 0.08593395  1.0516955  -2.6926894   1.9621598  -0.406203  ]\n",
      "Saved sample_0010_x-5.00_+5.00_v1.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 11...\n",
      "Epoch 0, Loss: 3111.248095\n",
      "  Learned weights: [-0.08019792  1.3260089  -2.553639    1.4091709  -0.10019871]\n",
      "Epoch 100, Loss: 2493.544195\n",
      "  Learned weights: [ 0.10427602  1.009471   -2.6424215   1.9227183  -0.39326435]\n",
      "Epoch 200, Loss: 2621.016305\n",
      "  Learned weights: [ 0.10378274  1.0084674  -2.6424618   1.9217607  -0.39452124]\n",
      "Epoch 300, Loss: 2069.908160\n",
      "  Learned weights: [ 0.10357209  1.0037179  -2.63612     1.9140484  -0.3934715 ]\n",
      "Epoch 400, Loss: 2035.230571\n",
      "  Learned weights: [ 0.10296547  1.0063597  -2.6353405   1.9168608  -0.3931448 ]\n",
      "Saved sample_0011_x-5.00_+5.00_v1.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 12...\n",
      "Epoch 0, Loss: 3686.862235\n",
      "  Learned weights: [-0.08965696  1.3364612  -2.5481586   1.3956844  -0.09477621]\n",
      "Epoch 100, Loss: 3211.664284\n",
      "  Learned weights: [ 0.08050433  1.0561062  -2.6827111   1.9591612  -0.41369092]\n",
      "Epoch 200, Loss: 3239.594600\n",
      "  Learned weights: [ 0.08081089  1.050897   -2.6769364   1.951136   -0.4137024 ]\n",
      "Epoch 300, Loss: 2401.972352\n",
      "  Learned weights: [ 0.08120489  1.060201   -2.6874514   1.9623822  -0.41696826]\n",
      "Epoch 400, Loss: 2394.341438\n",
      "  Learned weights: [ 0.08165592  1.0568808  -2.6839674   1.9591674  -0.4148164 ]\n",
      "Saved sample_0012_x-5.00_+5.00_v1.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 13...\n",
      "Epoch 0, Loss: 2752.975772\n",
      "  Learned weights: [-0.07827099  1.333575   -2.5596948   1.4068705  -0.10409449]\n",
      "Epoch 100, Loss: 1882.604378\n",
      "  Learned weights: [ 0.10549852  0.998515   -2.6275325   1.9121526  -0.3937051 ]\n",
      "Epoch 200, Loss: 1661.033443\n",
      "  Learned weights: [ 0.10420857  1.0034813  -2.6365278   1.921455   -0.39430216]\n",
      "Epoch 300, Loss: 1872.122496\n",
      "  Learned weights: [ 0.10485075  1.0002952  -2.6312804   1.9177057  -0.39205188]\n",
      "Epoch 400, Loss: 1665.193861\n",
      "  Learned weights: [ 0.10492872  1.0012028  -2.6313236   1.9179997  -0.3932484 ]\n",
      "Saved sample_0013_x-5.00_+5.00_v1.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 14...\n",
      "Epoch 0, Loss: 3739.399055\n",
      "  Learned weights: [-0.08592995  1.3282266  -2.539872    1.3941293  -0.09798256]\n",
      "Epoch 100, Loss: 2732.233673\n",
      "  Learned weights: [ 0.11222015  0.9773806  -2.593916    1.8934921  -0.39146125]\n",
      "Epoch 200, Loss: 3552.780701\n",
      "  Learned weights: [ 0.11255785  0.97350645 -2.591748    1.8892381  -0.38855278]\n",
      "Epoch 300, Loss: 2820.034140\n",
      "  Learned weights: [ 0.11076244  0.9799892  -2.6010702   1.8974332  -0.39518163]\n",
      "Epoch 400, Loss: 4206.817639\n",
      "  Learned weights: [ 0.10857118  0.9847117  -2.613793    1.9093784  -0.39515775]\n",
      "Saved sample_0014_x-5.00_+5.00_v1.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 15...\n",
      "Epoch 0, Loss: 3522.427424\n",
      "  Learned weights: [-0.08326066  1.3355535  -2.5594926   1.4049339  -0.10012434]\n",
      "Epoch 100, Loss: 2620.919528\n",
      "  Learned weights: [ 0.09459478  1.0345073  -2.6690812   1.9411082  -0.40591618]\n",
      "Epoch 200, Loss: 3072.607353\n",
      "  Learned weights: [ 0.09588599  1.0429264  -2.679844    1.9480293  -0.4119481 ]\n",
      "Epoch 300, Loss: 2357.275934\n",
      "  Learned weights: [ 0.09560521  1.0400387  -2.674025    1.9450381  -0.4069387 ]\n",
      "Epoch 400, Loss: 3124.296388\n",
      "  Learned weights: [ 0.09444112  1.0381314  -2.672864    1.9466455  -0.4038686 ]\n",
      "Saved sample_0015_x-5.00_+5.00_v1.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 16...\n",
      "Epoch 0, Loss: 3807.009835\n",
      "  Learned weights: [-0.0846923   1.3350405  -2.5541713   1.4037167  -0.10054675]\n",
      "Epoch 100, Loss: 2406.224102\n",
      "  Learned weights: [ 0.09370296  1.0268323  -2.665973    1.961138   -0.41936693]\n",
      "Epoch 200, Loss: 2448.832729\n",
      "  Learned weights: [ 0.09466509  1.0286325  -2.6676185   1.9623829  -0.42174965]\n",
      "Epoch 300, Loss: 2378.489491\n",
      "  Learned weights: [ 0.09563737  1.029169   -2.6697824   1.962932   -0.422774  ]\n",
      "Epoch 400, Loss: 2348.758525\n",
      "  Learned weights: [ 0.0945546   1.0262954  -2.6636012   1.9594324  -0.41939467]\n",
      "Saved sample_0016_x-5.00_+5.00_v1.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 17...\n",
      "Epoch 0, Loss: 4073.452423\n",
      "  Learned weights: [-0.08799458  1.3456188  -2.5746024   1.4134856  -0.10028122]\n",
      "Epoch 100, Loss: 3617.982822\n",
      "  Learned weights: [ 0.07980346  1.0610868  -2.6990635   1.972657   -0.42166024]\n",
      "Epoch 200, Loss: 2609.295866\n",
      "  Learned weights: [ 0.08004449  1.0651052  -2.7049005   1.9797308  -0.42120686]\n",
      "Epoch 300, Loss: 3618.608192\n",
      "  Learned weights: [ 0.07986391  1.0621381  -2.699783    1.9749827  -0.41922987]\n",
      "Epoch 400, Loss: 2584.060788\n",
      "  Learned weights: [ 0.07916064  1.0642018  -2.7031474   1.9778578  -0.42152205]\n",
      "Saved sample_0017_x-5.00_+5.00_v1.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 18...\n",
      "Epoch 0, Loss: 2621.217123\n",
      "  Learned weights: [-0.07919446  1.3406172  -2.5766478   1.4196562  -0.10417373]\n",
      "Epoch 100, Loss: 1525.640750\n",
      "  Learned weights: [ 0.08800074  1.0410249  -2.6804197   1.9435128  -0.40021726]\n",
      "Epoch 200, Loss: 1514.892906\n",
      "  Learned weights: [ 0.08745318  1.0380912  -2.6740632   1.9411119  -0.39343822]\n",
      "Epoch 300, Loss: 1534.141559\n",
      "  Learned weights: [ 0.0882278  1.0388234 -2.6780999  1.9423256 -0.3980603]\n",
      "Epoch 400, Loss: 1729.585623\n",
      "  Learned weights: [ 0.08684681  1.0402367  -2.6811864   1.9460855  -0.39502403]\n",
      "Saved sample_0018_x-5.00_+5.00_v1.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 19...\n",
      "Epoch 0, Loss: 2907.407257\n",
      "  Learned weights: [-0.07699243  1.317377   -2.5252717   1.3845544  -0.09960407]\n",
      "Epoch 100, Loss: 2055.372509\n",
      "  Learned weights: [ 0.11880025  0.9688003  -2.5926173   1.8867294  -0.38234866]\n",
      "Epoch 200, Loss: 2649.067023\n",
      "  Learned weights: [ 0.11889353  0.9675646  -2.594067    1.8863547  -0.38204098]\n",
      "Epoch 300, Loss: 2972.054643\n",
      "  Learned weights: [ 0.11643425  0.97011554 -2.593898    1.8893154  -0.38257027]\n",
      "Epoch 400, Loss: 1945.039677\n",
      "  Learned weights: [ 0.11660995  0.9672021  -2.5924323   1.8878272  -0.3809651 ]\n",
      "Saved sample_0019_x-5.00_+5.00_v1.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 20...\n",
      "Epoch 0, Loss: 3826.157302\n",
      "  Learned weights: [-0.08140099  1.3313236  -2.554363    1.4016311  -0.10028999]\n",
      "Epoch 100, Loss: 2950.096746\n",
      "  Learned weights: [ 0.09761538  1.0367038  -2.6760533   1.9535631  -0.41224545]\n",
      "Epoch 200, Loss: 2865.615921\n",
      "  Learned weights: [ 0.09479249  1.0340685  -2.6756968   1.9536031  -0.409565  ]\n",
      "Epoch 300, Loss: 2396.277268\n",
      "  Learned weights: [ 0.09675982  1.0394504  -2.6799085   1.9564474  -0.41360146]\n",
      "Epoch 400, Loss: 3230.672117\n",
      "  Learned weights: [ 0.09717184  1.0332534  -2.6703944   1.9475524  -0.41144794]\n",
      "Saved sample_0020_x-5.00_+5.00_v1.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 21...\n",
      "Epoch 0, Loss: 4080.278801\n",
      "  Learned weights: [-0.07881228  1.3303077  -2.549559    1.4005382  -0.10224879]\n",
      "Epoch 100, Loss: 2455.459060\n",
      "  Learned weights: [ 0.1271184   0.95784754 -2.579403    1.8649131  -0.3737862 ]\n",
      "Epoch 200, Loss: 2359.375695\n",
      "  Learned weights: [ 0.12589672  0.95867425 -2.583349    1.8702793  -0.37263903]\n",
      "Epoch 300, Loss: 2058.541266\n",
      "  Learned weights: [ 0.1264851   0.95734733 -2.5770009   1.8649365  -0.37223795]\n",
      "Epoch 400, Loss: 2441.587296\n",
      "  Learned weights: [ 0.12700957  0.95681465 -2.577654    1.8641747  -0.37199423]\n",
      "Saved sample_0021_x-5.00_+5.00_v1.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 22...\n",
      "Epoch 0, Loss: 3675.816825\n",
      "  Learned weights: [-0.08382802  1.3297306  -2.5488837   1.4002868  -0.09873474]\n",
      "Epoch 100, Loss: 3365.589422\n",
      "  Learned weights: [ 0.1041424   1.0137951  -2.6425192   1.9122442  -0.38961324]\n",
      "Epoch 200, Loss: 2640.972014\n",
      "  Learned weights: [ 0.10660217  1.0080141  -2.6265595   1.8998523  -0.3877905 ]\n",
      "Epoch 300, Loss: 2612.015135\n",
      "  Learned weights: [ 0.10492107  1.0063937  -2.627929    1.9009001  -0.38459733]\n",
      "Epoch 400, Loss: 2716.033624\n",
      "  Learned weights: [ 0.10494728  1.0008073  -2.6216612   1.8928767  -0.38388917]\n",
      "Saved sample_0022_x-5.00_+5.00_v1.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 23...\n",
      "Epoch 0, Loss: 3471.445570\n",
      "  Learned weights: [-0.08743261  1.3442211  -2.5694668   1.4124093  -0.09959321]\n",
      "Epoch 100, Loss: 2302.926239\n",
      "  Learned weights: [ 0.09441793  1.0259173  -2.6512299   1.916038   -0.39189628]\n",
      "Epoch 200, Loss: 2729.501435\n",
      "  Learned weights: [ 0.09215845  1.02563    -2.6558478   1.9217237  -0.38602376]\n",
      "Epoch 300, Loss: 2542.669399\n",
      "  Learned weights: [ 0.09430909  1.0253989  -2.6507514   1.9161276  -0.39133635]\n",
      "Epoch 400, Loss: 2938.920780\n",
      "  Learned weights: [ 0.09283508  1.0278769  -2.657052    1.9238479  -0.38948634]\n",
      "Saved sample_0023_x-5.00_+5.00_v1.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 24...\n",
      "Epoch 0, Loss: 3364.513974\n",
      "  Learned weights: [-0.08012106  1.3371174  -2.5776486   1.4193934  -0.10327055]\n",
      "Epoch 100, Loss: 1957.139599\n",
      "  Learned weights: [ 0.0870774  1.0483228 -2.6825953  1.9641031 -0.4148639]\n",
      "Epoch 200, Loss: 1961.031329\n",
      "  Learned weights: [ 0.08745325  1.048319   -2.6870368   1.9663234  -0.41453654]\n",
      "Epoch 300, Loss: 1961.113653\n",
      "  Learned weights: [ 0.0861837   1.0496869  -2.6900983   1.9691286  -0.41594517]\n",
      "Epoch 400, Loss: 1972.979840\n",
      "  Learned weights: [ 0.08685371  1.0467088  -2.6831436   1.9629176  -0.41448504]\n",
      "Saved sample_0024_x-5.00_+5.00_v1.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 0...\n",
      "Epoch 0, Loss: 3632.018418\n",
      "  Learned weights: [-0.08490824  1.3272396  -2.5371516   1.3932936  -0.0985927 ]\n",
      "Epoch 100, Loss: 2789.967687\n",
      "  Learned weights: [ 0.101145    1.0083045  -2.6249402   1.9050199  -0.38867763]\n",
      "Epoch 200, Loss: 2332.711877\n",
      "  Learned weights: [ 0.10179278  1.0065211  -2.624968    1.9022074  -0.38986275]\n",
      "Epoch 300, Loss: 2308.377458\n",
      "  Learned weights: [ 0.10191279  1.007053   -2.6232934   1.9020321  -0.38929635]\n",
      "Epoch 400, Loss: 2729.956412\n",
      "  Learned weights: [ 0.10092477  1.0056769  -2.6239936   1.9028877  -0.38748246]\n",
      "Saved sample_0000_x-5.00_+5.00_v2.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 1...\n",
      "Epoch 0, Loss: 3142.046394\n",
      "  Learned weights: [-0.08019165  1.3313392  -2.5494888   1.3944869  -0.09658754]\n",
      "Epoch 100, Loss: 1892.524859\n",
      "  Learned weights: [ 0.11615743  0.9848322  -2.6100078   1.8895746  -0.38260305]\n",
      "Epoch 200, Loss: 1860.375195\n",
      "  Learned weights: [ 0.11647121  0.99164665 -2.6193993   1.897791   -0.38873115]\n",
      "Epoch 300, Loss: 1868.800234\n",
      "  Learned weights: [ 0.1153831   0.98814344 -2.6162853   1.896429   -0.38368315]\n",
      "Epoch 400, Loss: 2010.245835\n",
      "  Learned weights: [ 0.11534424  0.9888958  -2.6197994   1.898668   -0.3856073 ]\n",
      "Saved sample_0001_x-5.00_+5.00_v2.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 2...\n",
      "Epoch 0, Loss: 3863.219239\n",
      "  Learned weights: [-0.08269546  1.3306129  -2.5479944   1.3997067  -0.09909923]\n",
      "Epoch 100, Loss: 3361.484143\n",
      "  Learned weights: [ 0.09447814  1.0285014  -2.6611094   1.9445491  -0.40663671]\n",
      "Epoch 200, Loss: 2396.811279\n",
      "  Learned weights: [ 0.09268364  1.0359914  -2.6745422   1.9542909  -0.41370827]\n",
      "Epoch 300, Loss: 2143.223601\n",
      "  Learned weights: [ 0.09333898  1.0390599  -2.6782985   1.9576273  -0.41616845]\n",
      "Epoch 400, Loss: 2605.246709\n",
      "  Learned weights: [ 0.09239843  1.0347478  -2.6747017   1.954011   -0.4114752 ]\n",
      "Saved sample_0002_x-5.00_+5.00_v2.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 3...\n",
      "Epoch 0, Loss: 3668.875235\n",
      "  Learned weights: [-0.0787893   1.312046   -2.5158837   1.3821778  -0.10074007]\n",
      "Epoch 100, Loss: 2723.695975\n",
      "  Learned weights: [ 0.1440693   0.9223864  -2.5396798   1.8383054  -0.36440513]\n",
      "Epoch 200, Loss: 3712.458386\n",
      "  Learned weights: [ 0.1415283   0.9277701  -2.5509038   1.8479475  -0.36767134]\n",
      "Epoch 300, Loss: 2564.504439\n",
      "  Learned weights: [ 0.14626862  0.9211234  -2.5353432   1.8335147  -0.3652892 ]\n",
      "Epoch 400, Loss: 2612.225462\n",
      "  Learned weights: [ 0.14508392  0.9249163  -2.5443928   1.8408483  -0.36871523]\n",
      "Saved sample_0003_x-5.00_+5.00_v2.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 4...\n",
      "Epoch 0, Loss: 3971.131849\n",
      "  Learned weights: [-0.08260413  1.3265121  -2.5335472   1.3855847  -0.09855874]\n",
      "Epoch 100, Loss: 2249.092812\n",
      "  Learned weights: [ 0.11108059  0.9924546  -2.6111147   1.8949797  -0.38960892]\n",
      "Epoch 200, Loss: 2220.250572\n",
      "  Learned weights: [ 0.11025895  0.9940912  -2.6119893   1.896332   -0.39003474]\n",
      "Epoch 300, Loss: 2506.493118\n",
      "  Learned weights: [ 0.11036091  0.99111193 -2.6099787   1.8937147  -0.3889808 ]\n",
      "Epoch 400, Loss: 2445.009507\n",
      "  Learned weights: [ 0.10959139  0.9938355  -2.614609    1.897889   -0.3901153 ]\n",
      "Saved sample_0004_x-5.00_+5.00_v2.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 5...\n",
      "Epoch 0, Loss: 2660.085140\n",
      "  Learned weights: [-0.08365102  1.3347201  -2.55688     1.4042395  -0.1018543 ]\n",
      "Epoch 100, Loss: 2150.675855\n",
      "  Learned weights: [ 0.08426376  1.0434062  -2.6722026   1.9454665  -0.39616773]\n",
      "Epoch 200, Loss: 1621.817991\n",
      "  Learned weights: [ 0.08261161  1.0481997  -2.6827447   1.9497788  -0.40492737]\n",
      "Epoch 300, Loss: 1724.366140\n",
      "  Learned weights: [ 0.08214265  1.0475754  -2.680195    1.9508426  -0.39895013]\n",
      "Epoch 400, Loss: 1611.861249\n",
      "  Learned weights: [ 0.08283297  1.0459666  -2.678631    1.9476366  -0.4007923 ]\n",
      "Saved sample_0005_x-5.00_+5.00_v2.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 6...\n",
      "Epoch 0, Loss: 4390.100200\n",
      "  Learned weights: [-0.08296892  1.3236948  -2.5401266   1.3965614  -0.09724841]\n",
      "Epoch 100, Loss: 2827.686613\n",
      "  Learned weights: [ 0.11883411  0.97570765 -2.5959852   1.8768266  -0.3774444 ]\n",
      "Epoch 200, Loss: 3041.534730\n",
      "  Learned weights: [ 0.11703413  0.9778148  -2.6032004   1.885225   -0.376411  ]\n",
      "Epoch 300, Loss: 2827.776647\n",
      "  Learned weights: [ 0.11908771  0.9777449  -2.6011436   1.8812443  -0.3779847 ]\n",
      "Epoch 400, Loss: 3235.946212\n",
      "  Learned weights: [ 0.11764469  0.9770456  -2.602763    1.8830236  -0.37636906]\n",
      "Saved sample_0006_x-5.00_+5.00_v2.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 7...\n",
      "Epoch 0, Loss: 3852.218499\n",
      "  Learned weights: [-0.08683594  1.3378242  -2.567714    1.4165752  -0.10182098]\n",
      "Epoch 100, Loss: 2383.890994\n",
      "  Learned weights: [ 0.08165457  1.0651582  -2.6968856   1.9572705  -0.40840808]\n",
      "Epoch 200, Loss: 2380.768767\n",
      "  Learned weights: [ 0.08365966  1.0690095  -2.6977077   1.9582411  -0.41107148]\n",
      "Epoch 300, Loss: 2406.290431\n",
      "  Learned weights: [ 0.08251276  1.0688534  -2.698688    1.957531   -0.41329223]\n",
      "Epoch 400, Loss: 2379.779201\n",
      "  Learned weights: [ 0.08241769  1.0668778  -2.6989124   1.9576546  -0.40909702]\n",
      "Saved sample_0007_x-5.00_+5.00_v2.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 8...\n",
      "Epoch 0, Loss: 2943.478784\n",
      "  Learned weights: [-0.07698536  1.3294514  -2.5563986   1.4115049  -0.10676407]\n",
      "Epoch 100, Loss: 2031.167917\n",
      "  Learned weights: [ 0.10204362  1.0154083  -2.65644     1.9462521  -0.40827355]\n",
      "Epoch 200, Loss: 1907.735896\n",
      "  Learned weights: [ 0.10262852  1.015817   -2.6578486   1.9470459  -0.4077829 ]\n",
      "Epoch 300, Loss: 1919.057373\n",
      "  Learned weights: [ 0.10102355  1.0191872  -2.6642735   1.9535885  -0.40976456]\n",
      "Epoch 400, Loss: 2195.125874\n",
      "  Learned weights: [ 0.10027126  1.0175973  -2.6655285   1.9560566  -0.40532333]\n",
      "Saved sample_0008_x-5.00_+5.00_v2.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 9...\n",
      "Epoch 0, Loss: 2184.543533\n",
      "  Learned weights: [-0.07894672  1.3388205  -2.5750775   1.4160333  -0.10111576]\n",
      "Epoch 100, Loss: 1138.609508\n",
      "  Learned weights: [ 0.10085697  1.0147053  -2.6565533   1.9339108  -0.3932756 ]\n",
      "Epoch 200, Loss: 1142.492214\n",
      "  Learned weights: [ 0.10219319  1.0178064  -2.665843    1.9387357  -0.39448804]\n",
      "Epoch 300, Loss: 1170.241307\n",
      "  Learned weights: [ 0.1018227   1.0173512  -2.6607888   1.9373376  -0.39469802]\n",
      "Epoch 400, Loss: 1482.359012\n",
      "  Learned weights: [ 0.10024307  1.0177175  -2.6685526   1.938566   -0.40050784]\n",
      "Saved sample_0009_x-5.00_+5.00_v2.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 10...\n",
      "Epoch 0, Loss: 3623.208321\n",
      "  Learned weights: [-0.07910249  1.3308827  -2.5477293   1.3970915  -0.10386242]\n",
      "Epoch 100, Loss: 2249.086255\n",
      "  Learned weights: [ 0.11670312  0.9790223  -2.6089818   1.9012557  -0.39027333]\n",
      "Epoch 200, Loss: 3444.183043\n",
      "  Learned weights: [ 0.11748766  0.9807215  -2.6140907   1.9069518  -0.39043   ]\n",
      "Epoch 300, Loss: 2269.852842\n",
      "  Learned weights: [ 0.11649523  0.97772086 -2.606664    1.9002284  -0.388348  ]\n",
      "Epoch 400, Loss: 2281.388527\n",
      "  Learned weights: [ 0.11747407  0.97843033 -2.610982    1.9026856  -0.38820407]\n",
      "Saved sample_0010_x-5.00_+5.00_v2.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 11...\n",
      "Epoch 0, Loss: 3168.747546\n",
      "  Learned weights: [-0.08632843  1.3371897  -2.5528123   1.3935872  -0.09598164]\n",
      "Epoch 100, Loss: 2003.538009\n",
      "  Learned weights: [ 0.07778061  1.0797455  -2.713159    1.9814259  -0.42836782]\n",
      "Epoch 200, Loss: 1976.692712\n",
      "  Learned weights: [ 0.07729816  1.0819384  -2.717389    1.9846764  -0.42909372]\n",
      "Epoch 300, Loss: 2036.670696\n",
      "  Learned weights: [ 0.07736951  1.0785733  -2.712943    1.9803257  -0.4268053 ]\n",
      "Epoch 400, Loss: 1976.776173\n",
      "  Learned weights: [ 0.0770663   1.0811492  -2.7164948   1.9851767  -0.42712232]\n",
      "Saved sample_0011_x-5.00_+5.00_v2.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 12...\n",
      "Epoch 0, Loss: 3760.925039\n",
      "  Learned weights: [-0.08052942  1.3365052  -2.5710616   1.4167376  -0.10293464]\n",
      "Epoch 100, Loss: 2413.979456\n",
      "  Learned weights: [ 0.09818121  1.0247124  -2.6503782   1.9344125  -0.40728796]\n",
      "Epoch 200, Loss: 2403.309917\n",
      "  Learned weights: [ 0.09656642  1.0252496  -2.6596918   1.9404129  -0.4082321 ]\n",
      "Epoch 300, Loss: 2397.389064\n",
      "  Learned weights: [ 0.09534368  1.0265015  -2.662935    1.9434463  -0.40765515]\n",
      "Epoch 400, Loss: 2922.444398\n",
      "  Learned weights: [ 0.09747346  1.029815   -2.6644883   1.9448259  -0.40979153]\n",
      "Saved sample_0012_x-5.00_+5.00_v2.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 13...\n",
      "Epoch 0, Loss: 3799.868864\n",
      "  Learned weights: [-0.08058843  1.3242576  -2.5421588   1.3901339  -0.09639622]\n",
      "Epoch 100, Loss: 2579.165436\n",
      "  Learned weights: [ 0.11693788  0.9937394  -2.6267657   1.9167649  -0.40149385]\n",
      "Epoch 200, Loss: 2546.889528\n",
      "  Learned weights: [ 0.11606305  0.9937501  -2.6281848   1.9179806  -0.40046945]\n",
      "Epoch 300, Loss: 2574.241769\n",
      "  Learned weights: [ 0.11536448  0.998164   -2.6351511   1.9239542  -0.4038591 ]\n",
      "Epoch 400, Loss: 3084.715150\n",
      "  Learned weights: [ 0.11663243  0.9975274  -2.6352634   1.9224335  -0.40342018]\n",
      "Saved sample_0013_x-5.00_+5.00_v2.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 14...\n",
      "Epoch 0, Loss: 3097.407808\n",
      "  Learned weights: [-0.07900059  1.3272026  -2.547537    1.399944   -0.10038184]\n",
      "Epoch 100, Loss: 2048.162665\n",
      "  Learned weights: [ 0.11864333  0.97841364 -2.5999057   1.8851119  -0.38166654]\n",
      "Epoch 200, Loss: 2551.003313\n",
      "  Learned weights: [ 0.11756296  0.98080575 -2.6055036   1.890201   -0.3842849 ]\n",
      "Epoch 300, Loss: 2664.549913\n",
      "  Learned weights: [ 0.11714345  0.98457    -2.6134114   1.8953854  -0.38620523]\n",
      "Epoch 400, Loss: 2082.666310\n",
      "  Learned weights: [ 0.11893126  0.97814584 -2.6011624   1.8850608  -0.38197404]\n",
      "Saved sample_0014_x-5.00_+5.00_v2.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 15...\n",
      "Epoch 0, Loss: 3608.389055\n",
      "  Learned weights: [-0.08491004  1.3363732  -2.5577643   1.4038832  -0.09975805]\n",
      "Epoch 100, Loss: 1803.459884\n",
      "  Learned weights: [ 0.08588276  1.0539881  -2.6812134   1.9508579  -0.4123013 ]\n",
      "Epoch 200, Loss: 2194.067988\n",
      "  Learned weights: [ 0.08453733  1.0554539  -2.6894972   1.9561377  -0.41478822]\n",
      "Epoch 300, Loss: 1751.836168\n",
      "  Learned weights: [ 0.0855565  1.0568731 -2.6838539  1.9549615 -0.4122507]\n",
      "Epoch 400, Loss: 1956.203221\n",
      "  Learned weights: [ 0.08597111  1.0605414  -2.6893547   1.9609178  -0.4120053 ]\n",
      "Saved sample_0015_x-5.00_+5.00_v2.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 16...\n",
      "Epoch 0, Loss: 4578.286823\n",
      "  Learned weights: [-0.08481252  1.3402282  -2.5623274   1.4080583  -0.10263161]\n",
      "Epoch 100, Loss: 2266.126204\n",
      "  Learned weights: [ 0.09952935  1.0184574  -2.6579094   1.9504468  -0.41807324]\n",
      "Epoch 200, Loss: 2182.120798\n",
      "  Learned weights: [ 0.10084189  1.0196111  -2.65633     1.9521602  -0.41715896]\n",
      "Epoch 300, Loss: 2224.183715\n",
      "  Learned weights: [ 0.09924519  1.020297   -2.6640785   1.9586432  -0.41541445]\n",
      "Epoch 400, Loss: 2173.740964\n",
      "  Learned weights: [ 0.09848815  1.0226994  -2.6685317   1.9610002  -0.41972724]\n",
      "Saved sample_0016_x-5.00_+5.00_v2.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 17...\n",
      "Epoch 0, Loss: 3128.318858\n",
      "  Learned weights: [-0.08131134  1.3378831  -2.554923    1.3965418  -0.09928443]\n",
      "Epoch 100, Loss: 1968.857518\n",
      "  Learned weights: [ 0.10645133  1.003895   -2.6343977   1.9258507  -0.40115213]\n",
      "Epoch 200, Loss: 1937.067173\n",
      "  Learned weights: [ 0.10636466  1.0044914  -2.6351776   1.9245275  -0.40248218]\n",
      "Epoch 300, Loss: 1991.840067\n",
      "  Learned weights: [ 0.10656301  1.0048453  -2.639581    1.9262049  -0.403775  ]\n",
      "Epoch 400, Loss: 2325.831494\n",
      "  Learned weights: [ 0.10475002  1.0091963  -2.6458182   1.9336011  -0.40352592]\n",
      "Saved sample_0017_x-5.00_+5.00_v2.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 18...\n",
      "Epoch 0, Loss: 2736.590764\n",
      "  Learned weights: [-0.08374757  1.342954   -2.5724137   1.4135312  -0.10242376]\n",
      "Epoch 100, Loss: 1740.998286\n",
      "  Learned weights: [ 0.09462006  1.0280461  -2.662402    1.9384822  -0.40215668]\n",
      "Epoch 200, Loss: 1828.608162\n",
      "  Learned weights: [ 0.09256357  1.0331748  -2.6752372   1.9474776  -0.40726805]\n",
      "Epoch 300, Loss: 1830.712491\n",
      "  Learned weights: [ 0.09421913  1.0286119  -2.6609294   1.937999   -0.4013802 ]\n",
      "Epoch 400, Loss: 1739.556229\n",
      "  Learned weights: [ 0.09381225  1.031331   -2.6693237   1.9436973  -0.4038124 ]\n",
      "Saved sample_0018_x-5.00_+5.00_v2.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 19...\n",
      "Epoch 0, Loss: 3787.188740\n",
      "  Learned weights: [-0.08652388  1.3436563  -2.5826828   1.4274943  -0.10426449]\n",
      "Epoch 100, Loss: 2459.217617\n",
      "  Learned weights: [ 0.08181025  1.0589223  -2.6992536   1.9743042  -0.41980216]\n",
      "Epoch 200, Loss: 3523.251236\n",
      "  Learned weights: [ 0.08283938  1.0615714  -2.700657    1.9756918  -0.4196326 ]\n",
      "Epoch 300, Loss: 3273.191398\n",
      "  Learned weights: [ 0.08225577  1.0541095  -2.691796    1.9659593  -0.4190613 ]\n",
      "Epoch 400, Loss: 2910.645823\n",
      "  Learned weights: [ 0.08331623  1.0599899  -2.6989334   1.9730078  -0.42023334]\n",
      "Saved sample_0019_x-5.00_+5.00_v2.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 20...\n",
      "Epoch 0, Loss: 3939.458463\n",
      "  Learned weights: [-0.08566757  1.3228977  -2.5220082   1.378332   -0.09852859]\n",
      "Epoch 100, Loss: 3064.272335\n",
      "  Learned weights: [ 0.1091833  0.994565  -2.6249838  1.9172192 -0.3968524]\n",
      "Epoch 200, Loss: 2432.176427\n",
      "  Learned weights: [ 0.10899648  0.9936891  -2.624898    1.9168618  -0.3961006 ]\n",
      "Epoch 300, Loss: 3882.940483\n",
      "  Learned weights: [ 0.1065203   1.0038921  -2.6424875   1.9314748  -0.40263626]\n",
      "Epoch 400, Loss: 2678.941878\n",
      "  Learned weights: [ 0.1086889  0.9953257 -2.6256492  1.9179194 -0.3983678]\n",
      "Saved sample_0020_x-5.00_+5.00_v2.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 21...\n",
      "Epoch 0, Loss: 2729.002355\n",
      "  Learned weights: [-0.07720883  1.3326267  -2.5542202   1.4024333  -0.10517286]\n",
      "Epoch 100, Loss: 1882.829651\n",
      "  Learned weights: [ 0.11285217  0.991546   -2.6075046   1.8904647  -0.3881182 ]\n",
      "Epoch 200, Loss: 1772.910851\n",
      "  Learned weights: [ 0.11574488  0.98635787 -2.5986378   1.88227    -0.3867045 ]\n",
      "Epoch 300, Loss: 2090.028308\n",
      "  Learned weights: [ 0.1148801  0.9856926 -2.5988224  1.8823502 -0.3870019]\n",
      "Epoch 400, Loss: 1800.182671\n",
      "  Learned weights: [ 0.11331126  0.99216837 -2.608283    1.8910693  -0.3895096 ]\n",
      "Saved sample_0021_x-5.00_+5.00_v2.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 22...\n",
      "Epoch 0, Loss: 3922.420204\n",
      "  Learned weights: [-0.08814988  1.3385237  -2.5559206   1.405618   -0.09989782]\n",
      "Epoch 100, Loss: 3290.658204\n",
      "  Learned weights: [ 0.09258869  1.034925   -2.6717427   1.9485648  -0.41043213]\n",
      "Epoch 200, Loss: 2600.161485\n",
      "  Learned weights: [ 0.09303769  1.0357711  -2.6724777   1.9494182  -0.41043857]\n",
      "Epoch 300, Loss: 2611.882982\n",
      "  Learned weights: [ 0.09331069  1.0321153  -2.6659746   1.9445745  -0.40775687]\n",
      "Epoch 400, Loss: 3401.825956\n",
      "  Learned weights: [ 0.09174407  1.0389562  -2.6782916   1.956111   -0.4103003 ]\n",
      "Saved sample_0022_x-5.00_+5.00_v2.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 23...\n",
      "Epoch 0, Loss: 3611.977796\n",
      "  Learned weights: [-0.07969829  1.3353047  -2.5690415   1.4136789  -0.10343543]\n",
      "Epoch 100, Loss: 1714.400269\n",
      "  Learned weights: [ 0.0979702   1.0185312  -2.656057    1.9329013  -0.39584494]\n",
      "Epoch 200, Loss: 1692.790542\n",
      "  Learned weights: [ 0.09813572  1.021927   -2.660896    1.9353144  -0.40057102]\n",
      "Epoch 300, Loss: 1706.117159\n",
      "  Learned weights: [ 0.0988722   1.018694   -2.65638     1.933007   -0.39677212]\n",
      "Epoch 400, Loss: 1657.181049\n",
      "  Learned weights: [ 0.09800883  1.019203   -2.6567154   1.932952   -0.39663842]\n",
      "Saved sample_0023_x-5.00_+5.00_v2.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 24...\n",
      "Epoch 0, Loss: 3596.327688\n",
      "  Learned weights: [-0.09000466  1.3439128  -2.5645785   1.4109815  -0.10286235]\n",
      "Epoch 100, Loss: 2231.254754\n",
      "  Learned weights: [ 0.10100731  1.0150242  -2.6456327   1.9368117  -0.40680084]\n",
      "Epoch 200, Loss: 2258.285654\n",
      "  Learned weights: [ 0.09955052  1.0183403  -2.6569734   1.9440573  -0.4106016 ]\n",
      "Epoch 300, Loss: 2252.728817\n",
      "  Learned weights: [ 0.09884763  1.0148803  -2.6519282   1.9403548  -0.40618604]\n",
      "Epoch 400, Loss: 2247.750487\n",
      "  Learned weights: [ 0.09992374  1.0148512  -2.647405    1.9376228  -0.40701512]\n",
      "Saved sample_0024_x-5.00_+5.00_v2.00_T128_learnedTrue.npz\n",
      "Training Laplacian for sample 0...\n",
      "Epoch 0, Loss: 3087.109017\n",
      "  Learned weights: [-0.08885044  1.3350322  -2.5498168   1.3947358  -0.09485006]\n",
      "Epoch 100, Loss: 1993.844256\n",
      "  Learned weights: [ 0.08939691  1.0477414  -2.6739912   1.9417046  -0.4057245 ]\n",
      "Epoch 200, Loss: 2774.085534\n",
      "  Learned weights: [ 0.08902953  1.0398976  -2.6647403   1.9343457  -0.39997774]\n",
      "Epoch 300, Loss: 3367.949335\n",
      "  Learned weights: [ 0.08499715  1.0520253  -2.684316    1.9530505  -0.40546533]\n",
      "Epoch 400, Loss: 1967.395714\n",
      "  Learned weights: [ 0.08764364  1.0476019  -2.6765013   1.9437548  -0.4057814 ]\n",
      "Saved sample_0000_x-5.00_+5.00_v1.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 1...\n",
      "Epoch 0, Loss: 2982.924267\n",
      "  Learned weights: [-0.08183721  1.3374071  -2.5626166   1.4075482  -0.10023811]\n",
      "Epoch 100, Loss: 1814.817800\n",
      "  Learned weights: [ 0.09180682  1.0441519  -2.6860034   1.9550196  -0.40488675]\n",
      "Epoch 200, Loss: 1651.402454\n",
      "  Learned weights: [ 0.09033981  1.043254   -2.687551    1.9566721  -0.4040985 ]\n",
      "Epoch 300, Loss: 1960.654494\n",
      "  Learned weights: [ 0.08936069  1.0396537  -2.6810107   1.9538078  -0.3987614 ]\n",
      "Epoch 400, Loss: 2043.369604\n",
      "  Learned weights: [ 0.08940328  1.0452067  -2.693683    1.9620973  -0.40464187]\n",
      "Saved sample_0001_x-5.00_+5.00_v1.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 2...\n",
      "Epoch 0, Loss: 4379.639728\n",
      "  Learned weights: [-0.08618896  1.332264   -2.548019    1.3990873  -0.09758922]\n",
      "Epoch 100, Loss: 2690.685822\n",
      "  Learned weights: [ 0.09581321  1.0230563  -2.6520908   1.9400156  -0.40965524]\n",
      "Epoch 200, Loss: 2684.202941\n",
      "  Learned weights: [ 0.09695215  1.0237267  -2.6537216   1.9399508  -0.41100338]\n",
      "Epoch 300, Loss: 2653.409016\n",
      "  Learned weights: [ 0.0945603   1.0239583  -2.658591    1.9437416  -0.41035894]\n",
      "Epoch 400, Loss: 3237.564679\n",
      "  Learned weights: [ 0.09537702  1.0243696  -2.6545415   1.9428773  -0.41015312]\n",
      "Saved sample_0002_x-5.00_+5.00_v1.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 3...\n",
      "Epoch 0, Loss: 3437.416512\n",
      "  Learned weights: [-0.08418246  1.3376     -2.566845    1.4104357  -0.0998131 ]\n",
      "Epoch 100, Loss: 2140.549834\n",
      "  Learned weights: [ 0.07269254  1.0760176  -2.7085867   1.9635545  -0.4062514 ]\n",
      "Epoch 200, Loss: 2118.914572\n",
      "  Learned weights: [ 0.07203008  1.0822527  -2.7184165   1.9731944  -0.40963817]\n",
      "Epoch 300, Loss: 3996.575901\n",
      "  Learned weights: [ 0.07211799  1.0792967  -2.7142897   1.9703574  -0.406971  ]\n",
      "Epoch 400, Loss: 2144.233007\n",
      "  Learned weights: [ 0.07299029  1.0769064  -2.708522    1.9657332  -0.40663704]\n",
      "Saved sample_0003_x-5.00_+5.00_v1.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 4...\n",
      "Epoch 0, Loss: 3430.275102\n",
      "  Learned weights: [-0.08299354  1.3272904  -2.5446103   1.3972844  -0.09846512]\n",
      "Epoch 100, Loss: 2317.723805\n",
      "  Learned weights: [ 0.1112185   0.9980437  -2.6237001   1.8987906  -0.38869703]\n",
      "Epoch 200, Loss: 2867.963247\n",
      "  Learned weights: [ 0.11067013  0.9991085  -2.6251035   1.9029217  -0.3870762 ]\n",
      "Epoch 300, Loss: 2498.778864\n",
      "  Learned weights: [ 0.11101372  0.9999042  -2.6266398   1.9016477  -0.387502  ]\n",
      "Epoch 400, Loss: 2294.796764\n",
      "  Learned weights: [ 0.11063366  0.99930745 -2.6275232   1.9030842  -0.38689435]\n",
      "Saved sample_0004_x-5.00_+5.00_v1.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 5...\n",
      "Epoch 0, Loss: 2979.670185\n",
      "  Learned weights: [-0.08339019  1.330977   -2.5486188   1.4010323  -0.09945104]\n",
      "Epoch 100, Loss: 1776.484002\n",
      "  Learned weights: [ 0.10553192  1.0055761  -2.6384277   1.9198028  -0.3915135 ]\n",
      "Epoch 200, Loss: 1811.763421\n",
      "  Learned weights: [ 0.10483808  1.0083492  -2.646473    1.9237772  -0.39546356]\n",
      "Epoch 300, Loss: 1750.518476\n",
      "  Learned weights: [ 0.10646646  1.0048329  -2.6356711   1.9169903  -0.39063525]\n",
      "Epoch 400, Loss: 2343.600900\n",
      "  Learned weights: [ 0.10304974  1.0093324  -2.6480477   1.9279653  -0.39289635]\n",
      "Saved sample_0005_x-5.00_+5.00_v1.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 6...\n",
      "Epoch 0, Loss: 3803.414871\n",
      "  Learned weights: [-0.08616269  1.3335001  -2.5523498   1.4012978  -0.09767359]\n",
      "Epoch 100, Loss: 2340.310589\n",
      "  Learned weights: [ 0.10142767  1.0157198  -2.6467612   1.9267386  -0.39762735]\n",
      "Epoch 200, Loss: 2864.628936\n",
      "  Learned weights: [ 0.10115092  1.0143591  -2.644167    1.9247116  -0.396095  ]\n",
      "Epoch 300, Loss: 2314.318861\n",
      "  Learned weights: [ 0.10213529  1.0185225  -2.6500843   1.9281517  -0.40056443]\n",
      "Epoch 400, Loss: 2311.460246\n",
      "  Learned weights: [ 0.10170783  1.0174661  -2.6482413   1.931009   -0.39606208]\n",
      "Saved sample_0006_x-5.00_+5.00_v1.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 7...\n",
      "Epoch 0, Loss: 2720.913261\n",
      "  Learned weights: [-0.08129045  1.3292885  -2.5404384   1.3914973  -0.09959263]\n",
      "Epoch 100, Loss: 1666.009347\n",
      "  Learned weights: [ 0.10585029  1.0169181  -2.6485488   1.9337534  -0.40666196]\n",
      "Epoch 200, Loss: 1730.760465\n",
      "  Learned weights: [ 0.10369744  1.0185158  -2.6508963   1.9347304  -0.40758744]\n",
      "Epoch 300, Loss: 1686.860000\n",
      "  Learned weights: [ 0.10450839  1.0210004  -2.6533775   1.9387487  -0.40905806]\n",
      "Epoch 400, Loss: 1672.877412\n",
      "  Learned weights: [ 0.10326143  1.0203717  -2.6540616   1.9381677  -0.4085983 ]\n",
      "Saved sample_0007_x-5.00_+5.00_v1.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 8...\n",
      "Epoch 0, Loss: 4469.148380\n",
      "  Learned weights: [-0.07791348  1.3381586  -2.5673132   1.4106002  -0.10339466]\n",
      "Epoch 100, Loss: 2197.887833\n",
      "  Learned weights: [ 0.07336902  1.0857736  -2.7196825   1.9799949  -0.41885173]\n",
      "Epoch 200, Loss: 2725.368620\n",
      "  Learned weights: [ 0.07373441  1.0892956  -2.7258544   1.9847759  -0.42041624]\n",
      "Epoch 300, Loss: 2165.621442\n",
      "  Learned weights: [ 0.07372136  1.0855763  -2.7204337   1.9795607  -0.41971934]\n",
      "Epoch 400, Loss: 2197.691417\n",
      "  Learned weights: [ 0.07171206  1.0851232  -2.7217934   1.9794928  -0.42062795]\n",
      "Saved sample_0008_x-5.00_+5.00_v1.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 9...\n",
      "Epoch 0, Loss: 2925.630208\n",
      "  Learned weights: [-0.08122928  1.3437946  -2.5737886   1.4122076  -0.10209651]\n",
      "Epoch 100, Loss: 1899.577379\n",
      "  Learned weights: [ 0.08933684  1.0437958  -2.6886344   1.9684969  -0.413285  ]\n",
      "Epoch 200, Loss: 1980.554853\n",
      "  Learned weights: [ 0.08763     1.0398692  -2.682647    1.963669   -0.41169962]\n",
      "Epoch 300, Loss: 1836.825937\n",
      "  Learned weights: [ 0.08838242  1.0408142  -2.684742    1.9631244  -0.41256824]\n",
      "Epoch 400, Loss: 1844.155090\n",
      "  Learned weights: [ 0.08804841  1.0417287  -2.6883147   1.9661121  -0.41318807]\n",
      "Saved sample_0009_x-5.00_+5.00_v1.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 10...\n",
      "Epoch 0, Loss: 3365.367111\n",
      "  Learned weights: [-0.08078295  1.3269385  -2.5430565   1.3984793  -0.10155958]\n",
      "Epoch 100, Loss: 2620.934523\n",
      "  Learned weights: [ 0.11362632  0.98398614 -2.6083488   1.8847607  -0.3762126 ]\n",
      "Epoch 200, Loss: 2262.239051\n",
      "  Learned weights: [ 0.11595729  0.98601156 -2.6111174   1.8843501  -0.37747946]\n",
      "Epoch 300, Loss: 2217.193963\n",
      "  Learned weights: [ 0.11579518  0.98754424 -2.613795    1.8866464  -0.37732965]\n",
      "Epoch 400, Loss: 2210.467613\n",
      "  Learned weights: [ 0.11715759  0.9871001  -2.6080167   1.8817726  -0.37949136]\n",
      "Saved sample_0010_x-5.00_+5.00_v1.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 11...\n",
      "Epoch 0, Loss: 3325.044031\n",
      "  Learned weights: [-0.08759128  1.3481799  -2.5678167   1.4058008  -0.0993769 ]\n",
      "Epoch 100, Loss: 1913.613304\n",
      "  Learned weights: [ 0.09457144  1.0240737  -2.6513274   1.9243786  -0.39339665]\n",
      "Epoch 200, Loss: 2677.806452\n",
      "  Learned weights: [ 0.09283911  1.0274067  -2.661325    1.9294862  -0.3968986 ]\n",
      "Epoch 300, Loss: 2288.601483\n",
      "  Learned weights: [ 0.09221284  1.0261192  -2.661432    1.9314835  -0.39717442]\n",
      "Epoch 400, Loss: 1890.074070\n",
      "  Learned weights: [ 0.09234041  1.022219   -2.649185    1.9222423  -0.39428276]\n",
      "Saved sample_0011_x-5.00_+5.00_v1.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 12...\n",
      "Epoch 0, Loss: 2783.166656\n",
      "  Learned weights: [-0.08186564  1.3329898  -2.5502794   1.3978924  -0.09874489]\n",
      "Epoch 100, Loss: 2340.741012\n",
      "  Learned weights: [ 0.08579442  1.0475544  -2.6868107   1.9664134  -0.42064786]\n",
      "Epoch 200, Loss: 2358.249627\n",
      "  Learned weights: [ 0.0870983   1.0421311  -2.6767128   1.9589692  -0.41619438]\n",
      "Epoch 300, Loss: 2561.373299\n",
      "  Learned weights: [ 0.08600258  1.0472142  -2.6854365   1.9689059  -0.415118  ]\n",
      "Epoch 400, Loss: 1988.561762\n",
      "  Learned weights: [ 0.08834159  1.0375321  -2.6720457   1.9541384  -0.41485956]\n",
      "Saved sample_0012_x-5.00_+5.00_v1.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 13...\n",
      "Epoch 0, Loss: 3280.255948\n",
      "  Learned weights: [-0.08094928  1.3348243  -2.5563092   1.4041488  -0.10108205]\n",
      "Epoch 100, Loss: 2027.059698\n",
      "  Learned weights: [ 0.10130288  1.0211388  -2.6554425   1.9337424  -0.4073648 ]\n",
      "Epoch 200, Loss: 2023.725555\n",
      "  Learned weights: [ 0.10271951  1.0209869  -2.6566045   1.9354557  -0.40472046]\n",
      "Epoch 300, Loss: 2057.381085\n",
      "  Learned weights: [ 0.10225019  1.0199006  -2.6533353   1.9335493  -0.40368456]\n",
      "Epoch 400, Loss: 2099.351257\n",
      "  Learned weights: [ 0.10133564  1.0207165  -2.6582706   1.9367467  -0.40337932]\n",
      "Saved sample_0013_x-5.00_+5.00_v1.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 14...\n",
      "Epoch 0, Loss: 2571.343089\n",
      "  Learned weights: [-0.08537436  1.3491738  -2.583355    1.4203614  -0.10163805]\n",
      "Epoch 100, Loss: 2194.401784\n",
      "  Learned weights: [ 0.07475454  1.0658141  -2.6967256   1.9665968  -0.40917316]\n",
      "Epoch 200, Loss: 1414.454472\n",
      "  Learned weights: [ 0.07591501  1.0707362  -2.704604    1.9722848  -0.41283193]\n",
      "Epoch 300, Loss: 1568.829712\n",
      "  Learned weights: [ 0.07536194  1.0715789  -2.705369    1.9749478  -0.41130462]\n",
      "Epoch 400, Loss: 1468.872548\n",
      "  Learned weights: [ 0.07556822  1.0715171  -2.7085207   1.9719571  -0.4154231 ]\n",
      "Saved sample_0014_x-5.00_+5.00_v1.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 15...\n",
      "Epoch 0, Loss: 4451.957971\n",
      "  Learned weights: [-0.08113799  1.331302   -2.5498183   1.4009068  -0.10737816]\n",
      "Epoch 100, Loss: 2182.724450\n",
      "  Learned weights: [ 0.1054698   1.0056624  -2.6223996   1.9081789  -0.39711043]\n",
      "Epoch 200, Loss: 3298.658190\n",
      "  Learned weights: [ 0.10450576  1.0082351  -2.62982     1.9133098  -0.39803746]\n",
      "Epoch 300, Loss: 2667.550760\n",
      "  Learned weights: [ 0.10570259  1.0061048  -2.6248884   1.9092562  -0.3977568 ]\n",
      "Epoch 400, Loss: 2691.588595\n",
      "  Learned weights: [ 0.10546796  1.0074602  -2.6272554   1.9108622  -0.39851746]\n",
      "Saved sample_0015_x-5.00_+5.00_v1.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 16...\n",
      "Epoch 0, Loss: 4503.103886\n",
      "  Learned weights: [-0.08465156  1.3367934  -2.551036    1.394891   -0.0982059 ]\n",
      "Epoch 100, Loss: 2434.571669\n",
      "  Learned weights: [ 0.09828842  1.0285246  -2.6583505   1.9286987  -0.4003528 ]\n",
      "Epoch 200, Loss: 2480.693251\n",
      "  Learned weights: [ 0.10130163  1.0217397  -2.648604    1.9202367  -0.39627567]\n",
      "Epoch 300, Loss: 2439.482397\n",
      "  Learned weights: [ 0.10007334  1.028608   -2.6581857   1.9282721  -0.39999515]\n",
      "Epoch 400, Loss: 2916.463735\n",
      "  Learned weights: [ 0.1000941  1.0250772 -2.655714   1.9268414 -0.3979954]\n",
      "Saved sample_0016_x-5.00_+5.00_v1.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 17...\n",
      "Epoch 0, Loss: 3266.551259\n",
      "  Learned weights: [-0.08081196  1.3325318  -2.5511534   1.4018142  -0.10305747]\n",
      "Epoch 100, Loss: 2152.305637\n",
      "  Learned weights: [ 0.11288583  0.9895842  -2.6185539   1.9015543  -0.38711408]\n",
      "Epoch 200, Loss: 2017.674292\n",
      "  Learned weights: [ 0.11394987  0.98912877 -2.6153436   1.9004205  -0.38622734]\n",
      "Epoch 300, Loss: 2025.144099\n",
      "  Learned weights: [ 0.11411218  0.99164027 -2.6240168   1.9035361  -0.39016125]\n",
      "Epoch 400, Loss: 2439.227065\n",
      "  Learned weights: [ 0.11614151  0.9903205  -2.6169403   1.8999997  -0.38879105]\n",
      "Saved sample_0017_x-5.00_+5.00_v1.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 18...\n",
      "Epoch 0, Loss: 4312.387645\n",
      "  Learned weights: [-0.08109048  1.331303   -2.5605674   1.408919   -0.1031737 ]\n",
      "Epoch 100, Loss: 2680.670284\n",
      "  Learned weights: [ 0.09839782  1.0344337  -2.6529465   1.915897   -0.39503756]\n",
      "Epoch 200, Loss: 2568.380659\n",
      "  Learned weights: [ 0.09846782  1.035861   -2.657618    1.9177238  -0.39720935]\n",
      "Epoch 300, Loss: 2688.175521\n",
      "  Learned weights: [ 0.09681527  1.0304304  -2.6511304   1.9142655  -0.39467913]\n",
      "Epoch 400, Loss: 2363.373035\n",
      "  Learned weights: [ 0.09817789  1.0360085  -2.656749    1.9180061  -0.3965983 ]\n",
      "Saved sample_0018_x-5.00_+5.00_v1.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 19...\n",
      "Epoch 0, Loss: 3367.062120\n",
      "  Learned weights: [-0.0797943   1.3147738  -2.527758    1.3880064  -0.09771816]\n",
      "Epoch 100, Loss: 2596.685958\n",
      "  Learned weights: [ 0.13295734  0.95010215 -2.561859    1.8431772  -0.3662823 ]\n",
      "Epoch 200, Loss: 2303.699814\n",
      "  Learned weights: [ 0.13499929  0.95605385 -2.5693238   1.8477665  -0.3715266 ]\n",
      "Epoch 300, Loss: 2198.489716\n",
      "  Learned weights: [ 0.13303913  0.9546828  -2.566966    1.8468536  -0.3698132 ]\n",
      "Epoch 400, Loss: 2401.468125\n",
      "  Learned weights: [ 0.13262819  0.9507731  -2.5600526   1.8433648  -0.3670283 ]\n",
      "Saved sample_0019_x-5.00_+5.00_v1.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 20...\n",
      "Epoch 0, Loss: 3867.357785\n",
      "  Learned weights: [-0.08461166  1.3284703  -2.5477285   1.4005746  -0.09737644]\n",
      "Epoch 100, Loss: 4313.167814\n",
      "  Learned weights: [ 0.12257223  0.97798127 -2.5942416   1.870248   -0.37684402]\n",
      "Epoch 200, Loss: 4283.346368\n",
      "  Learned weights: [ 0.12236177  0.9779772  -2.5953286   1.869681   -0.3776955 ]\n",
      "Epoch 300, Loss: 2806.415260\n",
      "  Learned weights: [ 0.12573373  0.9704206  -2.5790527   1.8574826  -0.37366232]\n",
      "Epoch 400, Loss: 2753.210647\n",
      "  Learned weights: [ 0.12688482  0.975198   -2.590285    1.8634971  -0.37801307]\n",
      "Saved sample_0020_x-5.00_+5.00_v1.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 21...\n",
      "Epoch 0, Loss: 2562.619615\n",
      "  Learned weights: [-0.08054271  1.331495   -2.5542743   1.4034181  -0.1000696 ]\n",
      "Epoch 100, Loss: 1604.389469\n",
      "  Learned weights: [ 0.10286702  1.0090257  -2.647155    1.9287251  -0.3961634 ]\n",
      "Epoch 200, Loss: 1485.968355\n",
      "  Learned weights: [ 0.10166454  1.0081513  -2.646555    1.9289408  -0.39394456]\n",
      "Epoch 300, Loss: 1462.381119\n",
      "  Learned weights: [ 0.10258363  1.0090579  -2.6490126   1.9291579  -0.39617363]\n",
      "Epoch 400, Loss: 1448.553288\n",
      "  Learned weights: [ 0.10485715  1.0083257  -2.6436253   1.9244322  -0.396636  ]\n",
      "Saved sample_0021_x-5.00_+5.00_v1.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 22...\n",
      "Epoch 0, Loss: 2675.352119\n",
      "  Learned weights: [-0.08175905  1.340264   -2.566363    1.4080857  -0.10171614]\n",
      "Epoch 100, Loss: 1495.937917\n",
      "  Learned weights: [ 0.08405323  1.0554163  -2.6990511   1.9633166  -0.41309452]\n",
      "Epoch 200, Loss: 1503.980997\n",
      "  Learned weights: [ 0.08680911  1.0523939  -2.6858993   1.9555284  -0.40889198]\n",
      "Epoch 300, Loss: 1493.441096\n",
      "  Learned weights: [ 0.0846127   1.0488268  -2.6821246   1.951435   -0.40740094]\n",
      "Epoch 400, Loss: 1746.282310\n",
      "  Learned weights: [ 0.08717485  1.0544584  -2.692552    1.959073   -0.41077957]\n",
      "Saved sample_0022_x-5.00_+5.00_v1.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 23...\n",
      "Epoch 0, Loss: 2939.408069\n",
      "  Learned weights: [-0.08651061  1.3458582  -2.5721276   1.4138899  -0.10064588]\n",
      "Epoch 100, Loss: 2224.915855\n",
      "  Learned weights: [ 0.09232476  1.03595    -2.6794062   1.9492153  -0.40355211]\n",
      "Epoch 200, Loss: 1541.588020\n",
      "  Learned weights: [ 0.09347472  1.0350493  -2.674948    1.9455942  -0.40447345]\n",
      "Epoch 300, Loss: 2621.976045\n",
      "  Learned weights: [ 0.09003235  1.0346857  -2.680512    1.9483975  -0.4032624 ]\n",
      "Epoch 400, Loss: 1503.149918\n",
      "  Learned weights: [ 0.09218936  1.0311335  -2.6680317   1.9420664  -0.40037024]\n",
      "Saved sample_0023_x-5.00_+5.00_v1.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 24...\n",
      "Epoch 0, Loss: 3390.038115\n",
      "  Learned weights: [-0.08707806  1.343182   -2.5663114   1.4085609  -0.09925431]\n",
      "Epoch 100, Loss: 2277.339901\n",
      "  Learned weights: [ 0.08372316  1.0559392  -2.6993842   1.972093   -0.41504648]\n",
      "Epoch 200, Loss: 2264.142180\n",
      "  Learned weights: [ 0.0845056  1.0518595 -2.6911185  1.9644846 -0.4131511]\n",
      "Epoch 300, Loss: 2266.961267\n",
      "  Learned weights: [ 0.08549845  1.0577883  -2.697648    1.9713634  -0.41687718]\n",
      "Epoch 400, Loss: 2967.990515\n",
      "  Learned weights: [ 0.08451736  1.0591606  -2.703427    1.9768724  -0.41633227]\n",
      "Saved sample_0024_x-5.00_+5.00_v1.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 0...\n",
      "Epoch 0, Loss: 2848.998133\n",
      "  Learned weights: [-0.07862276  1.327187   -2.5434997   1.395857   -0.10027251]\n",
      "Epoch 100, Loss: 3289.801906\n",
      "  Learned weights: [ 0.11417415  0.98619765 -2.611071    1.8884802  -0.3800183 ]\n",
      "Epoch 200, Loss: 1940.254975\n",
      "  Learned weights: [ 0.11485175  0.98659414 -2.6099973   1.8864758  -0.37997237]\n",
      "Epoch 300, Loss: 1957.152224\n",
      "  Learned weights: [ 0.11566658  0.98773926 -2.6126783   1.8879265  -0.38208854]\n",
      "Epoch 400, Loss: 1929.704037\n",
      "  Learned weights: [ 0.11415224  0.9845273  -2.6079972   1.886117   -0.3766154 ]\n",
      "Saved sample_0000_x-5.00_+5.00_v2.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 1...\n",
      "Epoch 0, Loss: 4160.027650\n",
      "  Learned weights: [-0.08159228  1.3164715  -2.533818    1.3983217  -0.10015496]\n",
      "Epoch 100, Loss: 2741.274125\n",
      "  Learned weights: [ 0.12585214  0.94870365 -2.5815277   1.8850628  -0.37865368]\n",
      "Epoch 200, Loss: 3678.609194\n",
      "  Learned weights: [ 0.12596019  0.947568   -2.5814257   1.8844777  -0.37824973]\n",
      "Epoch 300, Loss: 2759.883645\n",
      "  Learned weights: [ 0.1277772   0.95125896 -2.582398    1.8858292  -0.38106135]\n",
      "Epoch 400, Loss: 2739.681180\n",
      "  Learned weights: [ 0.12727888  0.9522701  -2.586981    1.888552   -0.38325107]\n",
      "Saved sample_0001_x-5.00_+5.00_v2.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 2...\n",
      "Epoch 0, Loss: 3179.394095\n",
      "  Learned weights: [-0.08287123  1.339237   -2.5542486   1.3950394  -0.0981762 ]\n",
      "Epoch 100, Loss: 1797.387869\n",
      "  Learned weights: [ 0.0535183   1.1194191  -2.7621028   2.023545   -0.43522593]\n",
      "Epoch 200, Loss: 2224.983371\n",
      "  Learned weights: [ 0.05266187  1.1168114  -2.7609882   2.0187547  -0.43727836]\n",
      "Epoch 300, Loss: 2511.226649\n",
      "  Learned weights: [ 0.05493486  1.1181695  -2.7591152   2.0201137  -0.43599892]\n",
      "Epoch 400, Loss: 1803.961985\n",
      "  Learned weights: [ 0.05483157  1.1206864  -2.7627208   2.0211618  -0.44012305]\n",
      "Saved sample_0002_x-5.00_+5.00_v2.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 3...\n",
      "Epoch 0, Loss: 3518.387839\n",
      "  Learned weights: [-0.07742705  1.33245    -2.5579758   1.40403    -0.10374035]\n",
      "Epoch 100, Loss: 1736.761768\n",
      "  Learned weights: [ 0.09488757  1.0309391  -2.6689992   1.9418762  -0.40433756]\n",
      "Epoch 200, Loss: 1871.241174\n",
      "  Learned weights: [ 0.09225316  1.0329461  -2.6739428   1.9479582  -0.40300652]\n",
      "Epoch 300, Loss: 2100.145094\n",
      "  Learned weights: [ 0.09369548  1.0356321  -2.6786878   1.9501201  -0.40585703]\n",
      "Epoch 400, Loss: 2124.260489\n",
      "  Learned weights: [ 0.09446432  1.0340967  -2.6715906   1.9437702  -0.4065529 ]\n",
      "Saved sample_0003_x-5.00_+5.00_v2.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 4...\n",
      "Epoch 0, Loss: 3003.890615\n",
      "  Learned weights: [-0.08118018  1.3408272  -2.5663607   1.4122795  -0.10455994]\n",
      "Epoch 100, Loss: 1807.228076\n",
      "  Learned weights: [ 0.10671412  1.0026379  -2.6425545   1.9291646  -0.3991595 ]\n",
      "Epoch 200, Loss: 2647.324478\n",
      "  Learned weights: [ 0.10780055  1.004297   -2.638938    1.927278   -0.4006171 ]\n",
      "Epoch 300, Loss: 2384.404895\n",
      "  Learned weights: [ 0.10730895  1.004558   -2.6431673   1.9295837  -0.4008663 ]\n",
      "Epoch 400, Loss: 2324.755271\n",
      "  Learned weights: [ 0.10638598  1.0018904  -2.6404607   1.9271716  -0.40073305]\n",
      "Saved sample_0004_x-5.00_+5.00_v2.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 5...\n",
      "Epoch 0, Loss: 3858.195968\n",
      "  Learned weights: [-0.07809705  1.3167763  -2.5203576   1.3795767  -0.09961657]\n",
      "Epoch 100, Loss: 2302.278470\n",
      "  Learned weights: [ 0.1251257  0.9650835 -2.59846    1.8971901 -0.3881159]\n",
      "Epoch 200, Loss: 2081.417661\n",
      "  Learned weights: [ 0.12337154  0.9619711  -2.6000009   1.8963912  -0.3873997 ]\n",
      "Epoch 300, Loss: 2473.943373\n",
      "  Learned weights: [ 0.12407388  0.9634775  -2.5981522   1.8961068  -0.38716   ]\n",
      "Epoch 400, Loss: 2629.581207\n",
      "  Learned weights: [ 0.12382795  0.9679407  -2.606338    1.9026816  -0.3895874 ]\n",
      "Saved sample_0005_x-5.00_+5.00_v2.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 6...\n",
      "Epoch 0, Loss: 3119.451234\n",
      "  Learned weights: [-0.07635037  1.3319025  -2.562368    1.4116793  -0.10583106]\n",
      "Epoch 100, Loss: 1956.121207\n",
      "  Learned weights: [ 0.10919422  0.9986324  -2.6259396   1.9091196  -0.3927593 ]\n",
      "Epoch 200, Loss: 1931.769745\n",
      "  Learned weights: [ 0.10758066  1.0009191  -2.6326103   1.915215   -0.3938623 ]\n",
      "Epoch 300, Loss: 2644.144353\n",
      "  Learned weights: [ 0.10724623  0.9977833  -2.6292865   1.9133556  -0.39136842]\n",
      "Epoch 400, Loss: 2530.502521\n",
      "  Learned weights: [ 0.10775331  1.0005095  -2.632751    1.9151915  -0.39314282]\n",
      "Saved sample_0006_x-5.00_+5.00_v2.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 7...\n",
      "Epoch 0, Loss: 3540.851836\n",
      "  Learned weights: [-0.08533383  1.3446927  -2.5676124   1.4143304  -0.1084222 ]\n",
      "Epoch 100, Loss: 1564.663142\n",
      "  Learned weights: [ 0.09163377  1.0321641  -2.6584363   1.9481117  -0.4120324 ]\n",
      "Epoch 200, Loss: 2099.229427\n",
      "  Learned weights: [ 0.09066758  1.0326966  -2.66467     1.9510698  -0.41196796]\n",
      "Epoch 300, Loss: 1559.701118\n",
      "  Learned weights: [ 0.09380326  1.031403   -2.659556    1.9458727  -0.41480875]\n",
      "Epoch 400, Loss: 1557.368151\n",
      "  Learned weights: [ 0.09175152  1.0307226  -2.6607416   1.947489   -0.41272035]\n",
      "Saved sample_0007_x-5.00_+5.00_v2.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 8...\n",
      "Epoch 0, Loss: 2843.533162\n",
      "  Learned weights: [-0.07874304  1.3356686  -2.5659494   1.4106834  -0.10222227]\n",
      "Epoch 100, Loss: 1913.663748\n",
      "  Learned weights: [ 0.11573759  0.9819766  -2.6120198   1.9022241  -0.38826728]\n",
      "Epoch 200, Loss: 1831.509615\n",
      "  Learned weights: [ 0.11525787  0.9834849  -2.6161637   1.9041032  -0.38996047]\n",
      "Epoch 300, Loss: 1913.538452\n",
      "  Learned weights: [ 0.11524473  0.9831931  -2.615622    1.9055576  -0.38834447]\n",
      "Epoch 400, Loss: 1992.932476\n",
      "  Learned weights: [ 0.11504854  0.98147655 -2.6108706   1.9001976  -0.3887468 ]\n",
      "Saved sample_0008_x-5.00_+5.00_v2.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 9...\n",
      "Epoch 0, Loss: 3349.001830\n",
      "  Learned weights: [-0.07933755  1.3211792  -2.5451517   1.397913   -0.09775263]\n",
      "Epoch 100, Loss: 2594.363508\n",
      "  Learned weights: [ 0.1249398  0.9622127 -2.6029942  1.8976121 -0.3837373]\n",
      "Epoch 200, Loss: 2096.653455\n",
      "  Learned weights: [ 0.1248646   0.96076256 -2.6004884   1.8952248  -0.3836189 ]\n",
      "Epoch 300, Loss: 2582.877523\n",
      "  Learned weights: [ 0.12476079  0.9666927  -2.6073387   1.9026318  -0.38553375]\n",
      "Epoch 400, Loss: 2085.051444\n",
      "  Learned weights: [ 0.1265887   0.96256113 -2.598151    1.8949224  -0.3842533 ]\n",
      "Saved sample_0009_x-5.00_+5.00_v2.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 10...\n",
      "Epoch 0, Loss: 2717.111025\n",
      "  Learned weights: [-0.0832554   1.3319093  -2.5506496   1.4009213  -0.10061784]\n",
      "Epoch 100, Loss: 1744.956513\n",
      "  Learned weights: [ 0.11019377  0.9886554  -2.6256144   1.9156852  -0.38870874]\n",
      "Epoch 200, Loss: 1764.898154\n",
      "  Learned weights: [ 0.11103702  0.9878539  -2.6219757   1.911343   -0.3911703 ]\n",
      "Epoch 300, Loss: 1966.185467\n",
      "  Learned weights: [ 0.11063091  0.99110913 -2.629863    1.9179887  -0.39066708]\n",
      "Epoch 400, Loss: 1901.450688\n",
      "  Learned weights: [ 0.11100747  0.9865089  -2.6164126   1.909056   -0.38839066]\n",
      "Saved sample_0010_x-5.00_+5.00_v2.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 11...\n",
      "Epoch 0, Loss: 3623.851658\n",
      "  Learned weights: [-0.07989696  1.3259871  -2.5375147   1.3906854  -0.09933457]\n",
      "Epoch 100, Loss: 2487.797394\n",
      "  Learned weights: [ 0.12106333  0.9685411  -2.6011121   1.8915956  -0.38122767]\n",
      "Epoch 200, Loss: 3054.327279\n",
      "  Learned weights: [ 0.1233253   0.9693448  -2.6015697   1.8909372  -0.38492084]\n",
      "Epoch 300, Loss: 2468.193849\n",
      "  Learned weights: [ 0.12282926  0.9703648  -2.5998385   1.8897822  -0.38484553]\n",
      "Epoch 400, Loss: 2581.402893\n",
      "  Learned weights: [ 0.12228773  0.96876836 -2.6002822   1.8892565  -0.38179913]\n",
      "Saved sample_0011_x-5.00_+5.00_v2.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 12...\n",
      "Epoch 0, Loss: 2965.744695\n",
      "  Learned weights: [-0.08616366  1.341092   -2.559389    1.4036539  -0.10017738]\n",
      "Epoch 100, Loss: 2122.280949\n",
      "  Learned weights: [ 0.10831468  1.0005552  -2.6273272   1.9103305  -0.3934903 ]\n",
      "Epoch 200, Loss: 2010.002518\n",
      "  Learned weights: [ 0.10783426  1.0007348  -2.627996    1.912198   -0.39041784]\n",
      "Epoch 300, Loss: 2748.797486\n",
      "  Learned weights: [ 0.10728189  1.00181    -2.631772    1.9147142  -0.39255118]\n",
      "Epoch 400, Loss: 2438.988841\n",
      "  Learned weights: [ 0.10858615  0.9996283  -2.627258    1.9087883  -0.3924792 ]\n",
      "Saved sample_0012_x-5.00_+5.00_v2.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 13...\n",
      "Epoch 0, Loss: 3336.815598\n",
      "  Learned weights: [-0.08353233  1.3312249  -2.5442336   1.3959854  -0.09933937]\n",
      "Epoch 100, Loss: 2220.889374\n",
      "  Learned weights: [ 0.11916508  0.9830281  -2.6124072   1.8982161  -0.38963827]\n",
      "Epoch 200, Loss: 2185.381506\n",
      "  Learned weights: [ 0.11872227  0.98176104 -2.6113508   1.8972839  -0.3888922 ]\n",
      "Epoch 300, Loss: 2213.747688\n",
      "  Learned weights: [ 0.11803651  0.9809053  -2.6071937   1.8951716  -0.38834196]\n",
      "Epoch 400, Loss: 2212.227987\n",
      "  Learned weights: [ 0.12152513  0.98371875 -2.6084893   1.8949599  -0.3936463 ]\n",
      "Saved sample_0013_x-5.00_+5.00_v2.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 14...\n",
      "Epoch 0, Loss: 4853.392639\n",
      "  Learned weights: [-0.07856938  1.336054   -2.5512698   1.3989034  -0.1047227 ]\n",
      "Epoch 100, Loss: 2379.820055\n",
      "  Learned weights: [ 0.12696175  0.9555001  -2.5940492   1.8945514  -0.38462153]\n",
      "Epoch 200, Loss: 2391.460570\n",
      "  Learned weights: [ 0.126333   0.9517016 -2.5861557  1.8886318 -0.3822561]\n",
      "Epoch 300, Loss: 2385.072651\n",
      "  Learned weights: [ 0.12528369  0.9618642  -2.6053274   1.904099   -0.3869458 ]\n",
      "Epoch 400, Loss: 2374.365306\n",
      "  Learned weights: [ 0.12786776  0.9556577  -2.593677    1.8933961  -0.38617307]\n",
      "Saved sample_0014_x-5.00_+5.00_v2.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 15...\n",
      "Epoch 0, Loss: 3357.106870\n",
      "  Learned weights: [-0.08133943  1.3395063  -2.5694964   1.4152937  -0.10551502]\n",
      "Epoch 100, Loss: 1972.278381\n",
      "  Learned weights: [ 0.08927367  1.0323579  -2.6671226   1.9406291  -0.40799907]\n",
      "Epoch 200, Loss: 2967.069655\n",
      "  Learned weights: [ 0.08974734  1.0417337  -2.6787574   1.9537554  -0.41016945]\n",
      "Epoch 300, Loss: 1983.700975\n",
      "  Learned weights: [ 0.08989182  1.0354298  -2.6677768   1.9459223  -0.40549037]\n",
      "Epoch 400, Loss: 1987.746849\n",
      "  Learned weights: [ 0.09185481  1.0343512  -2.6636872   1.9403228  -0.40764746]\n",
      "Saved sample_0015_x-5.00_+5.00_v2.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 16...\n",
      "Epoch 0, Loss: 2887.170206\n",
      "  Learned weights: [-0.08025981  1.3421565  -2.5721755   1.4100296  -0.10413376]\n",
      "Epoch 100, Loss: 1721.963546\n",
      "  Learned weights: [ 0.08542171  1.0499758  -2.6963654   1.9667488  -0.41478533]\n",
      "Epoch 200, Loss: 1614.251015\n",
      "  Learned weights: [ 0.08695076  1.0470648  -2.6870987   1.9609647  -0.4117745 ]\n",
      "Epoch 300, Loss: 1583.141751\n",
      "  Learned weights: [ 0.08723276  1.0511731  -2.693191    1.9676093  -0.41226128]\n",
      "Epoch 400, Loss: 1588.674238\n",
      "  Learned weights: [ 0.08639932  1.0499703  -2.6936076   1.9649014  -0.41422138]\n",
      "Saved sample_0016_x-5.00_+5.00_v2.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 17...\n",
      "Epoch 0, Loss: 2994.748743\n",
      "  Learned weights: [-0.0813884   1.3259921  -2.5377862   1.38855    -0.09676439]\n",
      "Epoch 100, Loss: 1973.172638\n",
      "  Learned weights: [ 0.11358666  0.9923242  -2.6196353   1.8938378  -0.38322806]\n",
      "Epoch 200, Loss: 1996.252837\n",
      "  Learned weights: [ 0.11139513  0.9952785  -2.6225083   1.8993193  -0.38265017]\n",
      "Epoch 300, Loss: 1966.413448\n",
      "  Learned weights: [ 0.1135334  0.9969337 -2.626268   1.8997611 -0.3850878]\n",
      "Epoch 400, Loss: 1948.443648\n",
      "  Learned weights: [ 0.11158585  0.99644935 -2.6256413   1.9000224  -0.38477707]\n",
      "Saved sample_0017_x-5.00_+5.00_v2.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 18...\n",
      "Epoch 0, Loss: 2640.108307\n",
      "  Learned weights: [-0.08038956  1.3391182  -2.5551846   1.3986189  -0.10388473]\n",
      "Epoch 100, Loss: 1325.695094\n",
      "  Learned weights: [ 0.09954255  1.0105029  -2.647941    1.9308442  -0.39869004]\n",
      "Epoch 200, Loss: 1332.774120\n",
      "  Learned weights: [ 0.10091544  1.0107278  -2.6450233   1.9311734  -0.39734927]\n",
      "Epoch 300, Loss: 1473.280127\n",
      "  Learned weights: [ 0.10176251  1.0104477  -2.6474292   1.9300647  -0.3978376 ]\n",
      "Epoch 400, Loss: 1313.035694\n",
      "  Learned weights: [ 0.0993549   1.0112334  -2.6529562   1.9338002  -0.39898214]\n",
      "Saved sample_0018_x-5.00_+5.00_v2.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 19...\n",
      "Epoch 0, Loss: 2713.273262\n",
      "  Learned weights: [-0.0822142   1.3314171  -2.5580056   1.4039809  -0.09838195]\n",
      "Epoch 100, Loss: 1906.223771\n",
      "  Learned weights: [ 0.09767491  1.0294461  -2.6575167   1.9288591  -0.39444575]\n",
      "Epoch 200, Loss: 1512.433545\n",
      "  Learned weights: [ 0.0970857   1.0306301  -2.6585371   1.9265186  -0.39789036]\n",
      "Epoch 300, Loss: 1521.857735\n",
      "  Learned weights: [ 0.09768111  1.0299217  -2.6595438   1.9268451  -0.39720878]\n",
      "Epoch 400, Loss: 1475.647076\n",
      "  Learned weights: [ 0.0962411   1.0269513  -2.6543927   1.9239001  -0.39477783]\n",
      "Saved sample_0019_x-5.00_+5.00_v2.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 20...\n",
      "Epoch 0, Loss: 3100.704563\n",
      "  Learned weights: [-0.07945424  1.3218937  -2.5374193   1.3924074  -0.09672135]\n",
      "Epoch 100, Loss: 1931.540655\n",
      "  Learned weights: [ 0.11846141  0.97606367 -2.5972378   1.8761171  -0.37499782]\n",
      "Epoch 200, Loss: 1913.577208\n",
      "  Learned weights: [ 0.1192655   0.9779373  -2.5991178   1.8784087  -0.37685803]\n",
      "Epoch 300, Loss: 1933.857110\n",
      "  Learned weights: [ 0.11765998  0.97469336 -2.5978277   1.8765635  -0.37464395]\n",
      "Epoch 400, Loss: 1982.739755\n",
      "  Learned weights: [ 0.11785997  0.9738961  -2.5966341   1.8753616  -0.37307474]\n",
      "Saved sample_0020_x-5.00_+5.00_v2.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 21...\n",
      "Epoch 0, Loss: 3797.085077\n",
      "  Learned weights: [-0.08503504  1.3279202  -2.5422454   1.3966974  -0.09851645]\n",
      "Epoch 100, Loss: 2527.242155\n",
      "  Learned weights: [ 0.11369851  0.97806746 -2.6019511   1.8910913  -0.38232875]\n",
      "Epoch 200, Loss: 3292.397452\n",
      "  Learned weights: [ 0.11293445  0.9781246  -2.609438    1.8920041  -0.3884282 ]\n",
      "Epoch 300, Loss: 2504.237180\n",
      "  Learned weights: [ 0.11454229  0.98095787 -2.6098855   1.8964838  -0.38467965]\n",
      "Epoch 400, Loss: 2686.953258\n",
      "  Learned weights: [ 0.11501759  0.98470736 -2.611297    1.9005715  -0.3860313 ]\n",
      "Saved sample_0021_x-5.00_+5.00_v2.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 22...\n",
      "Epoch 0, Loss: 3077.969826\n",
      "  Learned weights: [-0.08264729  1.3383969  -2.5666611   1.411287   -0.10280281]\n",
      "Epoch 100, Loss: 2267.820426\n",
      "  Learned weights: [ 0.09170235  1.0280017  -2.6692393   1.9373012  -0.39589572]\n",
      "Epoch 200, Loss: 1815.327989\n",
      "  Learned weights: [ 0.09626225  1.0292081  -2.661802    1.9331108  -0.3941763 ]\n",
      "Epoch 300, Loss: 1691.997367\n",
      "  Learned weights: [ 0.0955869  1.0269173 -2.6597238  1.9278427 -0.3958171]\n",
      "Epoch 400, Loss: 2015.720798\n",
      "  Learned weights: [ 0.09512375  1.0248524  -2.6599796   1.9286337  -0.39259174]\n",
      "Saved sample_0022_x-5.00_+5.00_v2.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 23...\n",
      "Epoch 0, Loss: 3248.946327\n",
      "  Learned weights: [-0.07661166  1.3287457  -2.555581    1.4020944  -0.10052414]\n",
      "Epoch 100, Loss: 2039.246543\n",
      "  Learned weights: [ 0.10600023  1.0139419  -2.6537318   1.9323349  -0.40153188]\n",
      "Epoch 200, Loss: 2107.388630\n",
      "  Learned weights: [ 0.10820645  1.0146149  -2.650218    1.9292264  -0.40279698]\n",
      "Epoch 300, Loss: 2081.171913\n",
      "  Learned weights: [ 0.10525828  1.0128434  -2.653412    1.9308524  -0.4011275 ]\n",
      "Epoch 400, Loss: 2071.832622\n",
      "  Learned weights: [ 0.10786831  1.014469   -2.6494982   1.9292952  -0.40173447]\n",
      "Saved sample_0023_x-5.00_+5.00_v2.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 24...\n",
      "Epoch 0, Loss: 3284.401820\n",
      "  Learned weights: [-0.07935207  1.3364013  -2.5582175   1.4039248  -0.10344706]\n",
      "Epoch 100, Loss: 1567.609103\n",
      "  Learned weights: [ 0.08593907  1.0500846  -2.68506     1.9622974  -0.41292068]\n",
      "Epoch 200, Loss: 1592.434448\n",
      "  Learned weights: [ 0.08776578  1.0494477  -2.6812701   1.9577035  -0.4152266 ]\n",
      "Epoch 300, Loss: 1588.673752\n",
      "  Learned weights: [ 0.08859074  1.0503768  -2.6818092   1.9591522  -0.41374612]\n",
      "Epoch 400, Loss: 2274.033467\n",
      "  Learned weights: [ 0.08808504  1.0501024  -2.683828    1.9587593  -0.41666248]\n",
      "Saved sample_0024_x-5.00_+5.00_v2.00_T256_learnedTrue.npz\n",
      "Training Laplacian for sample 0...\n",
      "Epoch 0, Loss: 173316.407773\n",
      "  Learned weights: [-0.08162542  1.3308529  -2.5543942   1.4044739  -0.10365728]\n",
      "Epoch 100, Loss: 80214.709292\n",
      "  Learned weights: [ 0.11590023  0.9857426  -2.6020079   1.8864007  -0.38839784]\n",
      "Epoch 200, Loss: 80113.774748\n",
      "  Learned weights: [ 0.11586204  0.9866596  -2.6033351   1.887796   -0.38896093]\n",
      "Epoch 300, Loss: 130773.525448\n",
      "  Learned weights: [ 0.11431518  0.9902476  -2.6088836   1.8940428  -0.38828185]\n",
      "Epoch 400, Loss: 81239.338302\n",
      "  Learned weights: [ 0.11469328  0.98464    -2.6013982   1.8875273  -0.38611555]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unstable explicit step:\n  dt=5.000e-03 > safety*min(dx/|u|, dx^2/(2Î½))=4.960e-03\n  (dx=3.150e-02, Î½=1.000e-01, max|u|=1.000e+00, safety=1.00)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m                 do_split(n_test, test_dir, xmin, xmax, nst, spd)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Generate dataset with learned Laplacian\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[43mgenerate_dataset_with_learned_laplacian\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgenerated_1d_burgers_learned\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnbx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboundary_condition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbc_neumann_zero\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mic_kinds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshock\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrarefaction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msmooth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcfl_safety\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearn_laplacian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m    114\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 93\u001b[0m, in \u001b[0;36mgenerate_dataset_with_learned_laplacian\u001b[0;34m(out_dir, nbx, x_min, x_max, dt, n_steps, nu, speed, boundary_condition, ic_kinds, n_train, n_test, cfl_safety, learn_laplacian, kernel_size, device)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nst \u001b[38;5;129;01min\u001b[39;00m stepsLs:\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m spd \u001b[38;5;129;01min\u001b[39;00m speeds:\n\u001b[0;32m---> 93\u001b[0m         \u001b[43mdo_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m         do_split(n_test, test_dir, xmin, xmax, nst, spd)\n",
      "Cell \u001b[0;32mIn[12], line 69\u001b[0m, in \u001b[0;36mgenerate_dataset_with_learned_laplacian.<locals>.do_split\u001b[0;34m(N, split_dir, xmin_val, xmax_val, nst_val, spd_val)\u001b[0m\n\u001b[1;32m     66\u001b[0m     learned_sim\u001b[38;5;241m.\u001b[39mtrain_laplacian(training_data\u001b[38;5;241m=\u001b[39mtraining_data, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Run simulation\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m U \u001b[38;5;241m=\u001b[39m \u001b[43mlearned_sim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Create tag and filename\u001b[39;00m\n\u001b[1;32m     72\u001b[0m tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkind_used\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m|x=[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxmin_val\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxmax_val\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]|T=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(nst_val)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m|v=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mfloat\u001b[39m(spd_val)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m|learned_laplacian=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearn_laplacian\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[3], line 183\u001b[0m, in \u001b[0;36mBurgers1DLearned.simulate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m U \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((nt, nx))\n\u001b[1;32m    181\u001b[0m U[\u001b[38;5;241m0\u001b[39m, :] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_condition(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid\u001b[38;5;241m.\u001b[39mx)\n\u001b[0;32m--> 183\u001b[0m \u001b[43mparent_sim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_cfl_burgers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mU\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nt \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    186\u001b[0m     u \u001b[38;5;241m=\u001b[39m U[n, :]\n",
      "Cell \u001b[0;32mIn[8], line 39\u001b[0m, in \u001b[0;36mBurgers1D.check_cfl_burgers\u001b[0;34m(self, u_now)\u001b[0m\n\u001b[1;32m     37\u001b[0m limit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfl_safety \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmin\u001b[39m(conv_limit, diff_limit)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dt \u001b[38;5;241m>\u001b[39m limit:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnstable explicit step:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  dt=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdt\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m > safety*min(dx/|u|, dx^2/(2Î½))=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlimit\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  (dx=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdx\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Î½=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnu\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, max|u|=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mumax\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, safety=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfl_safety\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Unstable explicit step:\n  dt=5.000e-03 > safety*min(dx/|u|, dx^2/(2Î½))=4.960e-03\n  (dx=3.150e-02, Î½=1.000e-01, max|u|=1.000e+00, safety=1.00)"
     ]
    }
   ],
   "source": [
    "# Create a custom data generation function using Burgers1DLearned\n",
    "'''\n",
    "def generate_dataset_with_learned_laplacian(\n",
    "    out_dir=\"generated_1d_burgers_learned\",\n",
    "    nbx=128,\n",
    "    x_min=[-5, -2],\n",
    "    x_max=[5, 2],\n",
    "    dt=5e-3,\n",
    "    n_steps=[128, 256],\n",
    "    nu=0.1,\n",
    "    speed=[1.0, 2.0],\n",
    "    boundary_condition=bc_neumann_zero,\n",
    "    ic_kinds=[\"shock\", \"rarefaction\", \"sine\", \"smooth\"],\n",
    "    n_train=25,\n",
    "    n_test=0,\n",
    "    cfl_safety=1,\n",
    "    learn_laplacian=True,\n",
    "    kernel_size=5,\n",
    "    device='cpu'\n",
    "):\n",
    "    \"\"\"Generate dataset using the learned Laplacian version\"\"\"\n",
    "    train_dir = os.path.join(out_dir, \"train\")\n",
    "    test_dir = os.path.join(out_dir, \"test\")\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    xmins = _tolist(x_min)\n",
    "    xmaxs = _tolist(x_max)\n",
    "    stepsLs = _tolist(n_steps)\n",
    "    speeds = _tolist(speed)\n",
    "\n",
    "    if len(xmins) != len(xmaxs):\n",
    "        if len(xmins) == 1:\n",
    "            xmins = xmins * len(xmaxs)\n",
    "        elif len(xmaxs) == 1:\n",
    "            xmaxs = xmaxs * len(xmins)\n",
    "        else:\n",
    "            raise ValueError(\"x_min and x_max must have the same length or be scalars.\")\n",
    "    x_pairs = list(zip(xmins, xmaxs))\n",
    "\n",
    "    def do_split(N, split_dir, xmin_val, xmax_val, nst_val, spd_val):\n",
    "        for i in range(N):\n",
    "            # Create grid\n",
    "            grid = make_grid(nbx, xmin_val, xmax_val, dt, n_steps=int(nst_val))\n",
    "            \n",
    "            # Choose initial condition kind\n",
    "            kind = None if ic_kinds is None else random.choice(ic_kinds)\n",
    "            ic_fn, kind_used = make_initial_condition_burgers_fn(grid.x, float(spd_val), kind)\n",
    "            \n",
    "            # Create learned simulator\n",
    "            learned_sim = Burgers1DLearned(\n",
    "                grid=grid,\n",
    "                nu=float(nu),\n",
    "                initial_condition=ic_fn,\n",
    "                boundary_condition=boundary_condition,\n",
    "                cfl_safety=cfl_safety,\n",
    "                learn_laplacian=learn_laplacian,\n",
    "                kernel_size=kernel_size,\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            # Train the Laplacian network if learning is enabled\n",
    "            if learn_laplacian:\n",
    "                print(f\"Training Laplacian for sample {i}...\")\n",
    "                training_data = learned_sim.generate_training_data(n_samples=50)\n",
    "                learned_sim.train_laplacian(training_data=training_data, epochs=500, batch_size=8)\n",
    "            \n",
    "            # Run simulation\n",
    "            U = learned_sim.simulate()\n",
    "            \n",
    "            # Create tag and filename\n",
    "            tag = f\"{kind_used}|x=[{xmin_val},{xmax_val}]|T={int(nst_val)}|v={float(spd_val)}|learned_laplacian={learn_laplacian}\"\n",
    "            filename = (\n",
    "                f\"sample_{i:04d}\"\n",
    "                f\"_x{float(xmin_val):+.2f}_{float(xmax_val):+.2f}\"\n",
    "                f\"_v{float(spd_val):.2f}\"\n",
    "                f\"_T{int(nst_val)}\"\n",
    "                f\"_learned{learn_laplacian}.npz\"\n",
    "            )\n",
    "            \n",
    "            # Save results\n",
    "            grid.save_npz(\n",
    "                os.path.join(split_dir, filename),\n",
    "                U, nu=float(nu), speed=float(spd_val), tag=tag\n",
    "            )\n",
    "            \n",
    "            print(f\"Saved {filename}\")\n",
    "\n",
    "    # Generate datasets\n",
    "    for (xmin, xmax) in x_pairs:\n",
    "        for nst in stepsLs:\n",
    "            for spd in speeds:\n",
    "                do_split(n_train, train_dir, xmin, xmax, nst, spd)\n",
    "                do_split(n_test, test_dir, xmin, xmax, nst, spd)\n",
    "\n",
    "# Generate dataset with learned Laplacian\n",
    "generate_dataset_with_learned_laplacian(\n",
    "    out_dir=\"generated_1d_burgers_learned\",\n",
    "    nbx=128,\n",
    "    x_min=[-5, -2],\n",
    "    x_max=[5, 2],\n",
    "    dt=5e-3,\n",
    "    n_steps=[128, 256],\n",
    "    nu=0.1,\n",
    "    speed=[1.0, 2.0],\n",
    "    boundary_condition=bc_neumann_zero,\n",
    "    ic_kinds=[\"shock\", \"rarefaction\", \"sine\", \"smooth\"],\n",
    "    n_train=25,\n",
    "    n_test=0,\n",
    "    cfl_safety=1,\n",
    "    learn_laplacian=True,\n",
    "    kernel_size=5,\n",
    "    device='cpu'\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eacf12-9887-465c-a260-ca579f473dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simpler version\n",
    "\n",
    "# Simple demonstration using Burgers1DLearned for a few simulations\n",
    "def run_learned_simulations_demo():\n",
    "    \"\"\"Run a few simulations with learned Laplacian for demonstration\"\"\"\n",
    "    \n",
    "    # Parameters for the simulation\n",
    "    nbx = 128\n",
    "    x_min, x_max = -5, 5\n",
    "    dt = 5e-3\n",
    "    n_steps = 128\n",
    "    nu = 0.1\n",
    "    speed = 2.0\n",
    "    \n",
    "    # Create grid\n",
    "    grid = make_grid(nbx, x_min, x_max, dt, n_steps=n_steps)\n",
    "    \n",
    "    # Create initial condition\n",
    "    ic_fn, kind_used = make_initial_condition_burgers_fn(grid.x, speed, \"shock\")\n",
    "    \n",
    "    # Create and train learned simulator\n",
    "    print(\"Creating learned Burgers simulator...\")\n",
    "    learned_sim = Burgers1DLearned(\n",
    "        grid=grid,\n",
    "        nu=nu,\n",
    "        initial_condition=ic_fn,\n",
    "        boundary_condition=bc_neumann_zero,\n",
    "        cfl_safety=1,\n",
    "        learn_laplacian=True,\n",
    "        kernel_size=5,\n",
    "        device='cpu'\n",
    "    )\n",
    "    \n",
    "    # Train the Laplacian network\n",
    "    print(\"Training Laplacian network...\")\n",
    "    training_data = learned_sim.generate_training_data(n_samples=50)\n",
    "    learned_sim.train_laplacian(training_data=training_data, epochs=500, batch_size=8)\n",
    "    \n",
    "    # Run simulation\n",
    "    print(\"Running simulation with learned Laplacian...\")\n",
    "    U_learned = learned_sim.simulate()\n",
    "    \n",
    "    # For comparison, run with analytical Laplacian\n",
    "    print(\"Running simulation with analytical Laplacian...\")\n",
    "    analytical_sim = Burgers1DLearned(\n",
    "        grid=grid,\n",
    "        nu=nu,\n",
    "        initial_condition=ic_fn,\n",
    "        boundary_condition=bc_neumann_zero,\n",
    "        cfl_safety=1,\n",
    "        learn_laplacian=False,  # Use analytical Laplacian\n",
    "        device='cpu'\n",
    "    )\n",
    "    U_analytical = analytical_sim.simulate()\n",
    "    \n",
    "    # Plot results for comparison\n",
    "    print(\"Plotting results...\")\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    grid.plot(U_analytical, title=\"Analytical Laplacian\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    grid.plot(U_learned, title=\"Learned Laplacian\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    diff = U_learned - U_analytical\n",
    "    grid.plot(diff, title=\"Difference\", cmap='RdBu')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print learned weights\n",
    "    if learned_sim.learn_laplacian and learned_sim.laplacian_net is not None:\n",
    "        weights = learned_sim.laplacian_net.get_effective_weights()\n",
    "        print(f\"Learned Laplacian weights: {weights}\")\n",
    "    \n",
    "    return U_analytical, U_learned\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e528fa-7419-4346-ab65-3b1482706300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the demonstration\n",
    "U_analytical, U_learned = run_learned_simulations_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfa5587",
   "metadata": {},
   "outputs": [],
   "source": [
    "#samuel's\n",
    "#visualize_random_sample(\"generated_1d_burgers/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708ce91c-1581-4ea7-8907-7f78c1e7211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_random_sample(\"generated_1d_burgers_learned/train\", \n",
    "                       title_prefix=\"Burgers with Learned Laplacian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cc185a-1490-4bdb-b35b-22261511bbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "print(\"Generating training data...\")\n",
    "training_data = generate_training_data_for_learning(n_samples=50)\n",
    "\n",
    "# Create simulator with learned Laplacian\n",
    "grid = make_grid(nbx=128, x_min=-5, x_max=5, dt=5e-3, n_steps=128)\n",
    "ic_fn, _ = make_initial_condition_burgers(grid.x, speed=2.0, kind=\"sine\")\n",
    "\n",
    "sim_learned = Burgers1DLearned(grid, nu=0.1, ic_fn, \n",
    "                              learn_laplacian=True, kernel_size=5)\n",
    "\n",
    "# Train the Laplacian operator\n",
    "print(\"Training Laplacian operator...\")\n",
    "sim_learned.train_laplacian(training_data, epochs=1000)\n",
    "\n",
    "# Run simulation with learned operator\n",
    "print(\"Running simulation with learned Laplacian...\")\n",
    "U_learned = sim_learned.simulate()\n",
    "\n",
    "# Compare with analytical\n",
    "sim_analytical = Burgers1D(grid, nu=0.1, ic_fn)\n",
    "U_analytical = sim_analytical.simulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c2368b-553a-49c5-b9c3-b15d6325fbb5",
   "metadata": {},
   "source": [
    "# Data Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b5eebc-7601-4d89-8d0d-acb4ebdaf157",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Utility function to compare learned vs analytical Laplacian\n",
    "def compare_laplacians(sim_learned, test_function=None):\n",
    "    \"\"\"Compare learned vs analytical Laplacian on test functions\"\"\"\n",
    "    if test_function is None:\n",
    "        # Default test function: sine wave\n",
    "        x = sim_learned.grid.x\n",
    "        test_function = np.sin(2 * np.pi * x / (x.max() - x.min()))\n",
    "    \n",
    "    analytical = sim_learned.analytical_laplacian(test_function)\n",
    "    learned = sim_learned.learned_laplacian(test_function)\n",
    "    \n",
    "    error = np.mean((analytical - learned)**2)\n",
    "    print(f\"MSE between learned and analytical Laplacian: {error:.6f}\")\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(sim_learned.grid.x, analytical, 'b-', label='Analytical', linewidth=2)\n",
    "    plt.plot(sim_learned.grid.x, learned, 'r--', label='Learned', linewidth=2)\n",
    "    plt.legend()\n",
    "    plt.title('Laplacian Comparison')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('âˆ‡Â²u')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(sim_learned.grid.x, analytical - learned, 'g-', label='Error')\n",
    "    plt.legend()\n",
    "    plt.title('Error')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('Difference')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return analytical, learned, error\n",
    "\n",
    "\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96293740-f462-497a-86f0-69a6ac4f712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate accurate training data\n",
    "print(\"Generating training data...\")\n",
    "training_data = generate_training_data_for_learning(n_samples=200)\n",
    "\n",
    "# Create simulator with learned Laplacian\n",
    "grid = make_grid(nbx=128, x_min=-5, x_max=5, dt=5e-3, n_steps=128)\n",
    "ic_fn, _ = make_initial_condition_burgers(grid.x, speed=2.0, kind=\"sine\")\n",
    "\n",
    "sim_learned = Burgers1DLearned(grid, nu=0.1, initial_condition=ic_fn, \n",
    "                              learn_laplacian=True, kernel_size=5)\n",
    "\n",
    "# Train the Laplacian operator\n",
    "print(\"Training Laplacian operator...\")\n",
    "sim_learned.train_laplacian(training_data, epochs=1000)\n",
    "\n",
    "# Validate the learned operator\n",
    "print(\"Validating learned Laplacian...\")\n",
    "#analytical, learned, error = compare_laplacians(sim_learned)\n",
    "\n",
    "# Run simulation with learned operator\n",
    "#print(\"Running simulation with learned Laplacian...\")\n",
    "#U_learned = sim_learned.simulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca85cfe2-9363-4c38-8a91-ac3ae43a69a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please run the demo first: U_analytical, U_learned = run_learned_simulations_demo()\n"
     ]
    }
   ],
   "source": [
    "# Visualize the demo results\n",
    "if 'U_learned' in locals() and 'U_analytical' in locals():\n",
    "    grid = make_grid(nbx=128, x_min=-5, x_max=5, dt=5e-3, n_steps=128)\n",
    "    \n",
    "    plt.figure(figsize=(15, 4))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(U_analytical, extent=[grid.x.min(), grid.x.max(), grid.t.max(), grid.t.min()],\n",
    "               cmap='seismic', aspect='auto', origin='upper')\n",
    "    plt.colorbar(label='u')\n",
    "    plt.title('Analytical Laplacian')\n",
    "    plt.xlabel('X'); plt.ylabel('T')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(U_learned, extent=[grid.x.min(), grid.x.max(), grid.t.max(), grid.t.min()],\n",
    "               cmap='seismic', aspect='auto', origin='upper')\n",
    "    plt.colorbar(label='u')\n",
    "    plt.title('Learned Laplacian')\n",
    "    plt.xlabel('X'); plt.ylabel('T')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    diff = U_learned - U_analytical\n",
    "    plt.imshow(diff, extent=[grid.x.min(), grid.x.max(), grid.t.max(), grid.t.min()],\n",
    "               cmap='RdBu', aspect='auto', origin='upper')\n",
    "    plt.colorbar(label='Difference')\n",
    "    plt.title('Difference')\n",
    "    plt.xlabel('X'); plt.ylabel('T')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Please run the demo first: U_analytical, U_learned = run_learned_simulations_demo()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1746d8c0-3648-48a5-a3b4-b8a43f15be7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
