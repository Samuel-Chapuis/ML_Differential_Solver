\chapter{Next Steps}
\label{chap:nextSteps}


% This chapter presents our comprehensive methodology for solving the one-dimensional Burgers equation using Neural Field Turing Machines (NFTMs). We detail the complete pipeline from data generation through model architecture, training procedures, and evaluation metrics. Our approach builds upon the NFTM framework while introducing several key innovations to improve stability, physical consistency, and generalization capabilities.

% %==============================================================================
% \section{Overview of the Proposed Pipeline}
% \label{sec:pipeline_overview}
% %==============================================================================

% Our neural PDE solver pipeline consists of four main components, illustrated in Figure~\ref{fig:pipeline_overview}:

% \begin{enumerate}
%     \item \textbf{Physics-Based Data Generation}: High-fidelity numerical solutions of the Burgers equation using stable finite difference methods
%     \item \textbf{Neural Architecture}: A causal temporal attention mechanism combined with convolutional decoders
%     \item \textbf{Training Strategy}: Multi-stage curriculum learning with physics-informed regularization
%     \item \textbf{Evaluation Framework}: Comprehensive metrics assessing accuracy, stability, and physical consistency
% \end{enumerate}

% The complete workflow operates as follows: training trajectories are generated by solving the Burgers equation numerically for various initial conditions and viscosity parameters. These trajectories are then used to train a neural network that learns to predict future states given a window of historical states. During inference, the model performs autoregressive rollout to generate long-term predictions. Finally, predictions are evaluated against ground truth using multiple metrics to assess both numerical accuracy and physical plausibility.

% %==============================================================================
% \section{Data Generation: Stable Numerical Solver}
% \label{sec:data_generation}
% %==============================================================================

% %------------------------------------------------------------------------------
% \subsection{Finite Difference Scheme}
% \label{subsec:finite_difference}
% %------------------------------------------------------------------------------

% To generate high-quality training data, we implement a stable numerical solver for the one-dimensional Burgers equation:
% \begin{equation}
% \frac{\partial u}{\partial t} + u\frac{\partial u}{\partial x} = \nu\frac{\partial^2 u}{\partial x^2}
% \label{eq:burgers_data_gen}
% \end{equation}

% We discretize the spatial domain \(x \in [0, 2\pi]\) with periodic boundary conditions using \(N = 128\) uniformly spaced grid points:
% \begin{equation}
% x_i = \frac{2\pi i}{N}, \quad i = 0, 1, \ldots, N-1
% \label{eq:spatial_discretization}
% \end{equation}

% The spatial step size is:
% \begin{equation}
% \Delta x = \frac{2\pi}{N} \approx 0.049
% \label{eq:dx}
% \end{equation}

% \paragraph{Diffusion Term.}
% The viscous diffusion term is discretized using central differences, which is second-order accurate in space:
% \begin{equation}
% \frac{\partial^2 u}{\partial x^2}\Big|_{x_i} \approx \frac{u_{i+1} - 2u_i + u_{i-1}}{\Delta x^2} = D_i
% \label{eq:diffusion_discrete}
% \end{equation}

% This stencil is applied with periodic boundary conditions: \(u_{-1} = u_{N-1}\) and \(u_N = u_0\).

% \paragraph{Advection Term.}
% For the nonlinear advection term \(u\frac{\partial u}{\partial x}\), we employ an upwind differencing scheme to maintain numerical stability:
% \begin{equation}
% \frac{\partial u}{\partial x}\Big|_{x_i} \approx 
% \begin{cases}
% \frac{u_i - u_{i-1}}{\Delta x} & \text{if } u_i > 0 \text{ (backward difference)} \\
% \frac{u_{i+1} - u_i}{\Delta x} & \text{if } u_i \leq 0 \text{ (forward difference)}
% \end{cases}
% \label{eq:upwind_scheme}
% \end{equation}

% The upwind scheme selects the differencing direction based on the local flow direction, which prevents spurious oscillations in regions with steep gradients or shocks.

% \paragraph{Time Integration.}
% We use the forward Euler method for time integration with timestep \(\Delta t = 0.01\):
% \begin{equation}
% u_i^{n+1} = u_i^n - \Delta t \cdot u_i^n \left(\frac{\partial u}{\partial x}\right)_i^n + \Delta t \cdot \nu \cdot D_i^n
% \label{eq:forward_euler}
% \end{equation}

% To ensure stability, the timestep satisfies the CFL (Courant-Friedrichs-Lewy) condition:
% \begin{equation}
% \Delta t \leq \min\left(\frac{\Delta x}{|u_{\max}|}, \frac{\Delta x^2}{2\nu}\right)
% \label{eq:cfl_condition_data}
% \end{equation}

% For our parameters (\(\Delta x \approx 0.049\), \(\nu \in [0.01, 0.1]\), \(|u_{\max}| \lesssim 1\)), \(\Delta t = 0.01\) satisfies this stability criterion.

% %------------------------------------------------------------------------------
% \subsection{Dataset Construction}
% \label{subsec:dataset_construction}
% %------------------------------------------------------------------------------

% \paragraph{Sampling Strategy.}
% For each trajectory in our dataset, we randomly sample:
% \begin{itemize}
%     \item \textbf{Viscosity parameter}: \(\nu \sim \mathcal{U}(0.01, 0.1)\), where \(\mathcal{U}(a,b)\) denotes the uniform distribution
%     \item \textbf{Initial condition}: \(u_0(x) = \sin(x)\) (smooth sinusoidal profile)
% \end{itemize}

% The sinusoidal initial condition is chosen because:
% \begin{enumerate}
%     \item It is smooth and differentiable (realistic physical scenario)
%     \item It produces rich dynamics including wave steepening and eventual shock formation for small \(\nu\)
%     \item It has zero mean: \(\int_0^{2\pi} \sin(x)\,dx = 0\), which tests mass conservation
%     \item It provides analytical benchmarks via the Cole-Hopf transformation
% \end{enumerate}

% \paragraph{Trajectory Length.}
% Each trajectory consists of \(T = 100\) timesteps, spanning total time \(T_{\text{total}} = 1.0\) seconds. The state at each timestep is stored:
% \begin{equation}
% \mathcal{T}_{\text{trajectory}} = \{u^0, u^1, \ldots, u^{99}\}, \quad u^t \in \mathbb{R}^{128}
% \label{eq:trajectory}
% \end{equation}

% \paragraph{Dataset Split.}
% We generate:
% \begin{itemize}
%     \item \textbf{Training set}: 1000 trajectories (100,000 state snapshots)
%     \item \textbf{Test set}: 200 trajectories (20,000 state snapshots)
% \end{itemize}

% No trajectory in the test set has the same viscosity parameter \(\nu\) as any training trajectory, ensuring evaluation on truly unseen parameter regimes.

% %------------------------------------------------------------------------------
% \subsection{Quality Assurance}
% \label{subsec:quality_assurance}
% %------------------------------------------------------------------------------

% We verify the quality of generated data through:

% \paragraph{Mass Conservation.}
% For periodic boundary conditions, the total mass \(M(t) = \int_0^{2\pi} u(x,t)\,dx\) should be conserved. Numerically:
% \begin{equation}
% M^n = \Delta x \sum_{i=0}^{N-1} u_i^n
% \label{eq:mass_discrete}
% \end{equation}

% We verify:
% \begin{equation}
% \left|\frac{M^{99} - M^0}{M^0 + \epsilon}\right| < 10^{-3}
% \label{eq:mass_conservation_check}
% \end{equation}
% where \(\epsilon = 10^{-6}\) prevents division by zero for the sinusoidal IC where \(M^0 \approx 0\).

% \paragraph{Energy Dissipation.}
% The kinetic energy:
% \begin{equation}
% E^n = \frac{\Delta x}{2} \sum_{i=0}^{N-1} (u_i^n)^2
% \label{eq:energy_discrete}
% \end{equation}
% must monotonically decrease due to viscous dissipation:
% \begin{equation}
% E^{n+1} \leq E^n + \epsilon_{\text{tol}}
% \label{eq:energy_monotonicity}
% \end{equation}
% where \(\epsilon_{\text{tol}} = 10^{-6}\) accounts for numerical roundoff.

% \paragraph{Stability Check.}
% We reject any trajectory where:
% \begin{equation}
% \max_{t,i} |u_i^t| > 5 \quad \text{or} \quad \exists\, (t,i): \text{isnan}(u_i^t) \vee \text{isinf}(u_i^t)
% \label{eq:stability_check}
% \end{equation}

% These checks ensure only physically plausible and numerically stable trajectories enter the training dataset.

% %==============================================================================
% \section{Neural Architecture: Improved NFTM with Temporal Attention}
% \label{sec:architecture}
% %==============================================================================

% Our neural architecture enhances the basic NFTM framework with causal temporal attention and multi-scale processing.

% %------------------------------------------------------------------------------
% \subsection{Input Representation and Window-Based Prediction}
% \label{subsec:input_representation}
% %------------------------------------------------------------------------------

% \paragraph{Historical Window.}
% Rather than predicting the next state from a single previous state, we use a sliding window of length \(W\):
% \begin{equation}
% \mathbf{U}_{\text{hist}} = [u^{t-W+1}, u^{t-W+2}, \ldots, u^t] \in \mathbb{R}^{W \times N}
% \label{eq:history_window}
% \end{equation}

% In our implementation, \(W = 20\), providing the model with approximately 0.2 seconds of history to capture temporal dynamics.

% \paragraph{Prediction Task.}
% Given \(\mathbf{U}_{\text{hist}}\), the model predicts the next state:
% \begin{equation}
% \hat{u}^{t+1} = f_{\theta}(\mathbf{U}_{\text{hist}})
% \label{eq:prediction_task}
% \end{equation}

% For autoregressive rollout over \(K\) steps:
% \begin{equation}
% \begin{aligned}
% \hat{u}^{t+1} &= f_{\theta}([u^{t-W+2}, \ldots, u^t]) \\
% \hat{u}^{t+2} &= f_{\theta}([u^{t-W+3}, \ldots, u^t, \hat{u}^{t+1}]) \\
% &\vdots \\
% \hat{u}^{t+K} &= f_{\theta}([\hat{u}^{t+K-W}, \ldots, \hat{u}^{t+K-1}])
% \end{aligned}
% \label{eq:autoregressive_rollout}
% \end{equation}

% This autoregressive formulation allows arbitrary-length predictions but accumulates errors over time.

% %------------------------------------------------------------------------------
% \subsection{Causal Temporal Attention Mechanism}
% \label{subsec:temporal_attention}
% %------------------------------------------------------------------------------

% \paragraph{Motivation.}
% The temporal attention mechanism allows the model to selectively weight different historical timesteps based on their relevance to the current prediction. This is particularly important for PDEs where recent states may be more informative than distant past states, but long-range temporal dependencies still matter.

% \paragraph{Architecture Overview.}
% The causal temporal attention module processes the history window \(\mathbf{U}_{\text{hist}} \in \mathbb{R}^{B \times W \times N}\) (batch size \(B\), window size \(W\), spatial points \(N\)) in three stages:

% \begin{enumerate}
%     \item \textbf{Feature Embedding}: Each timestep is embedded into a high-dimensional feature space
%     \item \textbf{Attention Computation}: Query-Key-Value attention focuses on relevant historical information
%     \item \textbf{Context Aggregation}: Weighted temporal features are aggregated into a context vector
% \end{enumerate}

% \paragraph{Feature Embedding.}
% First, we embed each spatial field into a feature space using a 1D convolutional layer:
% \begin{equation}
% \mathbf{H}^t = \text{GELU}(\text{Conv1D}(u^t; \theta_{\text{emb}}))
% \label{eq:embedding}
% \end{equation}
% where \(\mathbf{H}^t \in \mathbb{R}^{d \times N}\) and \(d = 32\) is the embedding dimension.

% For the full window:
% \begin{equation}
% \mathbf{H} = [\mathbf{H}^{t-W+1}, \ldots, \mathbf{H}^t] \in \mathbb{R}^{W \times d \times N}
% \label{eq:embedded_history}
% \end{equation}

% \paragraph{Query-Key-Value Projections.}
% We use the most recent frame \(\mathbf{H}^t\) to generate queries \(\mathbf{Q}\), while all historical frames generate keys \(\mathbf{K}\) and values \(\mathbf{V}\):
% \begin{align}
% \mathbf{Q} &= \text{Conv1D}(\mathbf{H}^t; \theta_Q) \in \mathbb{R}^{d \times N} \label{eq:query} \\
% \mathbf{K}^{\tau} &= \text{Conv1D}(\mathbf{H}^{\tau}; \theta_K) \in \mathbb{R}^{d \times N}, \quad \tau \in [t-W+1, t] \label{eq:key} \\
% \mathbf{V}^{\tau} &= \text{Conv1D}(\mathbf{H}^{\tau}; \theta_V) \in \mathbb{R}^{d \times N} \label{eq:value}
% \end{align}

% \paragraph{Attention Scores.}
% The attention scores measure similarity between the current query and past keys:
% \begin{equation}
% \alpha^{\tau}_i = \frac{\mathbf{Q}_i^T \mathbf{K}^{\tau}_i}{\sqrt{d}}, \quad i = 1, \ldots, N
% \label{eq:attention_scores}
% \end{equation}

% Normalization via softmax across the temporal dimension yields attention weights:
% \begin{equation}
% a^{\tau}_i = \frac{\exp(\alpha^{\tau}_i)}{\sum_{\tau'=t-W+1}^{t} \exp(\alpha^{\tau'}_i)}
% \label{eq:attention_weights}
% \end{equation}

% This ensures:
% \begin{equation}
% \sum_{\tau=t-W+1}^{t} a^{\tau}_i = 1, \quad a^{\tau}_i \geq 0
% \label{eq:attention_normalization}
% \end{equation}

% \paragraph{Context Aggregation.}
% The weighted sum of value vectors produces the context representation:
% \begin{equation}
% \mathbf{C}_i = \sum_{\tau=t-W+1}^{t} a^{\tau}_i \mathbf{V}^{\tau}_i
% \label{eq:context}
% \end{equation}

% \paragraph{Output Projection and Residual Connection.}
% The context is projected back to the feature dimension and combined with the last frame via residual connection:
% \begin{equation}
% \mathbf{F} = \text{GroupNorm}(\text{Conv1D}(\mathbf{C}; \theta_{\text{out}}) + \mathbf{H}^t)
% \label{eq:attention_output}
% \end{equation}

% The GroupNorm stabilizes training by normalizing activations across channel groups.

% %------------------------------------------------------------------------------
% \subsection{Convolutional Decoder}
% \label{subsec:decoder}
% %------------------------------------------------------------------------------

% The decoder transforms the attention-enriched features \(\mathbf{F} \in \mathbb{R}^{d \times N}\) into spatial corrections \(\Delta u \in \mathbb{R}^{1 \times N}\):

% \begin{equation}
% \begin{aligned}
% \mathbf{D}_1 &= \text{GELU}(\text{BatchNorm}(\text{Conv1D}(\mathbf{F}; \text{kernel}=5))) \\
% \Delta u &= \text{Conv1D}(\mathbf{D}_1; \text{kernel}=5, \text{out\_channels}=1)
% \end{aligned}
% \label{eq:decoder}
% \end{equation}

% \paragraph{Multi-Scale Receptive Field.}
% The two convolutional layers with kernel size 5 provide an effective receptive field:
% \begin{equation}
% R_{\text{eff}} = 1 + 2 \times (5 - 1) = 9
% \label{eq:receptive_field_decoder}
% \end{equation}

% This covers approximately \(\frac{9}{128} \times 2\pi \approx 0.44\) spatial units, sufficient to capture local gradients and shock structures.

% \paragraph{Bounded Correction via Tanh.}
% To prevent instabilities, corrections are bounded:
% \begin{equation}
% \Delta u_{\text{bounded}} = \tanh(\Delta u) \cdot c_{\text{clip}}
% \label{eq:tanh_clipping}
% \end{equation}
% where \(c_{\text{clip}} = 0.1\). This ensures:
% \begin{equation}
% |\Delta u_{\text{bounded}}| \leq 0.1
% \label{eq:correction_bound}
% \end{equation}

% preventing single-step divergence during autoregressive rollout.

% %------------------------------------------------------------------------------
% \subsection{Final Prediction}
% \label{subsec:final_prediction}
% %------------------------------------------------------------------------------

% The next state is computed via residual update:
% \begin{equation}
% \hat{u}^{t+1} = u^t + \Delta u_{\text{bounded}}
% \label{eq:residual_update}
% \end{equation}

% This formulation has two advantages:
% \begin{enumerate}
%     \item \textbf{Easier Optimization}: Learning perturbations \(\Delta u\) rather than absolute states \(u^{t+1}\) simplifies the learning task
%     \item \textbf{Physical Alignment}: For small timesteps, the true dynamics satisfy \(u^{t+1} \approx u^t + \Delta t \cdot f(u^t)\), matching our residual formulation
% \end{enumerate}

% %==============================================================================
% \section{Training Strategy}
% \label{sec:training_strategy}
% %==============================================================================

% %------------------------------------------------------------------------------
% \subsection{Loss Function Design}
% \label{subsec:loss_function}
% %------------------------------------------------------------------------------

% Our loss function combines multiple terms to enforce both accuracy and physical consistency.

% \paragraph{Mean Squared Error (MSE).}
% The primary loss is the L2 prediction error:
% \begin{equation}
% \mathcal{L}_{\text{MSE}} = \frac{1}{BN} \sum_{b=1}^B \sum_{i=1}^N (\hat{u}_i^b - u_i^b)^2
% \label{eq:mse_loss}
% \end{equation}

% \paragraph{Gradient Matching Loss.}
% To preserve spatial structures and shocks, we match spatial gradients:
% \begin{equation}
% \mathcal{L}_{\text{grad}} = \frac{1}{BN} \sum_{b=1}^B \sum_{i=1}^N \left(\frac{\partial \hat{u}_i^b}{\partial x} - \frac{\partial u_i^b}{\partial x}\right)^2
% \label{eq:gradient_loss}
% \end{equation}

% Gradients are computed via central differences:
% \begin{equation}
% \frac{\partial u_i}{\partial x} \approx \frac{u_{i+1} - u_{i-1}}{2\Delta x}
% \label{eq:gradient_computation}
% \end{equation}

% \paragraph{Energy Dissipation Constraint.}
% The Burgers equation dissipates energy due to viscosity:
% \begin{equation}
% E(t+\Delta t) \leq E(t)
% \label{eq:energy_physics}
% \end{equation}

% We enforce this via a penalty:
% \begin{equation}
% \mathcal{L}_{\text{energy}} = \frac{1}{B} \sum_{b=1}^B \text{ReLU}\left(\frac{E(\hat{u}^b) - E(u^{t,b})}{E(u^{t,b}) + \epsilon}\right)
% \label{eq:energy_loss}
% \end{equation}

% where:
% \begin{equation}
% E(u) = \frac{\Delta x}{2} \sum_{i=1}^N u_i^2
% \label{eq:energy_formula}
% \end{equation}

% The ReLU ensures we only penalize energy \textit{increases}, not decreases. Normalization by \(E(u^t)\) makes the penalty scale-invariant.

% \paragraph{Combined Loss.}
% The total loss is:
% \begin{equation}
% \mathcal{L}_{\text{total}} = \mathcal{L}_{\text{MSE}} + \lambda_{\text{grad}} \mathcal{L}_{\text{grad}} + \lambda_{\text{energy}} \mathcal{L}_{\text{energy}}
% \label{eq:total_loss}
% \end{equation}

% with weights \(\lambda_{\text{grad}} = 0.1\) and \(\lambda_{\text{energy}} = 0.05\).

% %------------------------------------------------------------------------------
% \subsection{Curriculum Learning Strategy}
% \label{subsec:curriculum_learning}
% %------------------------------------------------------------------------------

% To improve long-term stability, we employ curriculum learning that gradually increases task difficulty.

% \paragraph{Rollout Depth Scheduling.}
% Early in training, we use short rollouts; later, we extend to longer predictions:
% \begin{equation}
% K_{\text{rollout}}(\text{epoch}) = 
% \begin{cases}
% 8 & \text{epoch} < 10 \\
% 16 & 10 \leq \text{epoch} < 30 \\
% 32 & \text{epoch} \geq 30
% \end{cases}
% \label{eq:rollout_schedule}
% \end{equation}

% During each training iteration, we unroll for \(K_{\text{rollout}}\) steps and backpropagate through the entire sequence:
% \begin{equation}
% \mathcal{L}_{\text{rollout}} = \frac{1}{K_{\text{rollout}}} \sum_{k=1}^{K_{\text{rollout}}} \mathcal{L}_{\text{total}}(\hat{u}^{t+k}, u^{t+k})
% \label{eq:rollout_loss}
% \end{equation}

% \paragraph{Teacher Forcing.}
% To prevent error accumulation during training, we occasionally use ground truth states instead of predictions:
% \begin{equation}
% u^{t+k}_{\text{next}} = 
% \begin{cases}
% u^{t+k}_{\text{true}} & \text{with probability } p_{\text{TF}} \\
% \hat{u}^{t+k} & \text{with probability } 1 - p_{\text{TF}}
% \end{cases}
% \label{eq:teacher_forcing}
% \end{equation}

% The teacher forcing rate decays over training:
% \begin{equation}
% p_{\text{TF}}(\text{epoch}) = 
% \begin{cases}
% 0.5 & \text{epoch} < 10 \\
% 0.2 & 10 \leq \text{epoch} < 30 \\
% 0.05 & \text{epoch} \geq 30
% \end{cases}
% \label{eq:teacher_forcing_schedule}
% \end{equation}

% This provides stability early (when the model is weak) while ensuring the model learns to handle its own predictions later.

% %------------------------------------------------------------------------------
% \subsection{Noise Injection for Robustness}
% \label{subsec:noise_injection}
% %------------------------------------------------------------------------------

% After epoch 20, we inject Gaussian noise into the input history to improve robustness:
% \begin{equation}
% \tilde{\mathbf{U}}_{\text{hist}} = \mathbf{U}_{\text{hist}} + \epsilon \mathcal{N}(0, I)
% \label{eq:noise_injection}
% \end{equation}

% where:
% \begin{equation}
% \epsilon = 0.01 \times \min\left(1.0, \frac{\text{epoch}}{50}\right)
% \label{eq:noise_scale}
% \end{equation}

% This prevents overfitting to exact trajectories and improves generalization to perturbed initial conditions.

% %------------------------------------------------------------------------------
% \subsection{Optimization Details}
% \label{subsec:optimization}
% %------------------------------------------------------------------------------

% \paragraph{Optimizer.}
% We use AdamW optimizer with:
% \begin{equation}
% \theta_{t+1} = \theta_t - \eta \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} - \lambda_{\text{WD}} \theta_t
% \label{eq:adamw}
% \end{equation}

% where:
% \begin{itemize}
%     \item Learning rate: \(\eta = 0.001\)
%     \item Weight decay: \(\lambda_{\text{WD}} = 10^{-4}\)
%     \item \(\beta_1 = 0.9\), \(\beta_2 = 0.999\)
%     \item \(\epsilon = 10^{-8}\)
% \end{itemize}

% \paragraph{Learning Rate Schedule.}
% Cosine annealing reduces the learning rate:
% \begin{equation}
% \eta_t = \eta_{\min} + \frac{\eta_{\max} - \eta_{\min}}{2}\left(1 + \cos\left(\frac{t}{T_{\max}}\pi\right)\right)
% \label{eq:cosine_annealing}
% \end{equation}

% with \(\eta_{\max} = 0.001\), \(\eta_{\min} = 0\), and \(T_{\max} = 50\) epochs.

% \paragraph{Gradient Clipping.}
% To prevent gradient explosion during backpropagation through time:
% \begin{equation}
% \|\nabla_{\theta} \mathcal{L}\| > 1.0 \implies \nabla_{\theta} \mathcal{L} \leftarrow \frac{\nabla_{\theta} \mathcal{L}}{\|\nabla_{\theta} \mathcal{L}\|}
% \label{eq:gradient_clipping}
% \end{equation}

% \paragraph{Batch Size and Epochs.}
% \begin{itemize}
%     \item Batch size: \(B = 32\)
%     \item Total epochs: 50
%     \item Training samples per epoch: \(\frac{1000}{32} \approx 31\) batches
% \end{itemize}

% %==============================================================================
% \section{Evaluation Metrics}
% \label{sec:evaluation_metrics}
% %==============================================================================

% We employ comprehensive metrics to assess model performance across multiple dimensions.

% %------------------------------------------------------------------------------
% \subsection{Accuracy Metrics}
% \label{subsec:accuracy_metrics}
% %------------------------------------------------------------------------------

% \paragraph{Mean Squared Error (MSE).}
% \begin{equation}
% \text{MSE} = \frac{1}{TN} \sum_{t=1}^T \sum_{i=1}^N (\hat{u}_i^t - u_i^t)^2
% \label{eq:mse_metric}
% \end{equation}

% \paragraph{Relative L2 Error.}
% Scale-invariant error:
% \begin{equation}
% E_{L^2}^{\text{rel}} = \frac{\|\hat{\mathbf{u}} - \mathbf{u}\|_2}{\|\mathbf{u}\|_2 + \epsilon}
% \label{eq:relative_l2}
% \end{equation}

% \paragraph{Per-Timestep Relative L2.}
% \begin{equation}
% E_{L^2}^{\text{rel}}(t) = \frac{\sqrt{\sum_{i=1}^N (\hat{u}_i^t - u_i^t)^2}}{\sqrt{\sum_{i=1}^N (u_i^t)^2} + \epsilon}
% \label{eq:timestep_relative_l2}
% \end{equation}

% This reveals when predictions diverge during rollout.

% \paragraph{Peak Signal-to-Noise Ratio (PSNR).}
% \begin{equation}
% \text{PSNR} = 20 \log_{10}\left(\frac{\text{RANGE}}{\sqrt{\text{MSE}}}\right)
% \label{eq:psnr}
% \end{equation}
% where \(\text{RANGE} = \max(u) - \min(u)\).

% Higher PSNR indicates better reconstruction quality.

% \paragraph{Structural Similarity Index (SSIM).}
% Measures perceptual similarity:
% \begin{equation}
% \text{SSIM}(\mathbf{u}, \hat{\mathbf{u}}) = \frac{(2\mu_u\mu_{\hat{u}} + C_1)(2\sigma_{u\hat{u}} + C_2)}{(\mu_u^2 + \mu_{\hat{u}}^2 + C_1)(\sigma_u^2 + \sigma_{\hat{u}}^2 + C_2)}
% \label{eq:ssim}
% \end{equation}

% SSIM \(\in [-1, 1]\), with 1 indicating perfect structural match.

% %------------------------------------------------------------------------------
% \subsection{Physical Consistency Metrics}
% \label{subsec:physical_metrics}
% %------------------------------------------------------------------------------

% \paragraph{Mass Conservation Error.}
% For sinusoidal IC where \(M_0 \approx 0\):
% \begin{equation}
% E_{\text{mass}}(t) = |M(t) - M_0|
% \label{eq:mass_error}
% \end{equation}

% For non-zero initial mass:
% \begin{equation}
% E_{\text{mass}}^{\text{rel}}(t) = \frac{|M(t) - M_0|}{|M_0| + \epsilon}
% \label{eq:mass_error_relative}
% \end{equation}

% \paragraph{Energy Dissipation Consistency.}
% Verify that predicted energy decreases:
% \begin{equation}
% \text{Monotonicity} = \frac{1}{T-1}\sum_{t=1}^{T-1} \mathbb{1}[E(t+1) \leq E(t) + \epsilon_{\text{tol}}]
% \label{eq:energy_monotonicity_metric}
% \end{equation}

% Perfect monotonicity gives 1.0; violations reduce this score.

% %------------------------------------------------------------------------------
% \subsection{Correlation and Spectral Metrics}
% \label{subsec:correlation_metrics}
% %------------------------------------------------------------------------------

% \paragraph{Pearson Correlation per Timestep.}
% \begin{equation}
% \rho(t) = \frac{\text{Cov}(\hat{u}^t, u^t)}{\sigma_{\hat{u}^t} \sigma_{u^t}}
% \label{eq:pearson_correlation}
% \end{equation}

% High correlation (\(\rho \approx 1\)) indicates the model captures spatial patterns accurately.

% \paragraph{Energy Spectrum Error.}
% Fourier analysis reveals whether the model captures correct wavenumber content:
% \begin{equation}
% \hat{U}^t(k) = \text{FFT}(u^t), \quad S(k) = |\hat{U}(k)|^2
% \label{eq:spectrum}
% \end{equation}

% Spectrum error:
% \begin{equation}
% E_{\text{spec}} = \frac{1}{K} \sum_{k=1}^{K/2} \left|\log_{10} S_{\text{pred}}(k) - \log_{10} S_{\text{true}}(k)\right|
% \label{eq:spectrum_error}
% \end{equation}

% %==============================================================================
% \section{Implementation Details}
% \label{sec:implementation}
% %==============================================================================

% %------------------------------------------------------------------------------
% \subsection{Software and Hardware}
% \label{subsec:software_hardware}
% %------------------------------------------------------------------------------

% \paragraph{Framework.}
% All models are implemented in PyTorch 2.0+ with CUDA support for GPU acceleration.

% \paragraph{Hardware.}
% Training is performed on:
% \begin{itemize}
%     \item GPU: NVIDIA A100 (40GB) or equivalent
%     \item Training time: \(\sim\)30 minutes for 50 epochs
%     \item Inference time: \(\sim\)10ms per rollout step
% \end{itemize}

% %------------------------------------------------------------------------------
% \subsection{Reproducibility}
% \label{subsec:reproducibility}
% %------------------------------------------------------------------------------

% To ensure reproducibility:
% \begin{itemize}
%     \item Random seeds fixed: PyTorch, NumPy, Python (\texttt{seed=42})
%     \item Deterministic algorithms enabled where available
%     \item Complete hyperparameter logging
%     \item Model checkpoints saved every 10 epochs
% \end{itemize}

% %------------------------------------------------------------------------------
% \subsection{Code Organization}
% \label{subsec:code_organization}
% %------------------------------------------------------------------------------

% Our implementation follows modular design:

% \begin{verbatim}
% burgers_solver/
% |-- data_generation.py      # Finite difference solver
% |-- dataset.py              # PyTorch Dataset/DataLoader
% |-- model.py                # Neural architecture
% |   |-- CausalTemporalAttention
% |   +-- ImprovedBurgersNet
% |-- training.py             # Training loop
% |   |-- Loss functions
% |   +-- Curriculum learning
% |-- metrics.py              # Evaluation metrics
% +-- visualization.py        # Plotting utilities
% \end{verbatim}

% The complete source code and pre-trained models are available in our GitHub repository\footnote{\url{https://github.com/Samuel-Chapuis/ML_Differential_Solver}}.

% %==============================================================================
% \section{Summary}
% \label{sec:methodology_summary}
% %==============================================================================

% This chapter has presented our comprehensive methodology for neural PDE solving:

% \begin{enumerate}
%     \item \textbf{High-quality data generation} using stable finite difference methods with upwind advection and central difference diffusion
%     \item \textbf{Novel architecture} combining causal temporal attention with convolutional decoding for effective spatiotemporal modeling
%     \item \textbf{Physics-informed training} via energy dissipation constraints and gradient matching losses
%     \item \textbf{Curriculum learning} with scheduled rollout depth and teacher forcing for improved stability
%     \item \textbf{Comprehensive evaluation} across accuracy, physical consistency, and spectral metrics
% \end{enumerate}

% The next chapter presents experimental results demonstrating the effectiveness of these design choices and comparing performance against baseline methods.

% \paragraph{Transition to Chapter 5.}
% Having established our complete methodological framework, we now turn to empirical validation. Chapter~\ref{chap:experiments} presents comprehensive experiments evaluating our approach across multiple metrics, analyzing the contribution of each architectural component through ablation studies, and comparing performance against established baseline methods.


%==============================================================================
\section{Immediate Technical Improvements}
\label{sec:immediate_improvements}
%==============================================================================

Our current implementation has revealed several critical technical issues that require immediate attention to ensure robust and accurate predictions.

%------------------------------------------------------------------------------
\subsection{Boundary Condition Treatment}
\label{subsec:boundary_conditions}
%------------------------------------------------------------------------------

\paragraph{Current Issue.}
Our model currently employs symmetric padding at domain boundaries, which creates artificial artifacts in the predicted solutions. Specifically, the symmetry assumption \(u_{n+1} = u_0\) violates the physics of the Burgers equation near boundaries, leading to incorrect diffusion behavior and asymmetric predictions where symmetry is expected.

\paragraph{Proposed Solution.}
We will modify the padding configuration to use either zero padding (\(u_{n+1} = 0\)) or replication padding (\(u_{n+1} = u_n\)). Replication padding is preferred as it maintains local gradient continuity while avoiding the introduction of spurious boundary values. This change requires minimal modification to the model configuration and should immediately improve prediction quality near domain boundaries.

\paragraph{Implementation.}
The padding scheme can be adjusted in the convolutional layer definitions by changing the \texttt{padding\_mode} parameter from \texttt{'circular'} to \texttt{'replicate'} or \texttt{'zeros'}. We will evaluate both options and select based on empirical performance on held-out test trajectories.

%------------------------------------------------------------------------------
\subsection{Training Performance Optimization}
\label{subsec:training_optimization}
%------------------------------------------------------------------------------

\paragraph{Current Issue.}
Training currently requires several hours for convergence, whereas comparable models train in minutes. This inefficiency severely limits our ability to perform hyperparameter sweeps and architecture experiments.

\paragraph{Suspected Causes.}
Profiling suggests potential inefficiencies in the autoregressive rollout loop during training, possibly due to:
\begin{itemize}
    \item Unnecessary gradient computations during intermediate rollout steps
    \item Inefficient tensor operations in the temporal attention mechanism
    \item Suboptimal batch processing in the training loop
    \item Redundant data transfers between CPU and GPU
\end{itemize}

\paragraph{Proposed Solution.}
We will refactor the training loop following established best practices~\cite{chen2018neural}:
\begin{enumerate}
    \item Implement gradient checkpointing to reduce memory overhead during backpropagation through time
    \item Vectorize rollout operations where possible to leverage GPU parallelism
    \item Profile the code systematically using PyTorch Profiler to identify bottlenecks
    \item Review optimized reference implementations to identify architectural inefficiencies
\end{enumerate}

%------------------------------------------------------------------------------
\subsection{Generalization Verification}
\label{subsec:generalization_verification}
%------------------------------------------------------------------------------

\paragraph{Critical Finding.}
Our model demonstrates surprising generalization: after training on trajectories with a single viscosity value \(\nu_{\text{train}}\), it successfully predicts dynamics for unseen viscosity values \(\nu_{\text{test}} \neq \nu_{\text{train}}\). This is unexpected, as the viscosity parameter fundamentally changes the PDE dynamics.

\paragraph{Verification Needed.}
We must rigorously verify that this generalization is genuine and not an artifact of:
\begin{itemize}
    \item The model inadvertently accessing viscosity values from the dataset
    \item Similar-looking dynamics arising from different viscosity values
    \item Limited test set diversity failing to stress-test generalization
\end{itemize}

\paragraph{Experimental Protocol.}
We will:
\begin{enumerate}
    \item Remove all explicit viscosity inputs from the model architecture
    \item Generate test trajectories spanning viscosity values \(\nu \in [0.001, 0.5]\) well outside the training range
    \item Compute quantitative error metrics (relative \(L^2\), PSNR) as a function of \(|\nu_{\text{test}} - \nu_{\text{train}}|\)
    \item Compare against a baseline model explicitly conditioned on viscosity values
\end{enumerate}

If genuine, this generalization capability represents a significant scientific finding that warrants detailed analysis and reporting.

%==============================================================================
\section{Short-Term Research Objectives}
\label{sec:short_term_objectives}
%==============================================================================

Building on the immediate technical improvements, we outline our research agenda for the next two months.

%------------------------------------------------------------------------------
\subsection{Advanced Training Techniques}
\label{subsec:advanced_training}
%------------------------------------------------------------------------------

\paragraph{Teacher Forcing Implementation.}
Teacher forcing is a curriculum learning technique where the model is initially trained using ground truth states as input, gradually transitioning to using its own predictions. This technique is standard in the Neural ODE literature~\cite{chen2018neural} and available in reference NFTM implementations. We will implement scheduled teacher forcing with an exponentially decaying schedule:
\begin{equation}
p_{\text{teacher}}(t) = p_0 \exp\left(-\frac{t}{\tau}\right)
\label{eq:teacher_forcing_schedule}
\end{equation}
where \(p_{\text{teacher}}\) is the probability of using ground truth at training step \(t\), \(p_0 = 0.9\) is the initial probability, and \(\tau\) controls decay rate.

\paragraph{Hyperparameter Optimization.}
We will deploy Weights \& Biases with Bayesian optimization to systematically explore the hyperparameter space. Key parameters include:
\begin{itemize}
    \item Learning rate and scheduler parameters
    \item Rollout depth for curriculum learning
    \item Physics loss weighting coefficients \(\lambda_{\text{energy}}\), \(\lambda_{\text{grad}}\)
    \item Architectural choices (kernel size, number of channels, attention heads)
\end{itemize}

The Bayesian approach constructs a surrogate model of the loss landscape to intelligently propose parameter combinations, dramatically reducing computational cost compared to grid search.

%------------------------------------------------------------------------------
\subsection{Extension to Two-Dimensional Burgers Equation}
\label{subsec:2d_burgers}
%------------------------------------------------------------------------------

The natural next step is extending our framework to the two-dimensional Burgers equation:
\begin{equation}
\frac{\partial u}{\partial t} + u\frac{\partial u}{\partial x} + v\frac{\partial u}{\partial y} = \nu\left(\frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2}\right)
\label{eq:2d_burgers_u}
\end{equation}
\begin{equation}
\frac{\partial v}{\partial t} + u\frac{\partial v}{\partial x} + v\frac{\partial v}{\partial y} = \nu\left(\frac{\partial^2 v}{\partial x^2} + \frac{\partial^2 v}{\partial y^2}\right)
\label{eq:2d_burgers_v}
\end{equation}

\paragraph{Staged Approach.}
We will proceed incrementally:
\begin{enumerate}
    \item \textbf{Phase 1}: Solve 2D Burgers in an empty domain (no obstacles)
    \item \textbf{Phase 2}: Introduce simple geometric obstacles (cylinders, squares) to create vortex shedding and complex flow patterns
    \item \textbf{Phase 3}: Test generalization to unseen obstacle configurations
\end{enumerate}

The 2D case introduces significantly richer spatiotemporal dynamics including vortex formation, turbulent cascades, and boundary layer effects, providing a more stringent test of our architecture's capacity.

\paragraph{Architectural Modifications.}
The extension requires:
\begin{itemize}
    \item Replacing 1D convolutions with 2D convolutions
    \item Extending attention mechanisms to handle 2D spatial dimensions
    \item Modifying the memory field from \(\mathbb{R}^{N}\) to \(\mathbb{R}^{N_x \times N_y \times 2}\) (two velocity components)
\end{itemize}

%------------------------------------------------------------------------------
\subsection{Baseline Comparisons and Alignment}
\label{subsec:baseline_comparisons}
%------------------------------------------------------------------------------

To contextualize our results within the broader literature, we will implement and compare against established baseline methods:

\paragraph{Neural ODE.}
Neural ODEs~\cite{chen2018neural} represent the most closely related approach. They parameterize the time derivative directly:
\begin{equation}
\frac{du}{dt} = f_{\theta}(u(t), t)
\label{eq:neural_ode}
\end{equation}
and use ODE solvers for integration. We will implement a CNN-based Neural ODE and compare prediction accuracy, computational cost, and stability during long rollouts.

\paragraph{PINNs.}
While PINNs~\cite{raissi2019physics} are known to struggle with the Burgers equation due to spectral bias~\cite{rahaman2019spectral}, implementing a PINN baseline provides a reference point for data efficiency and physical consistency. We will train PINNs with identical physics constraints to isolate the effect of the training paradigm (physics loss vs. trajectory matching).

\paragraph{Evaluation Protocol.}
All methods will be trained on identical datasets and evaluated on:
\begin{itemize}
    \item Relative \(L^2\) error over rollout horizon
    \item Mass conservation accuracy
    \item Energy dissipation monotonicity
    \item Computational cost (training time, inference speed)
\end{itemize}

%==============================================================================
\section{Medium-Term Development}
\label{sec:medium_term_development}
%==============================================================================

Our medium-term objectives focus on demonstrating generalization across multiple PDE families and developing a unified neural solver.

%------------------------------------------------------------------------------
\subsection{Incompressible Navier-Stokes Equations}
\label{subsec:navier_stokes}
%------------------------------------------------------------------------------

The incompressible Navier-Stokes equations represent a significant step beyond Burgers, introducing a pressure term and incompressibility constraint:
\begin{align}
\frac{\partial \mathbf{u}}{\partial t} + (\mathbf{u} \cdot \nabla)\mathbf{u} &= -\nabla p + \nu \nabla^2 \mathbf{u} \label{eq:ns_momentum} \\
\nabla \cdot \mathbf{u} &= 0 \label{eq:ns_incompressibility}
\end{align}

\paragraph{Technical Challenges.}
The incompressibility constraint (\ref{eq:ns_incompressibility}) couples the velocity and pressure fields, requiring either:
\begin{itemize}
    \item Projection methods to enforce divergence-free velocities
    \item Pressure Poisson equation solvers
    \item Specialized loss functions penalizing divergence
\end{itemize}

We will investigate soft constraint approaches where the model is trained to minimize divergence rather than enforcing it exactly, following recent work on differentiable physics~\cite{raissi2019physics}.

%------------------------------------------------------------------------------
\subsection{Multi-PDE Generalization}
\label{subsec:multi_pde}
%------------------------------------------------------------------------------

A key scientific question is whether a single architecture can learn dynamics across fundamentally different PDE families. We will test our framework on:

\paragraph{Heat Equation.}
The parabolic heat equation:
\begin{equation}
\frac{\partial u}{\partial t} = \alpha \nabla^2 u
\label{eq:heat}
\end{equation}
represents pure diffusion without advection, testing whether the model can learn dynamics dominated by smoothing.

\paragraph{Reaction-Diffusion Systems.}
Systems like the Gray-Scott model couple diffusion with nonlinear reaction terms:
\begin{align}
\frac{\partial u}{\partial t} &= D_u \nabla^2 u - uv^2 + F(1-u) \label{eq:gray_scott_u} \\
\frac{\partial v}{\partial t} &= D_v \nabla^2 v + uv^2 - (F+k)v \label{eq:gray_scott_v}
\end{align}
These systems exhibit rich pattern formation (spots, stripes, spirals) that stress-test the model's ability to capture emergent spatial structures.

\paragraph{Evaluation Criteria.}
For each PDE, we will assess:
\begin{enumerate}
    \item Whether the same architecture (without hyperparameter tuning) achieves reasonable accuracy
    \item What architectural components transfer successfully across PDEs
    \item Whether pre-training on one PDE improves performance on others
\end{enumerate}

%==============================================================================
\section{Long-Term Scientific Goals}
\label{sec:long_term_goals}
%==============================================================================

Our long-term vision extends beyond accurate PDE simulation to fundamental questions about learning physical laws from data.

%------------------------------------------------------------------------------
\subsection{Equation Discovery from Data}
\label{subsec:equation_discovery}
%------------------------------------------------------------------------------

Can a neural network discover the governing PDE from trajectory data alone, without being told the equation form or parameter values? This inverse problem has profound implications for scientific modeling of complex systems where equations are unknown.

\paragraph{Approach.}
We will train models on trajectory data \emph{without} providing:
\begin{itemize}
    \item The explicit PDE form
    \item Parameter values (e.g., viscosity \(\nu\))
    \item Conservation laws or symmetries
\end{itemize}

Success would be demonstrated by the model learning dynamics that:
\begin{enumerate}
    \item Match ground truth trajectories quantitatively
    \item Respect underlying conservation laws (mass, energy, momentum)
    \item Generalize to initial conditions and parameter regimes outside the training distribution
\end{enumerate}

\paragraph{Interpretability.}
A key challenge is extracting interpretable equations from the learned model. We will investigate:
\begin{itemize}
    \item Symbolic regression techniques to recover closed-form equations~\cite{long2018pde}
    \item Analysis of learned convolutional filters to identify differential operators
    \item Dimensionality reduction to discover latent coordinates aligned with physical variables
\end{itemize}

%------------------------------------------------------------------------------
\subsection{Conservation Law Enforcement}
\label{subsec:conservation_laws}
%------------------------------------------------------------------------------

\paragraph{Current State.}
Hard-coding conservation laws (mass, momentum, energy) into neural network architectures remains an open problem. Existing approaches include:
\begin{itemize}
    \item Hamiltonian Neural Networks~\cite{greydanus2019hamiltonian}: Parameterize energy functions for conservative systems
    \item Port-Hamiltonian formulations: Extend to dissipative systems
    \item Projection methods: Post-process predictions to satisfy constraints
\end{itemize}

However, these methods either require restrictive assumptions (no dissipation) or conflict with gradient-based optimization when implemented as hard constraints.

\paragraph{Proposed Research Direction.}
We will develop \emph{soft constraint} methods that:
\begin{enumerate}
    \item Penalize violations of conservation laws through auxiliary loss terms
    \item Learn to satisfy constraints approximately rather than exactly
    \item Adaptively weight conservation vs. accuracy during training
\end{enumerate}

For example, mass conservation can be enforced softly via:
\begin{equation}
\mathcal{L}_{\text{mass}} = \lambda_{\text{mass}} \left|\int_{\Omega} u_{\text{pred}}(x,t)\,dx - \int_{\Omega} u_0(x)\,dx\right|^2
\label{eq:soft_mass_constraint}
\end{equation}

We will investigate whether soft constraints provide sufficient regularization to improve long-term stability without the optimization difficulties of hard constraints.

%------------------------------------------------------------------------------
\subsection{Improved Long-Horizon Prediction}
\label{subsec:long_horizon}
%------------------------------------------------------------------------------

\paragraph{Current Limitation.}
All current state-of-the-art methods for neural PDE solving suffer from \emph{divergence} during long autoregressive rollouts. Errors accumulate, and predictions eventually become physically implausible or numerically unstable.

\paragraph{Research Questions.}
\begin{itemize}
    \item What is the fundamental limit on prediction horizon for learned simulators?
    \item Can we develop provable stability guarantees for neural time-steppers?
    \item How do architectural choices (attention vs. convolution, residual connections) affect error accumulation?
\end{itemize}

\paragraph{Proposed Investigations.}
We will explore:
\begin{enumerate}
    \item \textbf{Multi-step prediction}: Train models to predict multiple steps ahead rather than single steps, reducing error accumulation
    \item \textbf{Stability-aware training}: Penalize predictions that amplify high-frequency modes
    \item \textbf{Hybrid approaches}: Combine learned time-steppers with traditional stabilization techniques (spectral filtering, artificial viscosity)
\end{enumerate}

%==============================================================================
\section{Summary and Timeline}
\label{sec:summary_timeline}
%==============================================================================

We conclude by summarizing our roadmap with an approximate timeline:

\paragraph{Immediate (1-2 weeks).}
\begin{itemize}
    \item Fix boundary padding configuration
    \item Optimize training performance
    \item Verify generalization to unseen viscosity values
    \item Set up Weights \& Biases for hyperparameter tuning
\end{itemize}

\paragraph{Short-term (1-2 months).}
\begin{itemize}
    \item Implement teacher forcing and curriculum learning
    \item Extend to 2D Burgers equation with obstacles
    \item Compare against Neural ODE and PINN baselines
    \item Systematic hyperparameter optimization
\end{itemize}

\paragraph{Medium-term (3-4 months).}
\begin{itemize}
    \item Incompressible Navier-Stokes implementation
    \item Multi-PDE generalization experiments (Heat, Reaction-Diffusion)
    \item Equation discovery investigations
\end{itemize}

\paragraph{Long-term (ongoing).}
\begin{itemize}
    \item Conservation law enforcement research
    \item Long-horizon prediction stability analysis
    \item Publication preparation and alignment with literature
\end{itemize}

This roadmap balances immediate practical improvements with ambitious scientific goals, positioning our work to make both methodological contributions (improved neural PDE solvers) and conceptual contributions (understanding what neural networks can learn about physical laws).
