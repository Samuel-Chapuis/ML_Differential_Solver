\chapter{Next Steps}
\label{chap:nextSteps}


% This chapter presents our comprehensive methodology for solving the one-dimensional Burgers equation using Neural Field Turing Machines (NFTMs). We detail the complete pipeline from data generation through model architecture, training procedures, and evaluation metrics. Our approach builds upon the NFTM framework while introducing several key innovations to improve stability, physical consistency, and generalization capabilities.

% %==============================================================================
% \section{Overview of the Proposed Pipeline}
% \label{sec:pipeline_overview}
% %==============================================================================

% Our neural PDE solver pipeline consists of four main components, illustrated in Figure~\ref{fig:pipeline_overview}:

% \begin{enumerate}
%     \item \textbf{Physics-Based Data Generation}: High-fidelity numerical solutions of the Burgers equation using stable finite difference methods
%     \item \textbf{Neural Architecture}: A causal temporal attention mechanism combined with convolutional decoders
%     \item \textbf{Training Strategy}: Multi-stage curriculum learning with physics-informed regularization
%     \item \textbf{Evaluation Framework}: Comprehensive metrics assessing accuracy, stability, and physical consistency
% \end{enumerate}

% The complete workflow operates as follows: training trajectories are generated by solving the Burgers equation numerically for various initial conditions and viscosity parameters. These trajectories are then used to train a neural network that learns to predict future states given a window of historical states. During inference, the model performs autoregressive rollout to generate long-term predictions. Finally, predictions are evaluated against ground truth using multiple metrics to assess both numerical accuracy and physical plausibility.

% %==============================================================================
% \section{Data Generation: Stable Numerical Solver}
% \label{sec:data_generation}
% %==============================================================================

% %------------------------------------------------------------------------------
% \subsection{Finite Difference Scheme}
% \label{subsec:finite_difference}
% %------------------------------------------------------------------------------

% To generate high-quality training data, we implement a stable numerical solver for the one-dimensional Burgers equation:
% \begin{equation}
% \frac{\partial u}{\partial t} + u\frac{\partial u}{\partial x} = \nu\frac{\partial^2 u}{\partial x^2}
% \label{eq:burgers_data_gen}
% \end{equation}

% We discretize the spatial domain \(x \in [0, 2\pi]\) with periodic boundary conditions using \(N = 128\) uniformly spaced grid points:
% \begin{equation}
% x_i = \frac{2\pi i}{N}, \quad i = 0, 1, \ldots, N-1
% \label{eq:spatial_discretization}
% \end{equation}

% The spatial step size is:
% \begin{equation}
% \Delta x = \frac{2\pi}{N} \approx 0.049
% \label{eq:dx}
% \end{equation}

% \paragraph{Diffusion Term.}
% The viscous diffusion term is discretized using central differences, which is second-order accurate in space:
% \begin{equation}
% \frac{\partial^2 u}{\partial x^2}\Big|_{x_i} \approx \frac{u_{i+1} - 2u_i + u_{i-1}}{\Delta x^2} = D_i
% \label{eq:diffusion_discrete}
% \end{equation}

% This stencil is applied with periodic boundary conditions: \(u_{-1} = u_{N-1}\) and \(u_N = u_0\).

% \paragraph{Advection Term.}
% For the nonlinear advection term \(u\frac{\partial u}{\partial x}\), we employ an upwind differencing scheme to maintain numerical stability:
% \begin{equation}
% \frac{\partial u}{\partial x}\Big|_{x_i} \approx 
% \begin{cases}
% \frac{u_i - u_{i-1}}{\Delta x} & \text{if } u_i > 0 \text{ (backward difference)} \\
% \frac{u_{i+1} - u_i}{\Delta x} & \text{if } u_i \leq 0 \text{ (forward difference)}
% \end{cases}
% \label{eq:upwind_scheme}
% \end{equation}

% The upwind scheme selects the differencing direction based on the local flow direction, which prevents spurious oscillations in regions with steep gradients or shocks.

% \paragraph{Time Integration.}
% We use the forward Euler method for time integration with timestep \(\Delta t = 0.01\):
% \begin{equation}
% u_i^{n+1} = u_i^n - \Delta t \cdot u_i^n \left(\frac{\partial u}{\partial x}\right)_i^n + \Delta t \cdot \nu \cdot D_i^n
% \label{eq:forward_euler}
% \end{equation}

% To ensure stability, the timestep satisfies the CFL (Courant-Friedrichs-Lewy) condition:
% \begin{equation}
% \Delta t \leq \min\left(\frac{\Delta x}{|u_{\max}|}, \frac{\Delta x^2}{2\nu}\right)
% \label{eq:cfl_condition_data}
% \end{equation}

% For our parameters (\(\Delta x \approx 0.049\), \(\nu \in [0.01, 0.1]\), \(|u_{\max}| \lesssim 1\)), \(\Delta t = 0.01\) satisfies this stability criterion.

% %------------------------------------------------------------------------------
% \subsection{Dataset Construction}
% \label{subsec:dataset_construction}
% %------------------------------------------------------------------------------

% \paragraph{Sampling Strategy.}
% For each trajectory in our dataset, we randomly sample:
% \begin{itemize}
%     \item \textbf{Viscosity parameter}: \(\nu \sim \mathcal{U}(0.01, 0.1)\), where \(\mathcal{U}(a,b)\) denotes the uniform distribution
%     \item \textbf{Initial condition}: \(u_0(x) = \sin(x)\) (smooth sinusoidal profile)
% \end{itemize}

% The sinusoidal initial condition is chosen because:
% \begin{enumerate}
%     \item It is smooth and differentiable (realistic physical scenario)
%     \item It produces rich dynamics including wave steepening and eventual shock formation for small \(\nu\)
%     \item It has zero mean: \(\int_0^{2\pi} \sin(x)\,dx = 0\), which tests mass conservation
%     \item It provides analytical benchmarks via the Cole-Hopf transformation
% \end{enumerate}

% \paragraph{Trajectory Length.}
% Each trajectory consists of \(T = 100\) timesteps, spanning total time \(T_{\text{total}} = 1.0\) seconds. The state at each timestep is stored:
% \begin{equation}
% \mathcal{T}_{\text{trajectory}} = \{u^0, u^1, \ldots, u^{99}\}, \quad u^t \in \mathbb{R}^{128}
% \label{eq:trajectory}
% \end{equation}

% \paragraph{Dataset Split.}
% We generate:
% \begin{itemize}
%     \item \textbf{Training set}: 1000 trajectories (100,000 state snapshots)
%     \item \textbf{Test set}: 200 trajectories (20,000 state snapshots)
% \end{itemize}

% No trajectory in the test set has the same viscosity parameter \(\nu\) as any training trajectory, ensuring evaluation on truly unseen parameter regimes.

% %------------------------------------------------------------------------------
% \subsection{Quality Assurance}
% \label{subsec:quality_assurance}
% %------------------------------------------------------------------------------

% We verify the quality of generated data through:

% \paragraph{Mass Conservation.}
% For periodic boundary conditions, the total mass \(M(t) = \int_0^{2\pi} u(x,t)\,dx\) should be conserved. Numerically:
% \begin{equation}
% M^n = \Delta x \sum_{i=0}^{N-1} u_i^n
% \label{eq:mass_discrete}
% \end{equation}

% We verify:
% \begin{equation}
% \left|\frac{M^{99} - M^0}{M^0 + \epsilon}\right| < 10^{-3}
% \label{eq:mass_conservation_check}
% \end{equation}
% where \(\epsilon = 10^{-6}\) prevents division by zero for the sinusoidal IC where \(M^0 \approx 0\).

% \paragraph{Energy Dissipation.}
% The kinetic energy:
% \begin{equation}
% E^n = \frac{\Delta x}{2} \sum_{i=0}^{N-1} (u_i^n)^2
% \label{eq:energy_discrete}
% \end{equation}
% must monotonically decrease due to viscous dissipation:
% \begin{equation}
% E^{n+1} \leq E^n + \epsilon_{\text{tol}}
% \label{eq:energy_monotonicity}
% \end{equation}
% where \(\epsilon_{\text{tol}} = 10^{-6}\) accounts for numerical roundoff.

% \paragraph{Stability Check.}
% We reject any trajectory where:
% \begin{equation}
% \max_{t,i} |u_i^t| > 5 \quad \text{or} \quad \exists\, (t,i): \text{isnan}(u_i^t) \vee \text{isinf}(u_i^t)
% \label{eq:stability_check}
% \end{equation}

% These checks ensure only physically plausible and numerically stable trajectories enter the training dataset.

% %==============================================================================
% \section{Neural Architecture: Improved NFTM with Temporal Attention}
% \label{sec:architecture}
% %==============================================================================

% Our neural architecture enhances the basic NFTM framework with causal temporal attention and multi-scale processing.

% %------------------------------------------------------------------------------
% \subsection{Input Representation and Window-Based Prediction}
% \label{subsec:input_representation}
% %------------------------------------------------------------------------------

% \paragraph{Historical Window.}
% Rather than predicting the next state from a single previous state, we use a sliding window of length \(W\):
% \begin{equation}
% \mathbf{U}_{\text{hist}} = [u^{t-W+1}, u^{t-W+2}, \ldots, u^t] \in \mathbb{R}^{W \times N}
% \label{eq:history_window}
% \end{equation}

% In our implementation, \(W = 20\), providing the model with approximately 0.2 seconds of history to capture temporal dynamics.

% \paragraph{Prediction Task.}
% Given \(\mathbf{U}_{\text{hist}}\), the model predicts the next state:
% \begin{equation}
% \hat{u}^{t+1} = f_{\theta}(\mathbf{U}_{\text{hist}})
% \label{eq:prediction_task}
% \end{equation}

% For autoregressive rollout over \(K\) steps:
% \begin{equation}
% \begin{aligned}
% \hat{u}^{t+1} &= f_{\theta}([u^{t-W+2}, \ldots, u^t]) \\
% \hat{u}^{t+2} &= f_{\theta}([u^{t-W+3}, \ldots, u^t, \hat{u}^{t+1}]) \\
% &\vdots \\
% \hat{u}^{t+K} &= f_{\theta}([\hat{u}^{t+K-W}, \ldots, \hat{u}^{t+K-1}])
% \end{aligned}
% \label{eq:autoregressive_rollout}
% \end{equation}

% This autoregressive formulation allows arbitrary-length predictions but accumulates errors over time.

% %------------------------------------------------------------------------------
% \subsection{Causal Temporal Attention Mechanism}
% \label{subsec:temporal_attention}
% %------------------------------------------------------------------------------

% \paragraph{Motivation.}
% The temporal attention mechanism allows the model to selectively weight different historical timesteps based on their relevance to the current prediction. This is particularly important for PDEs where recent states may be more informative than distant past states, but long-range temporal dependencies still matter.

% \paragraph{Architecture Overview.}
% The causal temporal attention module processes the history window \(\mathbf{U}_{\text{hist}} \in \mathbb{R}^{B \times W \times N}\) (batch size \(B\), window size \(W\), spatial points \(N\)) in three stages:

% \begin{enumerate}
%     \item \textbf{Feature Embedding}: Each timestep is embedded into a high-dimensional feature space
%     \item \textbf{Attention Computation}: Query-Key-Value attention focuses on relevant historical information
%     \item \textbf{Context Aggregation}: Weighted temporal features are aggregated into a context vector
% \end{enumerate}

% \paragraph{Feature Embedding.}
% First, we embed each spatial field into a feature space using a 1D convolutional layer:
% \begin{equation}
% \mathbf{H}^t = \text{GELU}(\text{Conv1D}(u^t; \theta_{\text{emb}}))
% \label{eq:embedding}
% \end{equation}
% where \(\mathbf{H}^t \in \mathbb{R}^{d \times N}\) and \(d = 32\) is the embedding dimension.

% For the full window:
% \begin{equation}
% \mathbf{H} = [\mathbf{H}^{t-W+1}, \ldots, \mathbf{H}^t] \in \mathbb{R}^{W \times d \times N}
% \label{eq:embedded_history}
% \end{equation}

% \paragraph{Query-Key-Value Projections.}
% We use the most recent frame \(\mathbf{H}^t\) to generate queries \(\mathbf{Q}\), while all historical frames generate keys \(\mathbf{K}\) and values \(\mathbf{V}\):
% \begin{align}
% \mathbf{Q} &= \text{Conv1D}(\mathbf{H}^t; \theta_Q) \in \mathbb{R}^{d \times N} \label{eq:query} \\
% \mathbf{K}^{\tau} &= \text{Conv1D}(\mathbf{H}^{\tau}; \theta_K) \in \mathbb{R}^{d \times N}, \quad \tau \in [t-W+1, t] \label{eq:key} \\
% \mathbf{V}^{\tau} &= \text{Conv1D}(\mathbf{H}^{\tau}; \theta_V) \in \mathbb{R}^{d \times N} \label{eq:value}
% \end{align}

% \paragraph{Attention Scores.}
% The attention scores measure similarity between the current query and past keys:
% \begin{equation}
% \alpha^{\tau}_i = \frac{\mathbf{Q}_i^T \mathbf{K}^{\tau}_i}{\sqrt{d}}, \quad i = 1, \ldots, N
% \label{eq:attention_scores}
% \end{equation}

% Normalization via softmax across the temporal dimension yields attention weights:
% \begin{equation}
% a^{\tau}_i = \frac{\exp(\alpha^{\tau}_i)}{\sum_{\tau'=t-W+1}^{t} \exp(\alpha^{\tau'}_i)}
% \label{eq:attention_weights}
% \end{equation}

% This ensures:
% \begin{equation}
% \sum_{\tau=t-W+1}^{t} a^{\tau}_i = 1, \quad a^{\tau}_i \geq 0
% \label{eq:attention_normalization}
% \end{equation}

% \paragraph{Context Aggregation.}
% The weighted sum of value vectors produces the context representation:
% \begin{equation}
% \mathbf{C}_i = \sum_{\tau=t-W+1}^{t} a^{\tau}_i \mathbf{V}^{\tau}_i
% \label{eq:context}
% \end{equation}

% \paragraph{Output Projection and Residual Connection.}
% The context is projected back to the feature dimension and combined with the last frame via residual connection:
% \begin{equation}
% \mathbf{F} = \text{GroupNorm}(\text{Conv1D}(\mathbf{C}; \theta_{\text{out}}) + \mathbf{H}^t)
% \label{eq:attention_output}
% \end{equation}

% The GroupNorm stabilizes training by normalizing activations across channel groups.

% %------------------------------------------------------------------------------
% \subsection{Convolutional Decoder}
% \label{subsec:decoder}
% %------------------------------------------------------------------------------

% The decoder transforms the attention-enriched features \(\mathbf{F} \in \mathbb{R}^{d \times N}\) into spatial corrections \(\Delta u \in \mathbb{R}^{1 \times N}\):

% \begin{equation}
% \begin{aligned}
% \mathbf{D}_1 &= \text{GELU}(\text{BatchNorm}(\text{Conv1D}(\mathbf{F}; \text{kernel}=5))) \\
% \Delta u &= \text{Conv1D}(\mathbf{D}_1; \text{kernel}=5, \text{out\_channels}=1)
% \end{aligned}
% \label{eq:decoder}
% \end{equation}

% \paragraph{Multi-Scale Receptive Field.}
% The two convolutional layers with kernel size 5 provide an effective receptive field:
% \begin{equation}
% R_{\text{eff}} = 1 + 2 \times (5 - 1) = 9
% \label{eq:receptive_field_decoder}
% \end{equation}

% This covers approximately \(\frac{9}{128} \times 2\pi \approx 0.44\) spatial units, sufficient to capture local gradients and shock structures.

% \paragraph{Bounded Correction via Tanh.}
% To prevent instabilities, corrections are bounded:
% \begin{equation}
% \Delta u_{\text{bounded}} = \tanh(\Delta u) \cdot c_{\text{clip}}
% \label{eq:tanh_clipping}
% \end{equation}
% where \(c_{\text{clip}} = 0.1\). This ensures:
% \begin{equation}
% |\Delta u_{\text{bounded}}| \leq 0.1
% \label{eq:correction_bound}
% \end{equation}

% preventing single-step divergence during autoregressive rollout.

% %------------------------------------------------------------------------------
% \subsection{Final Prediction}
% \label{subsec:final_prediction}
% %------------------------------------------------------------------------------

% The next state is computed via residual update:
% \begin{equation}
% \hat{u}^{t+1} = u^t + \Delta u_{\text{bounded}}
% \label{eq:residual_update}
% \end{equation}

% This formulation has two advantages:
% \begin{enumerate}
%     \item \textbf{Easier Optimization}: Learning perturbations \(\Delta u\) rather than absolute states \(u^{t+1}\) simplifies the learning task
%     \item \textbf{Physical Alignment}: For small timesteps, the true dynamics satisfy \(u^{t+1} \approx u^t + \Delta t \cdot f(u^t)\), matching our residual formulation
% \end{enumerate}

% %==============================================================================
% \section{Training Strategy}
% \label{sec:training_strategy}
% %==============================================================================

% %------------------------------------------------------------------------------
% \subsection{Loss Function Design}
% \label{subsec:loss_function}
% %------------------------------------------------------------------------------

% Our loss function combines multiple terms to enforce both accuracy and physical consistency.

% \paragraph{Mean Squared Error (MSE).}
% The primary loss is the L2 prediction error:
% \begin{equation}
% \mathcal{L}_{\text{MSE}} = \frac{1}{BN} \sum_{b=1}^B \sum_{i=1}^N (\hat{u}_i^b - u_i^b)^2
% \label{eq:mse_loss}
% \end{equation}

% \paragraph{Gradient Matching Loss.}
% To preserve spatial structures and shocks, we match spatial gradients:
% \begin{equation}
% \mathcal{L}_{\text{grad}} = \frac{1}{BN} \sum_{b=1}^B \sum_{i=1}^N \left(\frac{\partial \hat{u}_i^b}{\partial x} - \frac{\partial u_i^b}{\partial x}\right)^2
% \label{eq:gradient_loss}
% \end{equation}

% Gradients are computed via central differences:
% \begin{equation}
% \frac{\partial u_i}{\partial x} \approx \frac{u_{i+1} - u_{i-1}}{2\Delta x}
% \label{eq:gradient_computation}
% \end{equation}

% \paragraph{Energy Dissipation Constraint.}
% The Burgers equation dissipates energy due to viscosity:
% \begin{equation}
% E(t+\Delta t) \leq E(t)
% \label{eq:energy_physics}
% \end{equation}

% We enforce this via a penalty:
% \begin{equation}
% \mathcal{L}_{\text{energy}} = \frac{1}{B} \sum_{b=1}^B \text{ReLU}\left(\frac{E(\hat{u}^b) - E(u^{t,b})}{E(u^{t,b}) + \epsilon}\right)
% \label{eq:energy_loss}
% \end{equation}

% where:
% \begin{equation}
% E(u) = \frac{\Delta x}{2} \sum_{i=1}^N u_i^2
% \label{eq:energy_formula}
% \end{equation}

% The ReLU ensures we only penalize energy \textit{increases}, not decreases. Normalization by \(E(u^t)\) makes the penalty scale-invariant.

% \paragraph{Combined Loss.}
% The total loss is:
% \begin{equation}
% \mathcal{L}_{\text{total}} = \mathcal{L}_{\text{MSE}} + \lambda_{\text{grad}} \mathcal{L}_{\text{grad}} + \lambda_{\text{energy}} \mathcal{L}_{\text{energy}}
% \label{eq:total_loss}
% \end{equation}

% with weights \(\lambda_{\text{grad}} = 0.1\) and \(\lambda_{\text{energy}} = 0.05\).

% %------------------------------------------------------------------------------
% \subsection{Curriculum Learning Strategy}
% \label{subsec:curriculum_learning}
% %------------------------------------------------------------------------------

% To improve long-term stability, we employ curriculum learning that gradually increases task difficulty.

% \paragraph{Rollout Depth Scheduling.}
% Early in training, we use short rollouts; later, we extend to longer predictions:
% \begin{equation}
% K_{\text{rollout}}(\text{epoch}) = 
% \begin{cases}
% 8 & \text{epoch} < 10 \\
% 16 & 10 \leq \text{epoch} < 30 \\
% 32 & \text{epoch} \geq 30
% \end{cases}
% \label{eq:rollout_schedule}
% \end{equation}

% During each training iteration, we unroll for \(K_{\text{rollout}}\) steps and backpropagate through the entire sequence:
% \begin{equation}
% \mathcal{L}_{\text{rollout}} = \frac{1}{K_{\text{rollout}}} \sum_{k=1}^{K_{\text{rollout}}} \mathcal{L}_{\text{total}}(\hat{u}^{t+k}, u^{t+k})
% \label{eq:rollout_loss}
% \end{equation}

% \paragraph{Teacher Forcing.}
% To prevent error accumulation during training, we occasionally use ground truth states instead of predictions:
% \begin{equation}
% u^{t+k}_{\text{next}} = 
% \begin{cases}
% u^{t+k}_{\text{true}} & \text{with probability } p_{\text{TF}} \\
% \hat{u}^{t+k} & \text{with probability } 1 - p_{\text{TF}}
% \end{cases}
% \label{eq:teacher_forcing}
% \end{equation}

% The teacher forcing rate decays over training:
% \begin{equation}
% p_{\text{TF}}(\text{epoch}) = 
% \begin{cases}
% 0.5 & \text{epoch} < 10 \\
% 0.2 & 10 \leq \text{epoch} < 30 \\
% 0.05 & \text{epoch} \geq 30
% \end{cases}
% \label{eq:teacher_forcing_schedule}
% \end{equation}

% This provides stability early (when the model is weak) while ensuring the model learns to handle its own predictions later.

% %------------------------------------------------------------------------------
% \subsection{Noise Injection for Robustness}
% \label{subsec:noise_injection}
% %------------------------------------------------------------------------------

% After epoch 20, we inject Gaussian noise into the input history to improve robustness:
% \begin{equation}
% \tilde{\mathbf{U}}_{\text{hist}} = \mathbf{U}_{\text{hist}} + \epsilon \mathcal{N}(0, I)
% \label{eq:noise_injection}
% \end{equation}

% where:
% \begin{equation}
% \epsilon = 0.01 \times \min\left(1.0, \frac{\text{epoch}}{50}\right)
% \label{eq:noise_scale}
% \end{equation}

% This prevents overfitting to exact trajectories and improves generalization to perturbed initial conditions.

% %------------------------------------------------------------------------------
% \subsection{Optimization Details}
% \label{subsec:optimization}
% %------------------------------------------------------------------------------

% \paragraph{Optimizer.}
% We use AdamW optimizer with:
% \begin{equation}
% \theta_{t+1} = \theta_t - \eta \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} - \lambda_{\text{WD}} \theta_t
% \label{eq:adamw}
% \end{equation}

% where:
% \begin{itemize}
%     \item Learning rate: \(\eta = 0.001\)
%     \item Weight decay: \(\lambda_{\text{WD}} = 10^{-4}\)
%     \item \(\beta_1 = 0.9\), \(\beta_2 = 0.999\)
%     \item \(\epsilon = 10^{-8}\)
% \end{itemize}

% \paragraph{Learning Rate Schedule.}
% Cosine annealing reduces the learning rate:
% \begin{equation}
% \eta_t = \eta_{\min} + \frac{\eta_{\max} - \eta_{\min}}{2}\left(1 + \cos\left(\frac{t}{T_{\max}}\pi\right)\right)
% \label{eq:cosine_annealing}
% \end{equation}

% with \(\eta_{\max} = 0.001\), \(\eta_{\min} = 0\), and \(T_{\max} = 50\) epochs.

% \paragraph{Gradient Clipping.}
% To prevent gradient explosion during backpropagation through time:
% \begin{equation}
% \|\nabla_{\theta} \mathcal{L}\| > 1.0 \implies \nabla_{\theta} \mathcal{L} \leftarrow \frac{\nabla_{\theta} \mathcal{L}}{\|\nabla_{\theta} \mathcal{L}\|}
% \label{eq:gradient_clipping}
% \end{equation}

% \paragraph{Batch Size and Epochs.}
% \begin{itemize}
%     \item Batch size: \(B = 32\)
%     \item Total epochs: 50
%     \item Training samples per epoch: \(\frac{1000}{32} \approx 31\) batches
% \end{itemize}

% %==============================================================================
% \section{Evaluation Metrics}
% \label{sec:evaluation_metrics}
% %==============================================================================

% We employ comprehensive metrics to assess model performance across multiple dimensions.

% %------------------------------------------------------------------------------
% \subsection{Accuracy Metrics}
% \label{subsec:accuracy_metrics}
% %------------------------------------------------------------------------------

% \paragraph{Mean Squared Error (MSE).}
% \begin{equation}
% \text{MSE} = \frac{1}{TN} \sum_{t=1}^T \sum_{i=1}^N (\hat{u}_i^t - u_i^t)^2
% \label{eq:mse_metric}
% \end{equation}

% \paragraph{Relative L2 Error.}
% Scale-invariant error:
% \begin{equation}
% E_{L^2}^{\text{rel}} = \frac{\|\hat{\mathbf{u}} - \mathbf{u}\|_2}{\|\mathbf{u}\|_2 + \epsilon}
% \label{eq:relative_l2}
% \end{equation}

% \paragraph{Per-Timestep Relative L2.}
% \begin{equation}
% E_{L^2}^{\text{rel}}(t) = \frac{\sqrt{\sum_{i=1}^N (\hat{u}_i^t - u_i^t)^2}}{\sqrt{\sum_{i=1}^N (u_i^t)^2} + \epsilon}
% \label{eq:timestep_relative_l2}
% \end{equation}

% This reveals when predictions diverge during rollout.

% \paragraph{Peak Signal-to-Noise Ratio (PSNR).}
% \begin{equation}
% \text{PSNR} = 20 \log_{10}\left(\frac{\text{RANGE}}{\sqrt{\text{MSE}}}\right)
% \label{eq:psnr}
% \end{equation}
% where \(\text{RANGE} = \max(u) - \min(u)\).

% Higher PSNR indicates better reconstruction quality.

% \paragraph{Structural Similarity Index (SSIM).}
% Measures perceptual similarity:
% \begin{equation}
% \text{SSIM}(\mathbf{u}, \hat{\mathbf{u}}) = \frac{(2\mu_u\mu_{\hat{u}} + C_1)(2\sigma_{u\hat{u}} + C_2)}{(\mu_u^2 + \mu_{\hat{u}}^2 + C_1)(\sigma_u^2 + \sigma_{\hat{u}}^2 + C_2)}
% \label{eq:ssim}
% \end{equation}

% SSIM \(\in [-1, 1]\), with 1 indicating perfect structural match.

% %------------------------------------------------------------------------------
% \subsection{Physical Consistency Metrics}
% \label{subsec:physical_metrics}
% %------------------------------------------------------------------------------

% \paragraph{Mass Conservation Error.}
% For sinusoidal IC where \(M_0 \approx 0\):
% \begin{equation}
% E_{\text{mass}}(t) = |M(t) - M_0|
% \label{eq:mass_error}
% \end{equation}

% For non-zero initial mass:
% \begin{equation}
% E_{\text{mass}}^{\text{rel}}(t) = \frac{|M(t) - M_0|}{|M_0| + \epsilon}
% \label{eq:mass_error_relative}
% \end{equation}

% \paragraph{Energy Dissipation Consistency.}
% Verify that predicted energy decreases:
% \begin{equation}
% \text{Monotonicity} = \frac{1}{T-1}\sum_{t=1}^{T-1} \mathbb{1}[E(t+1) \leq E(t) + \epsilon_{\text{tol}}]
% \label{eq:energy_monotonicity_metric}
% \end{equation}

% Perfect monotonicity gives 1.0; violations reduce this score.

% %------------------------------------------------------------------------------
% \subsection{Correlation and Spectral Metrics}
% \label{subsec:correlation_metrics}
% %------------------------------------------------------------------------------

% \paragraph{Pearson Correlation per Timestep.}
% \begin{equation}
% \rho(t) = \frac{\text{Cov}(\hat{u}^t, u^t)}{\sigma_{\hat{u}^t} \sigma_{u^t}}
% \label{eq:pearson_correlation}
% \end{equation}

% High correlation (\(\rho \approx 1\)) indicates the model captures spatial patterns accurately.

% \paragraph{Energy Spectrum Error.}
% Fourier analysis reveals whether the model captures correct wavenumber content:
% \begin{equation}
% \hat{U}^t(k) = \text{FFT}(u^t), \quad S(k) = |\hat{U}(k)|^2
% \label{eq:spectrum}
% \end{equation}

% Spectrum error:
% \begin{equation}
% E_{\text{spec}} = \frac{1}{K} \sum_{k=1}^{K/2} \left|\log_{10} S_{\text{pred}}(k) - \log_{10} S_{\text{true}}(k)\right|
% \label{eq:spectrum_error}
% \end{equation}

% %==============================================================================
% \section{Implementation Details}
% \label{sec:implementation}
% %==============================================================================

% %------------------------------------------------------------------------------
% \subsection{Software and Hardware}
% \label{subsec:software_hardware}
% %------------------------------------------------------------------------------

% \paragraph{Framework.}
% All models are implemented in PyTorch 2.0+ with CUDA support for GPU acceleration.

% \paragraph{Hardware.}
% Training is performed on:
% \begin{itemize}
%     \item GPU: NVIDIA A100 (40GB) or equivalent
%     \item Training time: \(\sim\)30 minutes for 50 epochs
%     \item Inference time: \(\sim\)10ms per rollout step
% \end{itemize}

% %------------------------------------------------------------------------------
% \subsection{Reproducibility}
% \label{subsec:reproducibility}
% %------------------------------------------------------------------------------

% To ensure reproducibility:
% \begin{itemize}
%     \item Random seeds fixed: PyTorch, NumPy, Python (\texttt{seed=42})
%     \item Deterministic algorithms enabled where available
%     \item Complete hyperparameter logging
%     \item Model checkpoints saved every 10 epochs
% \end{itemize}

% %------------------------------------------------------------------------------
% \subsection{Code Organization}
% \label{subsec:code_organization}
% %------------------------------------------------------------------------------

% Our implementation follows modular design:

% \begin{verbatim}
% burgers_solver/
% |-- data_generation.py      # Finite difference solver
% |-- dataset.py              # PyTorch Dataset/DataLoader
% |-- model.py                # Neural architecture
% |   |-- CausalTemporalAttention
% |   +-- ImprovedBurgersNet
% |-- training.py             # Training loop
% |   |-- Loss functions
% |   +-- Curriculum learning
% |-- metrics.py              # Evaluation metrics
% +-- visualization.py        # Plotting utilities
% \end{verbatim}

% The complete source code and pre-trained models are available in our GitHub repository\footnote{\url{https://github.com/Samuel-Chapuis/ML_Differential_Solver}}.

% %==============================================================================
% \section{Summary}
% \label{sec:methodology_summary}
% %==============================================================================

% This chapter has presented our comprehensive methodology for neural PDE solving:

% \begin{enumerate}
%     \item \textbf{High-quality data generation} using stable finite difference methods with upwind advection and central difference diffusion
%     \item \textbf{Novel architecture} combining causal temporal attention with convolutional decoding for effective spatiotemporal modeling
%     \item \textbf{Physics-informed training} via energy dissipation constraints and gradient matching losses
%     \item \textbf{Curriculum learning} with scheduled rollout depth and teacher forcing for improved stability
%     \item \textbf{Comprehensive evaluation} across accuracy, physical consistency, and spectral metrics
% \end{enumerate}

% The next chapter presents experimental results demonstrating the effectiveness of these design choices and comparing performance against baseline methods.

% \paragraph{Transition to Chapter 5.}
% Having established our complete methodological framework, we now turn to empirical validation. Chapter~\ref{chap:experiments} presents comprehensive experiments evaluating our approach across multiple metrics, analyzing the contribution of each architectural component through ablation studies, and comparing performance against established baseline methods.
