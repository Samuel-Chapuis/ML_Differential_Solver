\chapter{Related Work}
\label{chap:related_work}
%It is expected to find:

%\begin{itemize}
%\item The research articles  related to your defined problem/objective 
%\item Try to present them by category, the main ideas behind and their limitations
%\item Justify your directions/choices. This part should make a link with the next chapter.

%\end{itemize}

%==============================================================================
\section{Memory Architectures}
\label{sec:related_memory}
%==============================================================================

The foundation of Neural Field Turing Machine approach lies in architectures that augment neural networks with external memory and sophisticated attention mechanisms.

%------------------------------------------------------------------------------
\subsection{Neural Turing Machines}
\label{subsec:ntm}
%------------------------------------------------------------------------------

Graves, Wayne, and Danihelka~\cite{graves2014neural} introduced Neural Turing Machines (NTMs) as differentiable analogues of classical Turing machines. The key innovation is coupling a neural network controller with an external memory matrix that can be read from and written to via learned attention mechanisms, all while remaining end-to-end differentiable~\cite{graves2014neural}.

\paragraph{Architecture.}
An NTM~\cite{graves2014neural} consists of:
\begin{itemize}
    \item \textbf{Controller}: A recurrent or feedforward network that processes inputs and emits control signals
    \item \textbf{Memory matrix}: \(\mathbf{M}_t \in \mathbb{R}^{N \times M}\) with \(N\) addressable locations of size \(M\)
    \item \textbf{Read/Write heads}: Attention-based addressing mechanisms
\end{itemize}

%The controller interacts with memory through two complementary addressing modes:

%\textit{Content-based addressing} uses similarity to focus on relevant memory locations:
%\begin{equation}
%w_t^c(i) = \frac{\exp\left(\beta_t \cdot K[\mathbf{k}_t, \mathbf{M}_t(i)]\right)}{\sum_j \exp\left(\beta_t \cdot K[\mathbf{k}_t, \mathbf{M}_t(j)]\right)}
%\label{eq:ntm_content_addressing}
%\end{equation}
%where \(K[\mathbf{u}, \mathbf{v}] = \frac{\mathbf{u}^T\mathbf{v}}{\|\mathbf{u}\|\|\mathbf{v}\|}\) is cosine similarity and \(\beta_t\) controls focus sharpness.

%\textit{Location-based addressing} enables algorithmic operations via shifting and sharpening:
%\begin{align}
%\tilde{w}_t &= s_t \circledast w_t^g \quad \text{(convolutional shift)} \label{eq:ntm_shift} \\
%w_t(i) &= \frac{[\tilde{w}_t(i)]^{\gamma_t}}{\sum_j [\tilde{w}_t(j)]^{\gamma_t}} \quad \text{(sharpening)} \label{eq:ntm_sharpen}
%\end{align}

While \textbf{Reading}~\cite{graves2014neural} extracts information via weighted average:
\begin{equation}
\mathbf{r}_t = \sum_{i=1}^N w_t^r(i) \mathbf{M}_t(i)
\label{eq:ntm_read}
\end{equation}

\textbf{Writing}~\cite{graves2014neural} combines selective erasure and addition:
\begin{equation}
\mathbf{M}_t(i) = \mathbf{M}_{t-1}(i) \odot [\mathbf{1} - w_t^w(i)\mathbf{e}_t] + w_t^w(i)\mathbf{a}_t
\label{eq:ntm_write}
\end{equation}

%\paragraph{Limitations for PDEs.}
%While groundbreaking for discrete algorithmic tasks, NTMs face fundamental obstacles for PDE solving:
%\begin{enumerate}
%    \item \textbf{Discrete memory structure}: The tabular format \(\mathbf{M} \in \mathbb{R}^{N \times M}\) is designed for discrete symbols, not continuous spatial fields
%    \item \textbf{No geometric structure}: Memory locations lack inherent spatial relationships critical for PDEs
%    \item \textbf{Scalability}: Content-based addressing costs \(\mathcal{O}(N \times M)\) per timestep
%    \item \textbf{Lack of spatial inductive bias}: No assumptions about locality or translation invariance
%\end{enumerate}

\paragraph{Limitations for PDEs.}
While groundbreaking for discrete algorithmic tasks~\cite{graves2014neural}, Neural Turing Machines face fundamental obstacles for Partial Differential Equation 
solving due to their discrete memory structure, which uses a tabular format \(\mathbf{M} \in \mathbb{R}^{N \times M}\) designed for discrete symbols rather than continuous spatial fields. 
Furthermore, their memory locations possess no inherent geometric or spatial relationships, which are critical for representing PDE domains. This design also leads to scalability issues, as content-based addressing incurs a cost of \(\mathcal{O}(N \times M)\) per timestep~\cite{graves2014neural}. Ultimately, the architecture lacks the necessary spatial inductive biases—such as assumptions about locality or translation invariance—that are essential for efficiently learning and solving PDEs.

\paragraph{Relevance.}
NTMs establish the foundational principle we coded, an external memory (the spatial field \(u(x,t)\)) combined with learned read/write operations (our controller implementation). However, we fundamentally adapt this paradigm from discrete memory slots to continuous spatial fields, replacing content-based addressing with local spatial convolutions.

%==============================================================================
\section{Physics-Informed Neural Networks}
\label{sec:related_pinns}
%==============================================================================

Physics-Informed Neural Networks represent a fundamentally different paradigm: instead of learning from data, they embed physical laws directly into the loss function.

%------------------------------------------------------------------------------
\subsection{PINN Framework}
\label{subsec:pinn_framework}
%------------------------------------------------------------------------------

Raissi, Perdikaris, and Karniadakis~\cite{raissi2019physics} introduced PINNs as a method for solving forward and inverse PDE problems by incorporating the governing equations as soft constraints during training.

For the Burgers equation~\cite{raissi2019physics}:
\begin{equation}
\frac{\partial u}{\partial t} + u\frac{\partial u}{\partial x} = \nu\frac{\partial^2 u}{\partial x^2}, \quad (x,t) \in \Omega \times [0,T]
\label{eq:burgers_pinn_pde}
\end{equation}

A PINN approximates \(u(x,t) \approx u_{\theta}(x,t)\) using a deep neural network and minimizes~\cite{raissi2019physics}:
\begin{equation}
\mathcal{L}_{\text{PINN}} = \lambda_f \mathcal{L}_f + \lambda_u \mathcal{L}_u + \lambda_b \mathcal{L}_b
\label{eq:pinn_total_loss}
\end{equation}

%where:
%\begin{align}
%\mathcal{L}_f &= \frac{1}{N_f}\sum_{i=1}^{N_f} \left|f_{\theta}(x_i^f, t_i^f)\right|^2 \label{eq:pinn_physics_loss} \\
%f_{\theta} &= \frac{\partial u_{\theta}}{\partial t} + u_{\theta}\frac{\partial u_{\theta}}{\partial x} - \nu\frac{\partial^2 u_{\theta}}{\partial x^2} \label{eq:pinn_residual} \\
%\mathcal{L}_u &= \frac{1}{N_u}\sum_{j=1}^{N_u} \left|u_{\theta}(x_j^u, t_j^u) - u_j\right|^2 \label{eq:pinn_data_loss} \\
%\mathcal{L}_b &= \frac{1}{N_b}\sum_{k=1}^{N_b} \left|u_{\theta}(x_k^b, t_k^b) - u_k^b\right|^2 \label{eq:pinn_boundary_loss}
%\end{align}

\paragraph{Advantages.}
The approach~\cite{raissi2019physics} is fundamentally \textbf{mesh-free}, eliminating the need for spatial discretization of the domain. This framework is naturally suited for \textbf{inverse problems}, as it can infer unknown physical parameters, such as the viscosity coefficient \(\nu\), directly from sparse and noisy observational data. Furthermore, it provides a \textbf{continuous solution} across space and time, allowing for evaluation at arbitrary query points \((x, t)\). Finally, it is effective in a \textbf{low data regime}, capable of producing accurate solutions with few or even no traditional high-fidelity training measurements~\cite{raissi2019physics}.

\paragraph{Limitations for Burgers Equation.}
Despite their theoretical elegance, Physics-Informed Neural Networks (PINNs) face severe challenges for convection-dominated PDEs such as Burgers' equation. A primary issue is the \textbf{spectral bias toward low frequencies}, where neural networks, as demonstrated by Rahaman et al.~\cite{rahaman2019spectral}, preferentially learn low-frequency components of a solution. For shock-forming solutions rich in high-frequency content, this inherent bias manifests as an inability to accurately represent sharp gradients, leads to oscillatory artifacts (Gibbs phenomenon) near discontinuities, and results in extremely slow convergence, often requiring over \(10^5\) training iterations~\cite{rahaman2019spectral}. This limitation is compounded by a significant \textbf{computational cost}. Training a PINN requires a dense set of collocation points (\(N_f \sim 10^4\) for 1D problems), the computation of high-order derivatives via expensive automatic differentiation, and a lengthy optimization process typically spanning \(50{,}000\) to \(200{,}000\) iterations for convergence~\cite{raissi2019physics}. These factors together present substantial practical barriers for solving convection-dominated flows.

\paragraph{Relevance to Our Work.}
Rather than minimizing PDE residuals, we learn from pre-computed high-fidelity trajectories. However, we retain physics-informed \textit{regularization} without requiring it as the primary loss.

%------------------------------------------------------------------------------

\section{Transformer-based Temporal Modeling}

This section explains the role and functioning of Transformer-inspired models used in this work for temporal prediction tasks.

\subsection{Motivation}

Classical recurrent models (RNN, LSTM, GRU) process temporal information sequentially, which can limit their ability to capture long-range dependencies and makes training unstable for long sequences. Transformers address this limitation by replacing recurrence with an attention mechanism, allowing the model to directly weight the importance of each past time step when making a prediction.

In the context of physical systems such as the Burgers equation, this property is particularly relevant: the future state may depend more strongly on specific past configurations (e.g. shocks or gradients) than on the immediately preceding state alone.

\subsection{General Principle of Transformers}

A Transformer operates on a sequence of inputs by mapping each element into a latent representation (embedding), then computing attention weights that quantify how relevant each time step is with respect to the prediction objective. Formally, attention can be interpreted as a learned weighted average over time, where the weights are data-dependent and normalized using a softmax function.

Unlike full self-attention Transformers used in NLP, the architecture employed here focuses on temporal attention only, which is sufficient for short-to-medium temporal windows and significantly reduces computational cost.

\subsection{TransformerController Architecture}

The \texttt{TransformerController} implemented in this work takes as input a temporal sequence of spatial patches and a physical parameter (the viscosity $\nu$). Each time step is processed independently by a shared encoder, then aggregated using attention.

\paragraph{Input encoding}
At each time step, the spatial patch and the viscosity are concatenated and passed through a multi-layer perceptron to produce a hidden embedding. This step lifts the raw physical variables into a higher-dimensional latent space where nonlinear relationships can be represented.

\paragraph{Temporal attention}
For a sequence of length $L$, the model computes a scalar attention score for each time step. These scores are normalized across time using a softmax function, producing attention weights that sum to one. The final temporal context vector is obtained as a weighted sum of the embeddings:
\[
\mathbf{c} = \sum_{t=1}^{L} \alpha_t , \mathbf{h}_t
\]
where $\alpha_t$ denotes the learned attention weight and $\mathbf{h}_t$ the embedding at time $t$.

\paragraph{Prediction head}
The aggregated context vector is then passed through a fully connected prediction head to output a scalar value. In this application, this scalar corresponds to the predicted value of the field at the center of the spatial patch at the next time step.

\subsection{Interpretability and Advantages}

A key advantage of this Transformer-based controller is interpretability. The attention weights provide direct insight into which past time steps are most influential for a given prediction. This is particularly useful in a physical setting, where one may want to verify whether the model focuses on physically meaningful events (e.g. shock formation).

Compared to recurrent alternatives, this architecture:
\begin{itemize}
\item avoids vanishing or exploding gradients caused by long recursions,
\item enables parallel processing over the temporal dimension,
\item and provides explicit, learnable temporal importance weights.
\end{itemize}

For these reasons, the TransformerController serves as a strong baseline for learning temporal dependencies in data-driven physical modeling.
